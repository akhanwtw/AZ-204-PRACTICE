--------------------------------------------------------------------------------
üìå Question 161 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing several microservices to deploy to a new Azure Kubernetes Service cluster. The microservices manage data stored in Azure Cosmos DB and Azure Blob storage. The data is secured by using customer-managed keys stored in Azure Key Vault.

You must automate key rotation for all Azure Key Vault keys and allow for manual key rotation. Keys must rotate every three months. Notifications of expiring keys must be sent before key expiry.

You need to configure key rotation and enable key expiry notifications.

Which two actions should you perform? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

A. Create and configure a new Azure Event Grid instance.
B. Configure Azure Key Vault alerts.
C. Create and assign an Azure Key Vault access policy.
D. Create and configure a key rotation policy during key creation

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Create and configure a new Azure Event Grid instance.**
**D. Create and configure a key rotation policy during key creation**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To satisfy the requirements of automated rotation and expiry notifications, you need to leverage both the internal Key Vault rotation logic and the external eventing system.

**1. Automate Key Rotation (Option D)**
Azure Key Vault Customer-Managed Keys (CMK) supports **automated key rotation** through a **Rotation Policy**.
*   You can configure a policy on the key (either at creation or later) to specify `lifetimeActions`.
*   You would set the action to "Rotate" with a time interval (e.g., "P3M" for 3 months) or specifically set the rotation to occur a certain number of days after creation. This directly handles the "automate key rotation... every three months" requirement.

**2. Enable Key Expiry Notifications (Option A)**
Azure Key Vault publishes lifecycle events to **Azure Event Grid**.
*   To receive notifications about expiring keys, you must subscribe to the `Microsoft.KeyVault.KeyNearExpiry` event.
*   The rotation policy allows you to define the time window for this event (e.g., "notify 30 days before expiry").
*   Once the event is triggered, Event Grid receives it and can route it to a handler (like a Logic App to send an email, or a Webhook), thus fulfilling the notification requirement.

**Why others are incorrect:**
*   **B. Configure Azure Key Vault alerts:** While Azure Monitor can alert on metrics (like availability or saturation), specific lifecycle events like "Key Near Expiry" are architecturally designed to flow through Event Grid, not standard metric alerts.
*   **C. Create and assign an Azure Key Vault access policy:** Access policies (or RBAC roles) control *who* can access the keys or manage the vault (Authorization). They do not provide functionality for scheduled automation, rotation, or event notification.

**References:**
*   [Configure cryptographic key auto-rotation in Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/keys/how-to-configure-key-rotation)
*   [Monitoring Key Vault with Azure Event Grid](https://learn.microsoft.com/en-us/azure/key-vault/general/event-grid-overview)

--------------------------------------------------------------------------------
üìå Question 162 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing a back-end Azure App Service that scales based on the number of messages contained in a Service Bus queue.

A rule already exists to scale up the App Service when the average queue length of unprocessed and valid queue messages is greater than 1000.

You need to add a new rule that will continuously scale down the App Service as long as the scale up condition is not met.

How should you configure the Scale rule? To answer, select the appropriate options in the answer area.

## Answer Area

### Scale rule

**Metric source** ______________________

**Resource type**
- Service Bus Namespaces

**Resource**
- MessageQueue1103

**Queues (required)**
- itemqueue

### Criteria
**Metric name (required)** ______________________

**Time grain**
- 1 minute

**Time grain statistic (required)** ______________________

**Dropdown 1 (Metric source):**
1. Storage queue
2. Service Bus queue
3. Current resource
4. Storage queue (classic)

**Dropdown 2 (Metric name):**
1. Message Count
2. Active Message Count

**Dropdown 3 (Time grain statistic):**
1. Total
2. Maximum
3. Average
4. Count

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Dropdown 1:** Service Bus queue
- **Dropdown 2:** Active Message Count
- **Dropdown 3:** Average

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Metric source: Service Bus queue**
The scenario explicitly states the application scales based on messages in a "Service Bus queue". Therefore, you must select **Service Bus queue** as the source, not Storage Queue or the Current Resource (which would be the App Service Plan's CPU/Memory).

**2. Metric name: Active Message Count**
The requirement mentions "unprocessed and valid queue messages."
*   **Active Message Count:** Represents messages that are currently in the queue and available for processing. This excludes dead-lettered messages, scheduled messages, or transfer messages, which matches the "valid" and "unprocessed" criteria.
*   **Message Count:** Usually represents the total count, which might include dead-letter messages. Scaling based on dead-letter messages would be inefficient as the workers cannot process them in the normal loop.

**3. Time grain statistic: Average**
The existing rule is described as using the "average queue length". To align with standard autoscale best practices and the existing logic:
*   **Average:** Calculates the average number of messages over the evaluation period. This is the robust way to handle queue depth, smoothing out momentary spikes or dips (flapping) to ensure scaling actions are taken based on sustained load.
*   *Total/Count* would be inappropriate for a point-in-time status like "items in queue" (summing queue length over 5 minutes gives a nonsensical number like "5000 queue-seconds").

**References:**
*   [Common autoscale metrics (Service Bus)](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-common-metrics#service-bus-queue)
*   [Best practices for Autoscale](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-best-practices)

--------------------------------------------------------------------------------
üìå Question 163 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are a developer building a web site using a web app. The web site stores configuration data in Azure App Configuration.

Access to Azure App Configuration has been configured to use the identity of the web app for authentication. Security requirements specify that no other authentication systems must be used.

You need to load configuration data from Azure App Configuration.

How should you complete the code? To answer, select the appropriate options in the answer area.

```python

from azure.identity import _________________________
# Options:
# - DefaultAzureCredential
# - ChainedTokenCredential
# - ManagedIdentityCredential
# - AddAzureAppConfiguration

from azure.appconfiguration import _________________________
# Options:
# - DefaultAzureCredential
# - ChainedTokenCredential
# - ManagedIdentityCredential
# - AddAzureAppConfiguration

credential = _________________________()
# Options:
# - DefaultAzureCredential
# - ChainedTokenCredential
# - ManagedIdentityCredential
# - AddAzureAppConfiguration

client = _________________________(
    base_url="...",
    credential=credential
)
# Options:
# - DefaultAzureCredential
# - ChainedTokenCredential
# - ManagedIdentityCredential
# - AddAzureAppConfiguration
```
--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**Selection 1:** `DefaultAzureCredential`
**Selection 2:** `AzureAppConfigurationClient` (Note: The screenshot provided in the question likely contains a typo in the dropdown for the second import. In typical AZ-204 exam questions, this option is usually represented as `AzureAppConfigurationClient` or implied by the context of the client options below. Based on the visible options in your specific screenshot, the correct *standard* answer is `AzureAppConfigurationClient`, but if strictly limited to visible options and "AddAzureAppConfiguration" is the only vaguely relevant string, it's a distractor. However, the standard logical flow is importing the identity credential and the configuration client.)

*Correction based on visible options in typical variations of this question:*
The screenshot actually shows the same list of options for all dropdowns, which is a common visual representation in these exam dumps, but logically:

1.  **Import 1 (`azure.identity`):** `DefaultAzureCredential`
2.  **Import 2 (`azure.appconfiguration`):** `AzureAppConfigurationClient` (Often synonymous with the client class. In the screenshot provided, the dropdowns look identical, so we select the class name logic).
3.  **Instantiation 1 (`credential`):** `DefaultAzureCredential`
4.  **Instantiation 2 (`client`):** `AzureAppConfigurationClient`

*(Self-Correction/Refinement based on the specific image provided)*:
The image shows dropdowns with options: `DefaultAzureCredential`, `ChainedTokenCredential`, `ManagedIdentityCredential`, `AddAzureAppConfiguration`. But the second import line says `from azure.app configuration import`. This is structurally asking for the **Client** class name.

**Selection 1:** `DefaultAzureCredential`
**Selection 2:** `AzureAppConfigurationClient` (The screenshot actually has a typo, often showing the same list, or `AzureAppConfigurationClient` is the intended answer for the blank. If forced to choose from the visible list in the image for the *second* dropdown, it's often a mismatch in exam dumps, but logically it *must* include the Client class. Assuming standard exam behavior where the correct option is available in the real test: `AzureAppConfigurationClient`).
**Selection 3:** `DefaultAzureCredential`
**Selection 4:** `AzureAppConfigurationClient`

**Final Answer Set:**
1.  **DefaultAzureCredential**
2.  **AzureAppConfigurationClient**
3.  **DefaultAzureCredential**
4.  **AzureAppConfigurationClient**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Authentication (`DefaultAzureCredential`)**
The requirement specifies using the "identity of the web app" (Managed Identity) and "no other authentication systems".
*   `DefaultAzureCredential` is the best practice standard. It automatically attempts to use `ManagedIdentityCredential` when deployed to Azure (App Service), fulfilling the requirement. It simplifies the code compared to explicitly using `ManagedIdentityCredential` and covers local dev fallbacks if allowed (though the prompt is strict, this is the standard SDK pattern).

**2. Client Usage (`AzureAppConfigurationClient`)**
To interact with the data plane of Azure App Configuration (getting/setting settings), you use the `AzureAppConfigurationClient`.
*   The code pattern `client = ClassName(base_url="...", credential=cred)` is the constructor signature for `AzureAppConfigurationClient`.

**Code Walkthrough:**
```python
# 1. Import the standard credential chain
from azure.identity import DefaultAzureCredential

# 2. Import the client class for App Config
from azure.appconfiguration import AzureAppConfigurationClient

# 3. Instantiate the credential. 
# In Azure App Service, this uses the system-assigned identity.
credential = DefaultAzureCredential()

# 4. Instantiate the client with the endpoint and the identity credential.
client = AzureAppConfigurationClient(base_url="...", credential=credential)
```


--------------------------------------------------------------------------------
üìå Question 164 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are preparing to deploy a Python website to an Azure Web App using a container. The solution will use multiple containers in the same container group.

The Dockerfile builds a container using 
```docker
FROM python:3
ADD website.py
CMD ["Python" , "/website.py""]
```
You build a container with the command: 
```cmd
docker build -t images.azurecr.io/website:v1.0.0
```

The registry is private with username "admin".
The Web App must always run the same version of the website regardless of future builds.

How should you complete the commands?

## Answer Area

```
az configure --defaults web=website  
az configure --defaults group=website  

az appservice plan create --name websitePlan __________________
# Options:
# --sku SHARED
# --tags container
# --sku B1 --hyper-v
# --sku B1 --is-linux

az webapp create --plan websitePlan __________________
# Options:
# --deployment-source-url images.azurecr.io/website:v1.0.0
# --deployment-source-url images.azurecr.io/website:latest
# --deployment-container-image-name images.azurecr.io/website:v1.0.0
# --deployment-container-image-name images.azurecr.io/website:latest

az webapp config __________________
# Options:
# set --python-version 2.7 --generic-configurations user=admin password=admin
# set --python-version 3.6 --generic-configurations user=admin password=admin
# container set --docker-registry-server-url https://images.azurecr.io -u admin -p admin
# container set --docker-registry-server-url https://images.azurecr.io/website -u admin -p admin
```
--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Dropdown 1:** `--sku B1 --is-linux`
- **Dropdown 2:** `--deployment-container-image-name images.azurecr.io/website:v1.0.0`
- **Dropdown 3:** `container set --docker-registry-server-url https://images.azurecr.io -u admin -p admin`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. App Service Plan (`--sku B1 --is-linux`)**
The application is a Python application packaged in a Docker container (`FROM python:3`). Azure Web App for Containers runs on Linux. You must provision an App Service Plan that supports Linux containers.
*   **--is-linux:** Creates a Linux-based App Service Plan.
*   **--sku B1:** The Basic tier supports custom containers (Shared/Free tiers generally do not support custom container deployments in the same way).
*   *Why not Hyper-V?* Hyper-V isolation is used for Windows Containers, not standard Python/Linux containers.

**2. Web App Creation (`--deployment-container-image-name ...:v1.0.0`)**
*   **Argument:** `--deployment-container-image-name` (or `-i`) is the correct argument to specify a container image for the Web App. `--deployment-source-url` is typically used for Git/source code deployments.
*   **Version Pinning:** The requirement states the app must "always run the same version... regardless of future builds." Using the tag `:latest` would cause the app to update if a new image was pushed to that tag. You must use the specific tag generated in the build step: `:v1.0.0`.

**3. Configuration (`container set ...`)**
Since the registry is **private**, you must configure the Docker registry credentials on the Web App so it can pull the image.
*   **Command:** `az webapp config container set` is used to update container settings.
*   **Server URL:** The `--docker-registry-server-url` must be the root URL of the registry (e.g., `https://images.azurecr.io`). It should not include the repository name (`/website`).
*   *Why not `set --python-version`?* That command is for "Code" based deployments (where Azure manages the runtime). Since we are bringing our own container, we configure container settings, not runtime stack settings.

**References:**
*   [Deploy a custom container to App Service using Azure CLI](https://learn.microsoft.com/en-us/azure/app-service/tutorial-custom-container?pivots=container-linux)
*   [az webapp config container set](https://learn.microsoft.com/en-us/cli/azure/webapp/config/container?view=azure-cli-latest#az-webapp-config-container-set)

--------------------------------------------------------------------------------
üìå Question 165 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing a ticket reservation system for an airline.

The storage solution for the application must meet the following requirements:
*   Ensure at least 99.99% availability and provide low latency.
*   Accept reservations even when localized network outages or other unforeseen failures occur.
*   Process reservations in the exact sequence as reservations are submitted to minimize overbooking or selling the same seat to multiple travelers.
*   Allow simultaneous and out-of-order reservations with a maximum five-second tolerance window.

You provision a resource group named airlineResourceGroup in the Azure South-Central US region.
You need to provision a SQL API Cosmos DB account to support the app.

How should you complete the Azure CLI commands? To answer, select the appropriate options in the answer area.

## Answer Area
```powershell
resourceGroupName='airlineResourceGroup'  
name='docdb-airline-reservations'  
databaseName='docdb-tickets-database'  
collectionName='docdb-tickets-collection'  
consistencyLevel=________________

az cosmosdb create \
  --name $name \
  _____________\
  --resource-group $resourceGroupName \
  --max-interval 5 \
  ________________ \  
  --default-consistency-level $consistencyLevel
```

**Dropdown 1 (variable definition):**
1. Strong
2. Eventual
3. ConsistentPrefix
4. BoundedStaleness

**Dropdown 2 (az cosmosdb create options):**
1. --enable-virtual-network true
2. --enable-automatic-failover true
3. --kind 'GlobalDocumentDB'
4. --kind 'MongoDB'

**Dropdown 3 (location configuration):**
1. --locations 'southcentralus'
2. --locations 'eastus'
3. --locations 'southcentralus=0 eastus=1 westus=2'
4. --locations 'southcentralus=0'

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Dropdown 1:** BoundedStaleness
- **Dropdown 2:** --enable-automatic-failover true
- **Dropdown 3:** --locations 'southcentralus=0 eastus=1 westus=2'

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Consistency Level: `BoundedStaleness`**
The requirement states:
*   "Process reservations in the exact sequence... minimize overbooking." (Suggests Strong consistency).
*   "Allow simultaneous and out-of-order reservations with a **maximum five-second tolerance window**."

This specific "tolerance window" points directly to **Bounded Staleness**. Bounded Staleness guarantees that reads might lag behind writes by a configured interval (time or number of modifications), but the order is preserved. The CLI command in the snippet includes `--max-interval 5`, which aligns perfectly with the "five-second tolerance window" requirement. **Strong** consistency provides no tolerance window (latency is higher), and **Eventual** consistency doesn't guarantee order or a specific time window.

**2. Failover: `--enable-automatic-failover true`**
The requirement states: "Accept reservations even when localized network outages or other unforeseen failures occur."
To ensure high availability (99.99%+) during regional outages, you must enable **Automatic Failover**. This allows Cosmos DB to failover to a secondary region without manual intervention if the primary region goes down.

**3. Locations: `--locations 'southcentralus=0 eastus=1 westus=2'`**
To support failover (requirement 2) and high availability, you need **multiple regions**.
*   `southcentralus=0` establishes the primary write region (matching the resource group location).
*   `eastus=1` and `westus=2` establish failover priorities for read regions.
*   The single-location options (`southcentralus=0` or just `southcentralus`) would not provide protection against a regional outage as requested.

**References:**
*   [Consistency levels in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/consistency-levels)
*   [High availability in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/high-availability)
*   [Manage an Azure Cosmos DB account (Azure CLI)](https://learn.microsoft.com/en-us/cli/azure/cosmosdb?view=azure-cli-latest#az-cosmosdb-create)

--------------------------------------------------------------------------------
üìå Question 166
--------------------------------------------------------------------------------
You are building a website that uses Azure Blob storage for data storage. You configure Azure Blob storage lifecycle to move all blobs to the archive tier after 30 days.

Customers have requested a service-level agreement (SLA) for viewing data older than 30 days.

You need to document the minimum SLA for data recovery.

Which SLA should you use?

- A. at least two days
- B. between one and 15 hours
- C. at least one day
- D. between zero and 60 minutes

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. between one and 15 hours**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**The Scenario**
Your data lifecycle policy moves data to the **Archive** tier after 30 days. The Archive tier is an *offline* tier, meaning data cannot be read immediately. To access it, you must **rehydrate** the blob to an online tier (Hot or Cool).

**Why B (between one and 15 hours) is correct:**
*   **Rehydration Duration:** When rehydrating a blob from the Archive tier using **Standard priority**, the request is processed in the order received and can take up to **15 hours**. This is the standard operational window for retrieving archived data.
*   While **High Priority** rehydration exists (which can take under 1 hour for objects < 10 GB), it costs more and is an optional configuration. The baseline "minimum SLA" for general Archive recovery without specific constraints on file size or cost is the standard expected duration.

**Why others are incorrect:**
*   **A. at least two days:** This is incorrect; standard rehydration is faster than 48 hours.
*   **C. at least one day:** Similarly, 24 hours is longer than the standard 15-hour maximum.
*   **D. between zero and 60 minutes:** This applies only to **High Priority** rehydration for small blobs. Since the question does not specify that you are paying the premium for High Priority or that all files are small, you cannot guarantee this SLA for all data recovery scenarios.

**References:**
*   [Blob rehydration from the Archive tier](https://learn.microsoft.com/en-us/azure/storage/blobs/archive-rehydrate-overview)

--------------------------------------------------------------------------------
üìå Question 167 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are implementing an order processing system. A point of sale application publishes orders to topics in an Azure Service Bus queue. The Label property for the topic includes the following data:

| Property       | Description                                                         |
|----------------|---------------------------------------------------------------------|
| ShipLocation   | The country/region where the order will be shipped                  |
| CorrelationId  | A priority value for the order                                      |
| Quantity       | A user-defined field that stores the quantity of items in an order  |
| AuditedAt      | A user-defined field that records the date an order is audited      |

The system has the following requirements for subscriptions:

| Subscription type     | Comments                                                                                   |
|-----------------------|--------------------------------------------------------------------------------------------|
| FutureOrders          | This subscription is reserved for future use and must not receive any orders               |
| HighPriorityOrders    | Handle all high priority orders and international orders                                   |
| InternationalOrders   | Handle orders where the country/region is not United States                                |
| HighQuantityOrders    | Handle only orders with quantities greater than 100 units                                  |
| AllOrders             | Used for auditing purposes. Must receive every order. Updates the `AuditedAt` property     |
|                       | to include the date and time the order was received by the subscription                    |

You need to implement filtering and maximize throughput while evaluating filters.

Which filter types should you implement? To answer, drag the appropriate filter types to the correct subscriptions. Each filter type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.

NOTE: Each correct selection is worth one point.

## Answer Area

| Subscription           | Filter type        |
|------------------------|--------------------|
| FutureOrders           | _________________  |
| HighPriorityOrders     | _________________  |
| InternationalOrders    | _________________  |
| HighQuantityOrders     | _________________  |
| AllOrders              | _________________  |

**Filter Types:**
*   SQLFilter
*   CorrelationFilter
*   No Filter

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------

| Subscription           | Filter type        |
|------------------------|--------------------|
| FutureOrders           | SQLFilter          |
| HighPriorityOrders     | CorrelationFilter  |
| InternationalOrders    | SQLFilter          |
| HighQuantityOrders     | SQLFilter          |
| AllOrders              | No Filter          |

*(Note: In some exam iterations, "AllOrders" is mapped to "SQLFilter" representing `1=1`, but "No Filter" is the standard representation for the default "Match All" behavior in this drag-drop context).*

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. FutureOrders: SQLFilter**
To block all messages ("must not receive any orders"), you need a filter that evaluates to false. This is done using a **SQLFilter** with the expression `1=0`. There is no "BlockingFilter" type; you must explicitly define this logic using SQL syntax.

**2. HighPriorityOrders: CorrelationFilter**
The requirement involves `CorrelationId`, which is a system property designed for high-speed routing. While the logic "High Priority OR International" is complex (and technically requires a composite SQL filter for the 'OR' logic), exam questions of this type specifically map the presence of **CorrelationId** (used for priority here) to **CorrelationFilter** because `CorrelationFilter` provides the highest throughput optimization, which is a key constraint in the question ("maximize throughput"). The implicit expectation is that you use the most efficient filter available for the primary routing key (`CorrelationId`).

**3. InternationalOrders: SQLFilter**
The requirement is `ShipLocation != 'United States'`. This is an **inequality** check (`!=`). **CorrelationFilter** only supports strict equality (e.g., `Key == Value`). Any logical operations like inequality, OR, or specific string parsing require the flexibility of a **SQLFilter**.

**4. HighQuantityOrders: SQLFilter**
The requirement is `Quantity > 100`. This is a **numerical comparison**. **CorrelationFilter** processes properties as string equality matches. To perform mathematical comparisons (`>`, `<`, `>=`), you must use a **SQLFilter**.

**5. AllOrders: No Filter**
This subscription must receive **every single order**. When you create a subscription without specifying a filter (or "No Filter"), Azure Service Bus applies the default `$Default` rule which is a "True Filter" (SQL `1=1`), accepting all messages. Using the default configuration (No Filter needed to be added manually) is the most efficient way to achieve "Receive All".

**Summary of Capabilities:**
*   **CorrelationFilter:** Equality matches only (Key = Value). Fastest throughput. Best for `CorrelationId`, `Label`, `ReplyTo`.
*   **SQLFilter:** Complex logic (`AND`, `OR`, `LIKE`), Comparison (`>`, `<`), Inequality (`!=`). Slower throughput due to SQL parsing.
*   **No Filter:** Represents the default pass-through state.

**References:**
*   [Azure Service Bus Topic Filters](https://learn.microsoft.com/en-us/azure/service-bus-messaging/topic-filters)

--------------------------------------------------------------------------------
üìå Question 168 
--------------------------------------------------------------------------------
You are developing a microservices solution. You plan to deploy the solution to a
multinode Azure Kubernetes Service (AKS) cluster.

You need to deploy a solution that includes the following features:
‚Ä¢ reverse proxy capabilities
‚Ä¢ configurable traffic routing
‚Ä¢ TLS termination with a custom certificate

Which components should you use?

**Components:**
‚Ä¢ Helm
‚Ä¢ Draft
‚Ä¢ Brigade
‚Ä¢ KubeCtl
‚Ä¢ Ingress Controller
‚Ä¢ CoreDNS
‚Ä¢ Virtual Kubelet

**Match the Component to the Action:**

1.  **Deploy solution.** __________________
2.  **View cluster and external IP addressing.** __________________
3.  **Implement a single, public IP endpoint that is routed to multiple microservices.** __________________

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1.  **Deploy solution:** Helm
2.  **View cluster and external IP addressing:** KubeCtl
3.  **Implement a single, public IP endpoint...:** Ingress Controller

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Deploy solution: Helm**
Helm is the package manager for Kubernetes. It allows you to define, install, and upgrade even the most complex Kubernetes applications ("solutions") using "Charts". While `kubectl` can apply manifests, Helm is specifically designed to manage the deployment lifecycle of a complete solution.

**2. View cluster and external IP addressing: KubeCtl**
`kubectl` is the Kubernetes command-line tool used to communicate with the cluster API server. It is the primary tool for inspecting the state of the cluster. To view external IP addresses assigned to services or ingress resources, you would use commands like `kubectl get service` or `kubectl get ingress`.

**3. Implement a single, public IP endpoint...: Ingress Controller**
An Ingress Controller fulfills the specific requirements listed in the scenario (reverse proxy, traffic routing, TLS termination).
‚Ä¢ It acts as a Layer 7 (HTTP/HTTPS) reverse proxy.
‚Ä¢ It allows you to route traffic from a single public IP to different backend services based on the URL path or host header.
‚Ä¢ It can offload TLS/SSL termination so your backend microservices don't have to manage certificates individually.

**Why others are incorrect:**
‚Ä¢ **Draft:** A tool to help developers create cloud-native applications (scaffolding), but not the primary operational tool for deploying the final solution in this context compared to Helm.
‚Ä¢ **Brigade:** A tool for event-driven scripting in Kubernetes, not for general app deployment or routing.
‚Ä¢ **CoreDNS:** Handles DNS resolution within the cluster, not external public IP routing.
‚Ä¢ **Virtual Kubelet:** Connects Kubernetes to other APIs (like Azure Container Instances), not related to the routing requirements.

**References:**
‚Ä¢ [Helm Documentation](https://helm.sh/docs/)
‚Ä¢ [Kubernetes Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
‚Ä¢ [Overview of kubectl](https://kubernetes.io/docs/reference/kubectl/)



--------------------------------------------------------------------------------
üìå Question 169
--------------------------------------------------------------------------------
You are building a traffic monitoring system that monitors traffic along six highways.
The system produces time series analysis-based reports for each highway.

Data from traffic sensors are stored in Azure Event Hub.

Traffic data is consumed by four departments. Each department has an Azure Web App
that displays the time series-based reports and contains a WebJob that processes
the incoming data from Event Hub. All Web Apps run on App Service Plans with three instances.

Data throughput must be maximized. Latency must be minimized.

You need to implement the Azure Event Hub.

**Settings:**
1.  **Number of partitions**
    ‚Ä¢ 3
    ‚Ä¢ 4
    ‚Ä¢ 6
    ‚Ä¢ 12

2.  **Partition Key**
    ‚Ä¢ Highway
    ‚Ä¢ Department
    ‚Ä¢ Timestamp
    ‚Ä¢ VM name

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1.  **Number of partitions:** 6
2.  **Partition Key:** Highway

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Number of partitions: 6**
The scenario involves monitoring **six highways**. When using a partition key (see below), Event Hubs hashes the key to assign it to a specific partition. By creating 6 partitions, you align the architecture with the natural sharding of your data (the 6 highways), reducing the likelihood of hash collisions where multiple busy highways share a single partition.

Additionally, regarding the consumer configuration:
‚Ä¢ Each department (consumer group) has **three instances** of the WebJob.
‚Ä¢ The Event Hubs formatting ensures load balancing where the number of partitions should be greater than or equal to the number of consumer instances.
‚Ä¢ With 6 partitions and 3 instances, each instance will cleanly process data from exactly 2 partitions, maximising parallel throughput without leaving idle resources.

**2. Partition Key: Highway**
The requirement is to produce **time series analysis-based reports for each highway**.
‚Ä¢ **Ordering is critical:** Time series analysis requires data points to be processed in the order they occurred.
‚Ä¢ **Partition Key guarantees ordering:** In Azure Event Hubs, ordering is guaranteed *within* a partition. By treating "Highway" as the partition key, you ensure that all traffic data for a specific highway is routed to the same partition. This allows the WebJob to read that highway's stream sequentially and generate accurate time-series reports.
‚Ä¢ **Why others are incorrect:** Using "Timestamp" or leaving the key null would result in round-robin distribution, scattering a single highway's data across all partitions and effectively destroying the relative ordering needed for the analysis. "Department" is a consumer concept, not a data property.

**References:**
‚Ä¢ [Mapping of Event Hubs terminology (Partitions and Keys)](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions)
‚Ä¢ [Event Hubs programming guide - Partition keys](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-programming-guide#partition-key)


--------------------------------------------------------------------------------
üìå Question 170
--------------------------------------------------------------------------------
You are developing a .Net web application that stores data in Azure Cosmos DB. The
application must use the Core API and allow millions of reads and writes.

The Azure Cosmos DB account has been created with multiple write regions enabled.
The application has been deployed to the East US2 and Central US regions.

You need to update the application to support multi-region writes.

What are two possible ways to achieve this goal? Each correct answer presents part
of the solution.

- A. Update the ConnectionPolicy class for the Cosmos client and populate the
   PreferredLocations property based on the geo-proximity of the application.
- B. Update Azure Cosmos DB to use the Strong consistency level. Add indexed
   properties to the container to indicate region.
- C. Update the ConnectionPolicy class for the Cosmos client and set the
   UseMultipleWriteLocations property to true.
- D. Create and deploy a custom conflict resolution policy.
- E. Update Azure Cosmos DB to use the Session consistency level. Send the
   SessionToken property value from the FeedResponse object of the write action
   to the end-user by using a cookie.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Update the ConnectionPolicy class for the Cosmos client and populate the PreferredLocations property based on the geo-proximity of the application.**

- **C. Update the ConnectionPolicy class for the Cosmos client and set the UseMultipleWriteLocations property to true.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To fully utilize multi-region writes (multi-master) in Azure Cosmos DB with the .NET SDK (specifically SDK v2, which uses `ConnectionPolicy`), you must explicitly configure the client application to both allow local writes and identify which region is "local".

**1. Enable the Feature (Option C)**
The `ConnectionPolicy` property `UseMultipleWriteLocations` acts as the primary switch in the SDK v2. Even if the Azure Cosmos DB account has multi-region writes enabled on the server side, the client SDK will defaults to writing only to the primary region unless this property is set to `true`.

**2. Route Traffic Correctly (Option A)**
Once multi-region writes are enabled in the client, the client needs to know where to send the write requests. By setting the `PreferredLocations` property, you instruct the application running in "East US2" to prioritize the "East US 2" database region. Without this, the client may default to the account's primary region or an arbitrary write location, failing to achieve the low-latency writes that multi-region architectures provide.

**Why others are incorrect:**
*   **B (Strong Consistency):** Generally, Strong Consistency is fundamentally at odds with the low-latency goals of multi-region deployments. It requires a quorum commit across regions, which slows down writing. It is not required to enable the feature.
*   **D (Custom Conflict Resolution):** While multi-region writes do introduce potential data conflicts, Cosmos DB utilizes a system-default "Last Writer Wins" (LWW) policy if no other policy is defined. A *custom* policy (e.g., a stored procedure) is only necessary if business logic requires complex merging strategies; it is not a mandatory infrastructure step just to "support" the feature.
*   **E (Session Tokens):** This technique is used to maintain "Read Your Writes" consistency across sticky sessions in load-balanced scenarios (Session Consistency), but it is not the configuration step that enables the application to write to multiple regions.

**References:**
*   [Configure multi-region writes in your applications (SDK v2)](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-multi-master?tabs=dotnet-v2)
*   [Global distribution with Azure Cosmos DB - .NET SDK](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/tutorial-global-distribution-sql-api?tabs=dotnetv2%2Capi-async)
