--------------------------------------------------------------------------------
üìå Question 181
--------------------------------------------------------------------------------
You are developing an application that monitors data added to an Azure Blob storage account.
You need to process each change made to the storage account.
How should you complete the code segment?

**Code:**
```csharp
var changeFeedClient = new BlobServiceClient("...").GetChangeFeedClient();
var x = default(string);
while (true)
{
    var changeFeed = changeFeedClient.____________________; // Selection 1

    foreach (var c in changeFeed)
    {
        x = c.____________________; // Selection 2
        ProcessChanges(c.Values);
    }
}
```
#### Selection 1 Options:

- GetChanges()
- GetChangesAsync()
- GetChanges(x).AsPages()
- GetChanges(x).GetEnumerator()

#### Selection 2 Options:

- ContinuationToken
- GetRawResponse().ReasonPhrase
- Values.Max(x => x.EventTime).ToString()
- Values.Min(x => x.EventTime).ToString()

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------

- Selection 1: GetChanges(x).AsPages()
- Selection 2: ContinuationToken

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------

1. GetChanges(x).AsPages() The BlobChangeFeedClient allows you to read the change feed log. The variable x is initialized as a string (default null) and is serving as the continuation token (cursor).

GetChanges(x): This method initiates the request to read changes starting from the position defined by the token x.
.AsPages(): This extension method is used to retrieve the data in pages (Page<BlobChangeFeedEvent>) rather than as a flat stream of events. This is necessary because the code inside the foreach loop accesses properties of a page (like c.Values and c.ContinuationToken) rather than individual event properties.
2. ContinuationToken Inside the foreach loop, the variable c represents a single Page<BlobChangeFeedEvent>.

c.ContinuationToken: To ensure that the processing can resume from the correct spot in the next iteration (or if the application restarts), you must capture the continuation token provided by the current page. The code assigns this back to x (x = c.ContinuationToken;), ensuring the loop maintains the correct cursor position.
Why others are incorrect:

GetChanges(): Without arguments, this doesn't utilize the x token to resume position, meaning it would read from the start every time in the while(true) loop.
GetEnumerator(): This returns an iterator for items, not pages, conflicting with the usage of c.Values inside the loop.
Values.Max(...): This calculates a timestamp, which is not a valid format for the Azure Storage continuation token.

--------------------------------------------------------------------------------
üìå Question 182
--------------------------------------------------------------------------------
 You have an Azure Cosmos DB for NoSQL account.

You plan to develop two apps named App1 and App2 that will use the change feed functionality to track changes to containers. App1 will use the pull model and App2 will use the push model.

You need to choose the method to track the most recently processed change in App1 and App2.

Which component should you use?  

**Components:**
1. Lease container
2. Integrated cache
3. Continuation token
 
**App (Model)** | **Component**  
--- | ---  
App1 (Pull model) | [ ? ]  
App2 (Push model) | [ ? ]  

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **App1 (Pull model) ‚Üí Continuation token**
- **App2 (Push model) ‚Üí Lease container**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. App1 (Pull model) ‚Üí Continuation token**
In the Cosmos DB Change Feed **Pull Model**, the client application (App1) acts like an iterator. It explicitly requests the next batch of changes. To keep track of where it left off (state management), the client must save the **Continuation Token** returned by the last response. When App1 asks for changes again, it presents this token to the service to resume from that specific point. It does not require an external container to store state automatically; the management of the token is the responsibility of the consuming application.

*   **Key Characteristic:** Client-side state management via tokens.

**2. App2 (Push model) ‚Üí Lease container**
The Change Feed **Push Model** (typically implemented via the Change Feed Processor library) abstracts away the complexity of managing state. It uses a separate container, known as the **Lease Container**, to store the state (leases) of the processing. This allows the processor to automatically bookmark where it left off and also enables dynamic scaling by distributing leases (partitions) across multiple consumers.

*   **Key Characteristic:** Server-side/Library-managed state via a dedicated storage container.

**Why "Integrated cache" is incorrect:**
The integrated cache is a feature of Azure Cosmos DB dedicated gateway used for optimizing read performance (point reads and queries). It is not involved in tracking the offset or checkpoint state for change feed processing in either model.

**References:**
*   [Change feed pull model in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/change-feed-pull-model)
*   [Change feed processor in Azure Cosmos DB (Push Model)](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/change-feed-processor)

--------------------------------------------------------------------------------
üìå Question 183
--------------------------------------------------------------------------------
You create and publish a new Azure App Service web app.

User authentication and authorization must use Azure Active Directory (Azure AD).

You need to configure authentication and authorization.

What should you do first?

- A. Add an identity provider.
- B. Map an existing custom DNS name.
- C. Create and configure a new app setting.
- D. Add a private certificate.
- E. Create and configure a managed identity.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
A. **Add an identity provider.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why A is correct (Add an identity provider):**
To secure an Azure App Service web app with Azure Active Directory (Microsoft Entra ID), you use the built-in **Authentication** feature (formerly known as "Easy Auth"). The first step in the Azure Portal configuration workflow is to navigate to the **Authentication** blade and click **Add identity provider**. From there, you select "Microsoft" (for Azure AD), which automatically guides you through registering the application and configuring the necessary settings.

**Why others are incorrect:**
*   **B. Map an existing custom DNS name:** This configures the public URL of the application but has no impact on enabling user authentication.
*   **C. Create and configure a new app setting:** While the authentication process *eventually* creates app settings (like `MICROSOFT_PROVIDER_AUTHENTICATION_SECRET`), you do not manually create these as the *first* step. You use the Identity Provider wizard to generate them.
*   **D. Add a private certificate:** This is used for SSL/TLS encyrption or client certificate authentication, not for standard user login via Azure AD.
*   **E. Create and configure a managed identity:** Managed identities allow the **App Service itself** to authenticate to other Azure resources (like SQL Database or Key Vault) securely without storing credentials. It is not used for authenticating incoming **users** to the web app.

**References:**
*   [Authentication and authorization in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/overview-authentication-authorization)
*   [Configure your App Service or Azure Functions app to use Microsoft Entra ID login](https://learn.microsoft.com/en-us/azure/app-service/configure-authentication-provider-aad)

--------------------------------------------------------------------------------
üìå Question 184 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
An organization deploys a blob storage account. Users take multiple snapshots of the blob storage account over time.

You need to delete all snapshots of the blob storage account. You must not delete the blob storage account itself.

How should you complete the code segment? To answer, select the appropriate options in the answer area.

**Code Segment:**
`Delete (Azure.Storage.Blobs.Models.DeleteSnapshotsOption snapshotsOption = Azure.Storage.Blobs.Models.[Dropdown 1].[Dropdown 2])`

**Dropdown 1 Choices:**
1. DeleteIfExists
2. DeleteSnapshotsOption
3. WithSnapshot
4. WithSnapshotCore

**Dropdown 2 Choices:**
1. IncludeSnapshots
2. None
3. OnlySnapshots

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Dropdown 1:** DeleteSnapshotsOption
- **Dropdown 2:** OnlySnapshots

**Completed Code:**
`Azure.Storage.Blobs.Models.DeleteSnapshotsOption.OnlySnapshots`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To delete snapshots without deleting the blob itself using the Azure SDK for .NET, you must specify the correct enum value from the `DeleteSnapshotsOption` enumeration.

**1. Dropdown 1: DeleteSnapshotsOption**
This is the name of the Enum type definition located within the `Azure.Storage.Blobs.Models` namespace. Valid C# syntax requires specifying the Enum type before accessing its members (Values).

**2. Dropdown 2: OnlySnapshots**
This specific value instructs the delete operation to remove **only** the snapshots associated with the blob, leaving the base blob intact.

*   **OnlySnapshots:** Deletes only the snapshots. (Correct for this scenario).
*   **IncludeSnapshots:** Deletes the base blob AND all its snapshots. (Incorrect, as the requirement says "must not delete the blob... itself").
*   **None:** Attempts to delete the blob but will fail (throw 409 Conflict) if snapshots exist, effectively preventing deletion rather than managing snapshot deletion.

**Reference:**
*   [DeleteSnapshotsOption Enum - Azure for .NET Developers](https://learn.microsoft.com/en-us/dotnet/api/azure.storage.blobs.models.deletesnapshotsoption)

--------------------------------------------------------------------------------
üìå Question 185 Case Study: VanArsdel, Ltd.
--------------------------------------------------------------------------------
HOTSPOT

-

Case study

-

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study

-

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background

-

VanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.

Current environment

-

Corporate website

-

The company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.

Retail Store Locations

-

The company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.

Requirements

-

The application components must meet the following requirements:

Corporate website

-

‚Ä¢ Secure the website by using SSL.

‚Ä¢ Minimize costs for data storage and hosting.

‚Ä¢ Implement native GitHub workflows for continuous integration and continuous deployment (CI/CD).

‚Ä¢ Distribute the website content globally for local use.

‚Ä¢ Implement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.

‚Ä¢ The website must have 99.95 percent uptime.

Retail store locations

-

‚Ä¢ Azure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.

‚Ä¢ Audit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.

Delivery services

-

‚Ä¢ Store service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.

‚Ä¢ Store delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.

Inventory services

-

The company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to

include read-only access to the data.

Security

-

‚Ä¢ All Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.

‚Ä¢ Authentication and authorization must use Azure AD and services must use managed identities where possible.

Issues

-

Retail Store Locations

-

‚Ä¢ You must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.

‚Ä¢ Azure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.

You need to implement the delivery service telemetry data.

How should you configure the solution? To answer, select the appropriate options in the answer area.

NOTE: Each correct selection is worth one point.


**Azure Cosmos DB: API**
1. Core (SQL)
2. Gremlin
3. Table
4. MongoDB

**Azure Cosmos DB: Partition Key**
1. Item id
2. Vehicle license plate
3. Vehicle package capacity
4. Vehicle location coordinates

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Azure Cosmos DB API:** Core (SQL)
- **Azure Cosmos DB Partition Key:** Vehicle license plate

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. API: Core (SQL)**
*   **Context:** The "Retail store locations" requirement specifically states: "Azure Functions must update Azure Cosmos DB by using **native SQL language queries**." While the Delivery Service requirement doesn't explicitly name the API, consistency across the solution is standard practice unless a specific data model (like Graph or existing MongoDB/Cassandra apps) requires otherwise.
*   **Feature Fit:** The Core (SQL) API is the native API for working with JSON documents and matches the requirement to store structured telemetry data (`item id`, `coordinates`, etc.).

**2. Partition Key: Vehicle license plate**
*   **Entity-Centric Partitioning:** In telemetry and IoT scenarios, the most effective partition key is typically the unique identifier of the device or entity generating the stream. Here, the "entity" is the delivery vehicle.
*   **High Cardinality:** A Partition Key needs a wide range of values to distribute storage and throughput evenly. `Vehicle license plate` is unique to every truck in the global fleet, ensuring excellent distribution (unlike `Vehicle package capacity`, which would only have a few distinct values and cause "hot partitions").
*   **Query Pattern:** Telemetry queries typically ask "Where is Vehicle X?" or "Show me the path of Vehicle Y." Partitioning by license plate keeps all data for a single vehicle together, making these queries efficient.
*   **Why not "Item id"?**: While present in the data, the attributes listed (capacity, coordinates, license plate) overwhelmingly describe the **vehicle**, not the item. The item is likely transient or cargo, whereas the vehicle is the persistent entity being tracked.
*   **Why not "Vehicle location coordinates"?**: Coordinates change constantly and are too granular. Using them as a partition key would result in every single write going to a different partition, making it nearly impossible to efficiently query history (you would have to query *all* partitions to find where a truck has been).

--------------------------------------------------------------------------------
üìå Question 186
--------------------------------------------------------------------------------
You are developing a web application by using the Azure SDK. The web application accesses data in a zone-redundant BlockBlobStorage storage account.

The application must determine whether the data has changed since the application last read the data. Update operations must use the latest data changes when writing data to the storage account.

You need to implement the update operations.

Which values should you use?  

**Code evaluation:**
1. **HTTP Header value**
    *   ETag
    *   Last Modified
    *   VersionId
2. **Conditional header**
    *   If-Match
    *   If-Modified-Since
    *   If-None-Match

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **HTTP Header value:** ETag
- **Conditional header:** If-Match

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. HTTP Header value: ETag**
The **ETag** (Entity Tag) is an identifier assigned by the web server (Azure Storage) to a specific version of a resource found at a URL. Whenever the blob content changes, the ETag value is updated. This is the industry standard for implementing Optimistic Concurrency Control (OCC). By comparing the ETag obtained during the read operation with the current ETag in storage, the application can definitively determine if the data has changed.

**2. Conditional header: If-Match**
To prevent "lost updates" (overwriting someone else's changes), you use the **If-Match** header during the write (PUT) operation. You set the value of this header to the ETag you received when you originally read the data.
*   **How it works:** Azure Storage checks if the ETag provided in the `If-Match` header matches the current ETag of the blob in storage.
*   **Result:**
    *   If they **match**, the write proceeds (the data hasn't changed).
    *   If they **do not match**, the server returns `412 Precondition Failed`. This tells the application that the data *has* changed since it was read, allowing the app to catch the error, re-read the new data, and try the update again.

**Why others are incorrect:**
*   **If-None-Match:** Typically used to ensure an object does *not* exist before creating it, or for caching (GET requests).
*   **If-Modified-Since:** Primarily used for optimizing Read (GET) operations (e.g., "only send me the file if it changed since yesterday").

--------------------------------------------------------------------------------
üìå Question 187
--------------------------------------------------------------------------------
You are developing a solution to store documents in Azure Blob storage. Customers upload documents to multiple containers. Documents consist of PDF, CSV, Microsoft Office format, and plain text files.

The solution must process millions of documents across hundreds of containers.
Requirements:
1. Documents must be categorized by a customer identifier as they are uploaded.
2. Allow filtering by the customer identifier.
3. Allow searching of information contained within a document.
4. Minimize costs.

You create and configure a standard general-purpose v2 storage account to support the solution.

You need to implement the solution.

**Requirement: Search and filter by customer identifier**
1. Azure Cognitive Search
2. Azure Blob index tags
3. Azure Blob inventory policy
4. Azure Blob metadata

**Requirement: Search information inside documents**
1. Azure Cognitive Search
2. Azure Blob index tags
3. Azure Blob inventory policy
4. Azure Blob metadata

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Search and filter by customer identifier:** Azure Blob index tags
- **Search information inside documents:** Azure Cognitive Search

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Search and filter by customer identifier: Azure Blob index tags**
Azure Blob Storage **Index Tags** allow you to categorize data by setting key-value tag attributes (like `CustomerID=123`) on a blob. Crucially, these tags are automatically indexed by Azure, allowing you to find specific blobs across multiple containers dynamically without traversing or listing all blobs. This satisfies the requirement to filter by customer identifier efficiently and effectively.
*   *Why not Metadata?* While you can store the ID in metadata, standard blob metadata is not natively indexed for cross-container filtering in the same way index tags are. Calculating results based on metadata would require slow listing operations.
*   *Why not Inventory Policy?* This is for generating daily/weekly reports on blob data, not for real-time application filtering.

**2. Search information inside documents: Azure Cognitive Search**
To search for information **contained within** a document (content scraping/indexing) for formats like PDF, CSV, and Office files, you must use **Azure Cognitive Search** (now known as Azure AI Search). It enables "document cracking" to extract text and metadata from binary files, creating a searchable index.
*   *Why not Index Tags/Metadata?* These only store properties *about* the file (filename, size, custom tags), they do not index the *contents* of the file (e.g., the text inside a PDF).

**Cost Minimization Note:**
Using **Index Tags** for the filtering requirement allows you to handle the categorization natively within the storage layer, which is significantly cheaper than ingesting all documents into a Search service solely for metadata filtering. You only pay for the Search service for the requirement that necessitates it (content searching).

**References:**
*   [Manage and find data on Azure Blob Storage with Blob Index](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-manage-find-blobs-byte-tag)
*   [Azure AI Search: Blob indexer](https://learn.microsoft.com/en-us/azure/search/search-howto-indexing-azure-blob-storage)

--------------------------------------------------------------------------------
üìå Question 188 
--------------------------------------------------------------------------------
You are developing an inventory tracking solution. The solution includes an Azure Function app containing multiple functions triggered by Azure Cosmos DB. You plan to deploy the solution to multiple Azure regions.

The solution must meet the following requirements:
*   Item results from Azure Cosmos DB must return the **most recent committed version** of an item.
*   Items written to Azure Cosmos DB must provide **ordering guarantees**.

You need to configure the consistency level for the Azure Cosmos DB deployments.

Which consistency level should you use?

- A. consistent prefix
- B. eventual
- C. bounded staleness
- D. strong
- E. session

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
D. **strong**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why D is correct (Strong):**
The requirement "Item results... must return the most recent committed version of an item" is the definition of **Strong Consistency**. In this mode, users are guaranteed to read the latest committed write. If a write is acknowledged, any subsequent read from any region (in a multi-region write or single-region write scenario depending on exact configuration details, though Strong is typically scoped to single-region write accounts or globally distributed accounts willing to pay the high latency cost) will see that value. Additionally, Strong consistency provides strict ordering guarantees (linearizability).

**Why others are incorrect:**
*   **A. Consistent Prefix:** Guarantees that reads never see out-of-order writes, but data might not be the most recent version (it could be stale).
*   **B. Eventual:** Offering the lowest latency, this ensures that eventually, all replicas will converge, but provides no ordering guarantees for reads and allows for significant staleness.
*   **C. Bounded Staleness:** Guarantees that reads are not too far behind writes (based on time or number of versions), but it still allows for *some* staleness. It does not strictly guarantee the *absolute most recent* committed version at the instant of the read like Strong does.
*   **E. Session:** This guarantees "Read Your Own Writes" and Strong consistency *only within the same session*. Other sessions or regions might see stale data, failing the global requirement for the "most recent committed version."

**References:**
*   [Consistency levels in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/consistency-levels)

--------------------------------------------------------------------------------
üìå Question 189
--------------------------------------------------------------------------------
You are developing a static website hosted on Azure Blob Storage. You create a storage account and enable static website hosting.

The website must support the following requirements:
*   Custom domain name
*   Custom header values for all responses
*   Custom SSL certificate

You need to implement the static website.

What should you configure?

**Requirement: Custom domain name**
1. Blob index tags
2. Azure Content Delivery Network (CDN)
3. Cross-Origin Resource Sharing (CORS)
4. Azure Storage Service Encryption (SSE)

**Requirement: Custom header values**
1. Blob index tags
2. Azure Content Delivery Network (CDN)
3. Cross-Origin Resource Sharing (CORS)
4. Azure Storage Service Encryption (SSE)

**Requirement: Custom SSL certificate**
1. Blob index tags
2. Azure Content Delivery Network (CDN)
3. Cross-Origin Resource Sharing (CORS)
4. Azure Storage Service Encryption (SSE)

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Custom domain name:** Azure Content Delivery Network (CDN)
- **Custom header values:** Azure Content Delivery Network (CDN)
- **Custom SSL certificate:** Azure Content Delivery Network (CDN)

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The key to this question is understanding the limitations of Azure Blob Storage's native static website hosting compared to what happens when you front it with **Azure CDN**.

**1. Custom domain name: Azure Content Delivery Network (CDN)**
While Azure Blob Storage natively allows you to map a custom domain (via CNAME), you cannot use that native feature if you also need *HTTPS with a custom SSL certificate* on that domain (without using CDN). However, in the context of the exam options provided, Azure CDN is the standard way to map a custom domain to a static website endpoint to gain performance and other features.

**2. Custom header values: Azure Content Delivery Network (CDN)**
Azure Blob Storage itself does not have a native "Rules Engine" to rewrite headers or inject custom headers (like security headers `X-Frame-Options`, `Content-Security-Policy`, etc.) into every response globally. To do this, you must use **Azure CDN**, which provides a Rules Engine where you can modify response headers.

**3. Custom SSL certificate: Azure Content Delivery Network (CDN)**
This is the most critical constraint. Azure Blob Storage static website hosting natively supports HTTPS only for the default endpoint (`*.web.core.windows.net`). It **does not** natively support custom SSL certificates for custom domains (e.g., `https://www.contoso.com`). To support HTTPS on a custom domain, you **must** use Azure CDN (which can provision a managed certificate or allow you to bring your own) or a similar proxy service like Azure Front Door.

**Why others are incorrect:**
*   **Blob index tags:** Used for categorizing and finding data, not for web serving configuration.
*   **Cross-Origin Resource Sharing (CORS):** Used to allow browsers to make requests from one domain to another. It deals with access control, not domain mapping, SSL, or arbitrary custom headers.
*   **Azure Storage Service Encryption (SSE):** This is for data-at-rest encryption, not for web serving configuration.

**References:**
*   [Integrate a static website with Azure CDN](https://learn.microsoft.com/en-us/azure/storage/blobs/static-website-content-delivery-network)
*   [Map a custom domain with HTTPS enabled](https://learn.microsoft.com/en-us/azure/cdn/cdn-custom-ssl?tabs=option-1-default-enable-https-with-a-cdn-managed-certificate)

--------------------------------------------------------------------------------
üìå Question 190
--------------------------------------------------------------------------------
You develop two Python scripts to process data.

The Python scripts must be deployed to two, separate Linux containers running in an Azure Container Instance container group.
The containers must access external data by using the **Server Message Block (SMB) protocol**.
Containers in the container group **must run only once**.

You need to configure the Azure Container Instance.

Which configuration value should you use?

**Configuration Setting: External data volume**
1. Secret
2. Empty directory
3. Cloned git repo
4. Azure file share

**Configuration Setting: Container restart policy**
1. Never
2. Always
3. OnFailure

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **External data volume:** Azure file share
- **Container restart policy:** Never

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. External data volume: Azure file share**
The requirement specifies that the containers must access external data using the **SMB protocol**. Azure Files offers fully managed file shares in the cloud that are accessible via the industry-standard Server Message Block (SMB) protocol. In Azure Container Instances (ACI), you can mount an Azure file share as a volume to provide persistent, shared storage that uses SMB.

*   *Why others are incorrect:*
    *   **Secret:** Used to pass sensitive information (like passwords/keys) to the container, not for general data storage via SMB.
    *   **Empty directory:** Provides a temporary scratch space that is lost when the container stops; it does not use SMB or connect to external data.
    *   **Cloned git repo:** efficient for initializing a volume with content from a Git repository, but it does not provide SMB access to external data.

**2. Container restart policy: Never**
The requirement states the containers must run **only once**. This indicates a batch processing scenario where the container performs a task and then terminates.
*   **Never:** This policy ensures that the container runs exactly once. Whether it succeeds or fails, ACI will not attempt to restart it. This strictly adheres to the "run only once" constraint.
*   *Why others are incorrect:*
    *   **Always:** Used for long-running services (like web servers). The container would restart immediately after the script finishes, causing an infinite loop.
    *   **OnFailure:** This restarts the container if the script crashes (returns a non-zero exit code). While often used for batch jobs, it technically allows the container to run *more* than once if errors occur, whereas "Never" guarantees a single execution attempt.

**References:**
*   [Mount an Azure file share in Azure Container Instances](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-volume-azure-files)
*   [Container restart policies in Azure Container Instances](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-restart-policy)
