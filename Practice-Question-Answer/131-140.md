--------------------------------------------------------------------------------
üìå Question 131 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You have an Azure Web app that uses Cosmos DB as a data store. You create a CosmosDB container by running the following PowerShell script:

- $resourceGroupName = "testResourceGroup"
- $accountName = "testCosmosAccount"
- $databaseName = "testDatabase"
- $containerName = "testContainer"
- $partitionKeyPath = "/EmployeeId"
- $autoscaleMaxThroughput = 5000
------------------------------------------------------------------------------
- New-AzCosmosDBSqlContainer -
- ResourceGroupName $resourceGroupName
- AccountName $accountName
- DatabaseName $databaseName
- Name $containerName
- PartitionKeyKind Hash
- PartitionKeyPath $partitionKeyPath
- AutoscaleMaxThroughput $autoscaleMaxThroughput

You create the following queries that target the container:
1. SELECT * FROM c WHERE c.EmployeeId > '12345'
2. SELECT * FROM c WHERE c.UserID = '12345'

For each of the following statements, select Yes if the statement is true. Otherwise, select No.

**Statements:**
1. The minimum throughput for the container is 400 R/Us. (Yes/No)
2. The first query statement is an in-partition query. (Yes/No)
3. The second query statement is a cross-partition query. (Yes/No)

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **The minimum throughput for the container is 400 R/Us.** -> ‚ùå **No**
2. **The first query statement is an in-partition query.** -> ‚ùå **No**
3. **The second query statement is a cross-partition query.** -> ‚úÖ **Yes**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. The minimum throughput for the container is 400 R/Us: NO**
When using **Autoscale** throughput in Azure Cosmos DB, the throughput scales between range `T` to `10% of T`. The script sets `$autoscaleMaxThroughput = 5000`.
Therefore, the range is **500 RU/s** to **5000 RU/s**. The minimum throughput is 10% of the maximum, which is 500 RU/s, not 400 RU/s. (Note: 400 RU/s is the minimum for standard *manual* throughput, but autoscale math is strictly `0.1 * Max`).

**2. The first query statement is an in-partition query: NO**
Query: `SELECT * FROM c WHERE c.EmployeeId > '12345'`
The partition key is `/EmployeeId`.
An **in-partition** query must target a *specific* partition key value (Where EmployeeId = 'Value').
Because this query uses a range operator (`>`), Cosmos DB cannot direct this to a single logical partition. It must check all partitions that might contain values greater than '12345'. Therefore, this is treated as a cross-partition query (fan-out), not a single-partition query.

**3. The second query statement is a cross-partition query: YES**
Query: `SELECT * FROM c WHERE c.UserID = '12345'`
The partition key is `/EmployeeId`.
The query filters by `UserID`, not the partition key (`EmployeeId`).

Since the query does not provide the partition key value, Cosmos DB does not know which specific partition holds the data. Consequently, it must broadcast the query to **all** partitions to find the matching UserID. This is the definition of a cross-partition query.

**References:**
*   [Azure Cosmos DB Autoscale Throughput](https://learn.microsoft.com/en-us/azure/cosmos-db/provision-throughput-autoscale#how-autoscale-works)
*   [Partitioning and horizontal scaling in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/partitioning-overview)

--------------------------------------------------------------------------------
üìå Question 132
--------------------------------------------------------------------------------
You are developing a solution that will use a multi-partitioned Azure Cosmos DB database. You plan to use the latest Azure Cosmos DB SDK for development.

The solution must meet the following requirements:
*   Send insert and update operations to an Azure Blob storage account.
*   Process changes to all partitions immediately.
*   Allow parallelization of change processing.

You need to process the Azure Cosmos DB operations.

What are two possible ways to achieve this goal? Each correct answer presents a complete solution.

- A. Create an Azure App Service API and implement the change feed estimator of the SDK. Scale the API by using multiple Azure App Service instances.
- B. Create a background job in an Azure Kubernetes Service and implement the change feed feature of the SDK.
- C. Create an Azure Function to use a trigger for Azure Cosmos DB. Configure the trigger to connect to the container.
- D. Create an Azure Function that uses a FeedIterator object that processes the change feed by using the pull model on the container. Use a FeedRange object to parallelize the processing of the change feed across multiple functions.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. Create a background job in an Azure Kubernetes Service and implement the change feed feature of the SDK.**
**C. Create an Azure Function to use a trigger for Azure Cosmos DB. Configure the trigger to connect to the container.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To process the Azure Cosmos DB Change Feed efficiently with support for parallelization and immediate processing, you generally use the **Change Feed Processor** (Push Model). This component manages the logic for watching partitions and distributing the work across multiple clients.

**1. Option B: Background Job in AKS (Change Feed Processor)**
The Azure Cosmos DB SDK includes the **Change Feed Processor** library. This library is designed to run in a host application (like a customized background worker containerized in AKS). 
*   **Parallelization:** The processor automatically creates leases for partitions and distributes them among all running instances (pods). If you scale your AKS pods, the processor rebalances the work, fulfilling the requirement for parallelization.
*   **Immediate Processing:** It listens for changes and pushes them to the delegate method immediately.

**2. Option C: Azure Functions Trigger**
The Azure Functions Trigger for Cosmos DB is essentially a serverless wrapper around the Change Feed Processor.
*   **Parallelization:** The Azure Functions platform automatically scales out the number of function instances based on the volume of changes pending in the change feed.
*   **Simplicity:** It connects directly to the monitored container and handles the "plumbing" (leases and state) automatically.

**Why others are incorrect:**
*   **A:** The **Change Feed Estimator** is a side-car tool used to calculate "lag" (how far behind the processor is). It measures pending work but does not *process* the work itself. You cannot use it to send data to Blob Storage.
*   **D:** The **Pull Model** (`FeedIterator`) puts the burden of state management (continuation tokens) and parallelization orchestration on the developer. While technically possible to build a custom parallel solution with `FeedRange`, it is significantly more complex and manual than using the Push Model (CFP) or the Trigger, which handle parallelization natively out-of-the-box.

**References:**
*   [Change feed processor in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/change-feed-processor)
*   [Serverless event-based architectures with Azure Cosmos DB and Azure Functions](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/change-feed-functions)

--------------------------------------------------------------------------------
üìå Question 133
--------------------------------------------------------------------------------
You are developing an application that uses a premium block blob storage account and have configured the following Lifecycle Management policy (JSON provided in snippet).
```json
{
  "rules": [
    {
      "name": "agingDataRule",
      "enabled": true,
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": ["blockBlob"],
          "prefixMatch": [
            "container1/salesorders",
            "container2/inventory"
          ]
        },
        "actions": {
          "baseBlob": {
            "tierToCool": {
              "daysAfterModificationGreaterThan": 60
            },
            "tierToArchive": {
              "daysAfterModificationGreaterThan": 120
            }
          }
        }
      }
    },
    {
      "enabled": true,
      "name": "lastAccessedDataRule",
      "type": "Lifecycle",
      "definition": {
        "actions": {
          "baseBlob": {
            "enableAutoTierToHotFromCool": true,
            "tierToCool": {
              "daysAfterLastAccessTimeGreaterThan": 30
            }
          }
        },
        "filters": {
          "blobTypes": ["blockBlob"]
        }
      }
    },
    {
      "rules": [
        {
          "name": "expirationDataRule",
          "enabled": true,
          "type": "Lifecycle",
          "definition": {
            "filters": {
              "blobTypes": ["blockBlob"]
            },
            "actions": {
              "baseBlob": {
                "delete": {
                  "daysAfterModificationGreaterThan": 730
                }
              }
            }
          }
        }
      ]
    }
  ]
}

```
**Policy Review:**
*   **Rule 1 (`agingDataRule`):** Targeted at blobs in `container1/salesorders` and `container2/inventory`. Moves to **Cool** after 60 days modified; moves to **Archive** after 120 days modified.
*   **Rule 2 (`lastAccessedDataRule`):** Targeted at *all* block blobs (no prefix filter). Moves to **Cool** if not accessed for 30 days. Enables auto-tiering back to **Hot** on access.
*   **Rule 3 (`expirationDataRule`):** Targeted at *all* block blobs. Deletes after 730 days modified.

**Statements:**
1.  Block blobs prefixed with container1/salesorders or container2/inventory which have not been modified in over 60 days are moved to cool storage. Blobs that have not been modified in 120 days are moved to the archive tier. (Yes/No)
2.  Blobs are moved to cool storage if they have not been accessed for 30 days. (Yes/No)
3.  Blobs will automatically be tiered from cool back to hot if accessed again after being tiered to cool. (Yes/No)
4.  All block blobs older than 730 days will be deleted. (Yes/No)

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1.  **Block blobs prefixed with... moved to cool storage... moved to the archive tier:** -> ‚úÖ **Yes**
2.  **Blobs are moved to cool storage if they have not been accessed for 30 days:** -> ‚úÖ **Yes**
3.  **Blobs will automatically be tiered from cool back to hot if accessed again...:** -> ‚úÖ **Yes**
4.  **All block blobs older than 730 days will be deleted:** -> ‚ùå **No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Rule 1 (`agingDataRule`): YES**
The JSON explicitly defines a rule for blobs matching the prefixes `container1/salesorders` and `container2/inventory`.
*   `tierToCool`: `{ "daysAfterModificationGreaterThan": 60 }`
*   `tierToArchive`: `{ "daysAfterModificationGreaterThan": 120 }`
This statement is a direct translation of the first rule definition.

**2. Rule 2 (`lastAccessedDataRule`): YES**
This rule collects all block blobs (`filters: { "blobTypes": ["blockBlob"] }`) without a prefix constraint, making it valid for the general storage account.
*   `tierToCool`: `{ "daysAfterLastAccessTimeGreaterThan": 30 }`
While blobs matched by Rule 1 are excluded from this rule (due to matching the first rule), this statement describes the active behavior of the configured policy for the rest of the separate data in the account.

**3. Auto-Tiering (`enableAutoTierToHotFromCool`): YES**
In the `lastAccessedDataRule`, the property `"enableAutoTierToHotFromCool": true` is set. This enables the specific feature where accessing a blob (reading/writing) while it is in the Cool tier automatically moves it back to the Hot tier.

**4. Rule 3 (`expirationDataRule`) and Deletion: NO**
Azure Lifecycle Management rules are executed in **sequential order** (Top to Bottom).
1.  **Rule 1** catches and processes specific prefixes.
2.  **Rule 2** acts as a "Catch-All" because it has no prefix filter (it matches all block blobs). Any blob not caught by Rule 1 will match Rule 2.
3.  **Rule 3** is **unreachable (dead code)**. Because Rule 2 matches "all block blobs" and Azure ignores subsequent rules once a blob finds a match, no blob will ever reach the `expirationDataRule`. Consequently, no files will ever be deleted by this policy. Furthermore, the statement claims "**All** block blobs"‚Äîblobs in `container1` (Rule 1) definitely do not include a delete action, making the statement false on multiple counts.

**References:**
*   [Optimize costs by automatically managing the data lifecycle](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview#rule-evaluation-order)
*   [Lifecycle management policy definition JSON](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-json)

--------------------------------------------------------------------------------
üìå Question 134
--------------------------------------------------------------------------------
You develop and deploy an Azure Logic app that calls an Azure Function app. The Azure Function app includes an OpenAPI (Swagger) definition and uses an Azure Blob storage account. All resources are secured by using Azure Active Directory (Azure AD).

The Azure Logic app must securely access the Azure Blob storage account. Azure AD resources must remain if the Azure Logic app is deleted.

You need to secure the Azure Logic app.

What should you do?

- A. Create a user-assigned managed identity and assign role-based access controls.
- B. Create an Azure AD custom role and assign the role to the Azure Blob storage account.
- C. Create an Azure Key Vault and issue a client certificate.
- D. Create a system-assigned managed identity and issue a client certificate.
- E. Create an Azure AD custom role and assign role-based access controls.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Create a user-assigned managed identity and assign role-based access controls.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Managed Identities for Azure Resources**
Managed identities are the recommended way to secure communication between Azure services (like Logic Apps and Blob Storage). They provide an identity for applications to use when connecting to resources that support Azure AD authentication, eliminating the need to manage credentials (like connection strings or secrets).

**2. User-Assigned vs. System-Assigned**
The critical requirement in the question is: **"Azure AD resources must remain if the Azure Logic app is deleted."**

*   **System-assigned managed identity:** The identity is created as part of the Azure resource (the Logic App). It shares the lifecycle of the resource. If you delete the Logic App, Azure automatically deletes the identity. This **violates** the requirement.
*   **User-assigned managed identity:** The identity is created as a standalone Azure resource. It has its own lifecycle independent of the Logic App. If you delete the Logic App, the user-assigned identity **remains**. This **satisfies** the requirement.

**3. Role-Based Access Control (RBAC)**
Once the user-assigned identity is created and assigned to the Logic App, you grant it access to the Blob Storage by assigning an appropriate RBAC role (e.g., "Storage Blob Data Contributor") to that identity on the storage account scope.

**Why others are incorrect:**
*   **B & E:** Creating custom roles defines *permissions*, but it does not create an *identity* for the Logic App to use. You still need a principal (user, service principal, or managed identity) to assign that role to.
*   **C:** Storing certificates in Key Vault is a valid security pattern but adds management overhead (rotating secrets/certificates). Managed identities are preferred for Azure-to-Azure communication.
*   **D:** As noted above, a system-assigned identity would be deleted along with the Logic App, failing the specific lifecycle requirement.

**References:**
*   [What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)
*   [Managed identities for Azure Logic Apps](https://learn.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity)

--------------------------------------------------------------------------------
üìå Question 135
--------------------------------------------------------------------------------
You are developing an Azure-hosted application that must use an on-premises hardware security module (HSM) key.

The key must be transferred to your existing Azure Key Vault by using the Bring Your Own Key (BYOK) process.

You need to securely transfer the key to Azure Key Vault.

Which four actions should you perform in sequence?

**Available Actions:**
*   Generate a key transfer blob file by using the HSM vendor-provided tool.
*   Generate a Key Exchange Key (KEK).
*   Create a custom policy definition in Azure Policy.
*   Run the `az keyvault key import` command.
*   Run the `az keyvault key restore` command.
*   Retrieve the Key Exchange Key (KEK) public key.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**Step 1:** Generate a Key Exchange Key (KEK).
**Step 2:** Retrieve the Key Exchange Key (KEK) public key.
**Step 3:** Generate a key transfer blob file by using the HSM vendor-provided tool.
**Step 4:** Run the `az keyvault key import` command.

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The "Bring Your Own Key" (BYOK) process for Azure Key Vault allows you to generate a key in your on-premises HSM and securely transfer it to Azure Key Vault. The key never leaves the HSM boundary in plain text.

**The Sequential Workflow:**

1.  **Generate a Key Exchange Key (KEK):**
    First, you must create a Key Exchange Key (KEK) in your Azure Key Vault. This is an RSA key that will be used to encrypt the key you are about to transfer. The KEK ensures your key remains encrypted securely until it reaches the Azure HSM.

2.  **Retrieve the Key Exchange Key (KEK) public key:**
    You need to download the public portion of the KEK (as a `.pem` file) from Azure Key Vault. This public key will be used by your on-premises device to wrap (encrypt) your target key.

3.  **Generate a key transfer blob file by using the HSM vendor-provided tool:**
    Using the HSM vendor's specific tool and the downloaded KEK public key, you export the target key from your on-premises HSM. The tool creates a "transfer blob" (usually a `.byok` file) which contains your key encrypted by the KEK.

4.  **Run the `az keyvault key import` command:**
    Finally, you upload this encrypted transfer blob to Azure Key Vault. Azure uses the private portion of the KEK (which never left the Azure HSM) to decrypt the blob and import your key. The command explicitly uses the `--kty` (key type) and guarantees the key is imported as an HSM-protected key.

**Why others are incorrect/not used:**
*   **Run the `az keyvault key restore` command:** Restore is used for backing up and restoring keys between Azure Key Vaults (e.g., for disaster recovery), typically using previously created backup blobs. It is not part of the standard BYOK import flow.
*   **Create a custom policy definition in Azure Policy:** While Azure Policy is useful for governance, it is not a functional step required to perform the cryptographic transfer of a key.

**References:**
*   [Specification for Bring Your Own Key(BYOK)](https://learn.microsoft.com/en-us/azure/key-vault/keys/byok-specification)
*   [Import HSM-protected keys to Key Vault (BYOK)](https://learn.microsoft.com/en-us/azure/key-vault/keys/hsm-protected-keys-byok)

--------------------------------------------------------------------------------
üìå Question 136 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You develop a REST API. You implement a user delegation SAS token to communicate with Azure Blob storage.

The token is compromised.

You need to revoke the token.

What are two possible ways to achieve this goal? Each correct answer presents a complete solution.

- A. Revoke the delegation key.
- B. Delete the stored access policy.
- C. Regenerate the account key.
- D. Remove the role assignment for the security principle.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Revoke the delegation key.**
- **D. Remove the role assignment for the security principle.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
A **User Delegation SAS** is secured using Azure Active Directory (Azure AD/Entra ID) credentials rather than the storage account keys. To create one, an Azure AD security principal requests a *user delegation key* from the storage account, and that key is used to sign the SAS.

**1. Revoke the delegation key (Option A)**
Because the SAS token is essentially "signed" by the specific user delegation key, revoking that key immediately invalidates all SAS tokens associated with it. You can perform this action via Azure PowerShell (`Revoke-AzStorageAccountUserDelegationKeys`) or the Azure CLI (`az storage account revoke-delegation-keys`). By default, these keys are valid for 7 days, but revoking them cuts access immediately.

**2. Remove the role assignment (Option D)**
When a client uses a user delegation SAS to access a blob, Azure Storage validates two things:
1.  Verify the SAS signature using the user delegation key.
2.  Verify that the security principal (the user or service principal backing the delegation key) *still* has the appropriate RBAC permissions (e.g., "Storage Blob Data Reader") to access the requested resource.
If you remove the role assignment (RBAC) for that security principal, the SAS token will be rejected upon use because the backing identity no longer has permission to the data, effectively revoking access.

**Why others are incorrect:**
*   **B. Delete the stored access policy:** Stored access policies allows you to group and revoke **Service SAS** tokens (which are signed with the storage account key). They are **not** supported for User Delegation SAS tokens.
*   **C. Regenerate the account key:** User Delegation SAS tokens are not signed by the storage account master keys (unlike Account SAS or Service SAS). Regenerating the master keys has no effect on a user delegation SAS.

**References:**
*   [Grant limited access to Azure Storage resources using SAS](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview#revoke-a-user-delegation-sas)
*   [Create a user delegation SAS](https://learn.microsoft.com/en-us/rest/api/storageservices/create-user-delegation-sas)

--------------------------------------------------------------------------------
üìå Question
--------------------------------------------------------------------------------
You are developing an Azure App Service REST API.

The API must be called by an Azure App Service web app. The API must retrieve and update user profile information stored in Azure Active Directory (Azure AD).

You need to configure the API to make the updates.

Which two tools should you use? Each correct answer presents part of the solution.

- A. Microsoft Graph API
- B. Microsoft Authentication Library (MSAL)
- C. Azure API Management
- D. Microsoft Azure Security Center
- E. Microsoft Azure Key Vault SDK

--------------------------------------------------------------------------------
‚úÖ Correct Answer 137
--------------------------------------------------------------------------------
- **A. Microsoft Graph API**
- **B. Microsoft Authentication Library (MSAL)**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Microsoft Graph API**
Microsoft Graph is the gateway to data and intelligence in Microsoft 365. It provides a unified programmability model that you can use to access the tremendous amount of data in Microsoft 365, Windows 10, and Enterprise Mobility + Security. Specifically, to **retrieve and update user profile information** stored in Azure Active Directory (now Microsoft Entra ID), your application must call the endpoints exposed by the Microsoft Graph API (e.g., `PATCH /users/{id-or-upn}`).

**2. Microsoft Authentication Library (MSAL)**
To call the Microsoft Graph API, your application must acquire an access token from the Microsoft identity platform. **MSAL** is the specific library provided by Microsoft to enable developers to acquire tokens from the Microsoft identity platform in order to authenticate users and access secured web APIs (like Microsoft Graph). It handles the complex details of OAuth 2.0 flows, caching, and token refreshing.

**Why others are incorrect:**
*   **C. Azure API Management:** This is a gateway service for publishing APIs to external/internal developers. It manages the traffic to your API but does not provide the capability to read/write internal Azure AD user data.
*   **D. Microsoft Azure Security Center:** This is a tool for security posture management and threat protection, not for application logic or directory updates.
*   **E. Microsoft Azure Key Vault SDK:** While Key Vault might be used to securely store the client secret used by MSAL, it is not the primary tool for performing the update or handling the identity negotiation required to talk to Azure AD.

**References:**
*   [Overview of Microsoft Graph](https://learn.microsoft.com/en-us/graph/overview)
*   [Overview of the Microsoft Authentication Library (MSAL)](https://learn.microsoft.com/en-us/entra/identity-platform/msal-overview)

--------------------------------------------------------------------------------
üìå Question 138 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing an Azure solution.
You need to develop code to access a secret stored in Azure Key Vault.

How should you complete the code segment?

`string var1 = Environment.GetEnvironmentVariable("KEY_VAULT_URI");`
`var var2 = new _________________ ( new Uri(var1), new _________________ ());`

**Available Options:**
*   DefaultAzureCredential
*   ClientSecretCredential
*   CloudClients
*   SecretClient

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Box 1:** `SecretClient`
- **Box 2:** `DefaultAzureCredential`

The completed code looks like this:
`var var2 = new SecretClient(new Uri(var1), new DefaultAzureCredential());`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Box 1: SecretClient**
To interact with secrets in Azure Key Vault using the Azure SDK for .NET (`Azure.Security.KeyVault.Secrets`), you instantiate the **`SecretClient`** class. This client is specifically designed to set, retrieve, and delete secrets. The constructor requires the Vault URI and a credential object.

**2. Box 2: DefaultAzureCredential**
The second parameter of the `SecretClient` constructor expects a class that implements `TokenCredential`.
*   **`DefaultAzureCredential`** is the standard class from the `Azure.Identity` library. It provides a seamless authentication flow by trying multiple methods in order (Environment Variables -> Managed Identity -> Visual Studio -> Azure CLI -> etc.). Crucially, it has a **parameterless constructor** (`new DefaultAzureCredential()`), which matches the syntax in the code snippet.
*   `ClientSecretCredential` is a valid credential type but requires arguments (TenantId, ClientId, ClientSecret) for its constructor, so it does not fit the code snippet `new ... ()`.

**References:**
*   [Quickstart: Azure Key Vault secret client library for .NET](https://learn.microsoft.com/en-us/azure/key-vault/secrets/quick-create-net)
*   [DefaultAzureCredential Class](https://learn.microsoft.com/en-us/dotnet/api/azure.identity.defaultazurecredential)

--------------------------------------------------------------------------------
üìå Question 139 
--------------------------------------------------------------------------------
You develop and deploy an Azure App Service web app named App1. You create a new Azure Key Vault named Vault1. You import several API keys, passwords, certificates, and cryptographic keys into Vault1.

You need to grant App1 access to Vault1 and automatically rotate credentials. Credentials must not be stored in code.

What should you do?

- A. Enable App Service authentication for Appl. Assign a custom RBAC role to Vault1.
- B. Add a TLS/SSL binding to App1.
- C. Upload a self-signed client certificate to Vault1. Update App1 to use the client certificate.
- D. Assign a managed identity to App1.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. Assign a managed identity to App1.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Managed Identities for Azure Resources**
The core requirement is to access Azure Key Vault without storing credentials in code. **Managed Identities** allow Azure resources (like App Service) to authenticate to other services (like Key Vault) using an identity managed by Azure AD. No secrets or connection strings need to be stored in the application code or configuration.

**2. Key Vault Access Policy or RBAC**
Once the App Service has a managed identity, you grant that identity access to `Vault1` (e.g., via an Access Policy or Role-Based Access Control) to read secrets, keys, and certificates.

**3. Automatic Rotation**
Key Vault supports automatic rotation for secrets and keys. Because the App Service uses its Managed Identity to fetch the *current* version of a secret at runtime, the application automatically receives the new value after rotation without needing code changes or redeployments (assuming the app code is written to fetch the secret on demand or refresh its cache).

**Why others are incorrect:**
*   **A. Enable App Service authentication:** This feature (Easy Auth) is for authenticating *users* trying to access the web app (e.g., via Google, Facebook, or Azure AD). It is not the mechanism used for the app to authenticate *itself* to backend services like Key Vault.
*   **B & C:** Using client certificates involves managing the certificate lifecycle, uploading it, and potentially storing the certificate or its thumbprint in configuration‚Äîviolating the "no credentials in code" spirit compared to the seamless nature of Managed Identities.

**References:**
*   [Use a managed identity for App Service and Azure Functions](https://learn.microsoft.com/en-us/azure/app-service/overview-managed-identity)
*   [Key Vault references for App Service and Azure Functions](https://learn.microsoft.com/en-us/azure/app-service/app-service-key-vault-references)

--------------------------------------------------------------------------------
üìå Question 140
--------------------------------------------------------------------------------
You develop and deploy a web app to Azure App service. The web app allows users to authenticate by using social identity providers through the Azure B2C service. All user profile information is stored in Azure B2C.

You must update the web app to display common user properties from Azure B2C to include the following information:
‚Ä¢ Email address
‚Ä¢ Job title
‚Ä¢ First name
‚Ä¢ Last name
‚Ä¢ Office location

You need to implement the user properties in the web app.

Which code library and API should you use?

**Requirement 1: API to access user properties**
1. Microsoft Graph
2. Azure AD Graph
3. Azure Key Vault
4. Azure AD entitlement management

**Requirement 2: Code library to interface to Azure AD B2C**
1. Microsoft Authentication Library (MSAL)
2. Microsoft Azure Key Vault SDK
3. Azure Identity library

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **API to access user properties:** Microsoft Graph
2. **Code library to interface to Azure AD B2C:** Microsoft Authentication Library (MSAL)

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. API to access user properties: Microsoft Graph**
**Microsoft Graph** is the unified API endpoint (`graph.microsoft.com`) used to access data across Microsoft 365 services, including Azure Active Directory (Entra ID) and **Azure AD B2C**. It is the modern replacement for the legacy **Azure AD Graph API** (which is deprecated). To retrieve user attributes like Job Title, Office Location, or custom attributes stored in the B2C directory after a user logs in, the web app must query the Microsoft Graph API.

*   **Azure Key Vault** is used for secret management, not user profiles.
*   **Azure AD entitlement management** is for governance (access packages), not reading user profile fields.

**2. Code library to interface to Azure AD B2C: Microsoft Authentication Library (MSAL)**
**MSAL** (Microsoft Authentication Library) is the standard library designed to support the Microsoft identity platform (including Azure AD B2C). It handles the complex authentication protocols (OpenID Connect, OAuth 2.0), manages user sessions, and‚Äîcrucially‚Äîacquires the **access tokens** needed to call the Microsoft Graph API securely.

*   **Microsoft Azure Key Vault SDK** is for interacting with vaults, not B2C policies.
*   **Azure Identity library** is generally used for service-to-service authentication (like Managed Identities) or unified client authentication in the Azure SDKs. While powerful, **MSAL** is the specific library optimized for handling the user flows, redirects, and token acquisition scenarios specific to Azure B2C and social identity providers.

**References:**
*   [Overview of Microsoft Graph in Azure AD B2C](https://learn.microsoft.com/en-us/azure/active-directory-b2c/microsoft-graph-operations)
*   [Microsoft Authentication Library (MSAL) overview](https://learn.microsoft.com/en-us/entra/identity-platform/msal-overview)
