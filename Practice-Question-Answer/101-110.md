
--------------------------------------------------------------------------------
üìå Question 101 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You develop a web application that sells access to last-minute openings for child camps that run on the weekends. The application uses Azure Application Insights for all alerting and monitoring.

The application must alert operators when a technical issue is preventing sales to camps.

You need to build an alert to detect technical issues.

Which alert type should you use?

- A. Metric alert using multiple time series
- B. Metric alert using dynamic thresholds
- C. Log alert using multiple time series
- D. Log alert using dynamic thresholds

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. Metric alert using dynamic thresholds**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Metric alert using dynamic thresholds is correct:**
The application has a highly variable and seasonal traffic pattern ("last-minute openings", "weekends").
1.  **Dynamic Thresholds:** Standard static thresholds (e.g., "alert if sales < 5") often cause false positives during off-hours or false negatives during peak hours. **Dynamic thresholds** use advanced machine learning algorithms to learn the application's historical behavior and seasonality (e.g., knowing that high traffic is expected on Friday mornings but low traffic on Monday nights).
2.  **Detecting Technical Issues:** If a technical issue prevents sales, the metric for "Completed Sales" or "Requests" will drop significantly below the *expected* range calculated by the ML model. The alert triggers automatically when the metric deviates from this learned pattern, effectively catching the outage without manual threshold tuning.

**Why others are incorrect:**
*   **A. Metric alert using multiple time series:** This feature allows monitoring multiple resources or dimensions (like ensuring every individual server CPU is low), but it typically relies on static thresholds. It does not inherently solve the problem of defining "what is normal" for a seasonal sales pattern.
*   **C & D. Log alerts:** Log alerts rely on running KQL queries at set intervals. While powerful for complex logic, they are generally slower (latency) than metric alerts and are less suited for real-time anomaly detection on standard volume metrics compared to the built-in Dynamic Thresholds feature.

**References:**
*   [Metric Alerts with Dynamic Thresholds in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-dynamic-thresholds)

--------------------------------------------------------------------------------
üìå Question 102 Case Study
--------------------------------------------------------------------------------

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study -

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background -

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment -

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website -

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms -

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors -

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements -

The application components must meet the following requirements:

Corporate website -

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms -

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors -

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff -

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security -

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues -

Corporate website -

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors -

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to implement an aggregate of telemetry values for distributor API calls.

Which Application Insights API method should you use?

**Scenario Context:**
*   Distributors integrate their applications via APIs.
*   **Requirement:** The company must track a custom telemetry value with each API call. These values must be charted to evaluate variations and trends.
*   **Issue:** Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis.


- A. TrackEvent
- B. TrackDependency
- C. TrackMetric
- D. TrackException
- E. TrackTrace

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. TrackMetric**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why TrackMetric is correct:**
To address the issue of high telemetry volume and cost while maintaining statistical accuracy (mean, standard deviation, min, max), you should use **Metrics**.
*   **Aggregation:** The `TrackMetric` method (or the `GetMetric` pattern in newer SDKs) allows the Application Insights SDK to aggregate measurements locally (in memory) over a time window (typically 1 minute) before sending a single summary telemetry item to Azure.
*   **Efficiency:** Instead of sending 1,000 individual log entries for 1,000 API calls (which `TrackEvent` or `TrackTrace` would do), `TrackMetric` sends 1 object containing the count (1,000), sum, average, etc. This satisfies the requirement to "reduce telemetry traffic... while preserving a statistically correct analysis."
*   **Charting:** Metrics are specifically designed for efficient charting of numerical trends and variations over time.

**Why others are incorrect:**
*   **A. TrackEvent:** Used for counting usage patterns (e.g., "Button Clicked"). While valuable, sending a `TrackEvent` for every individual high-frequency numeric sample is inefficient and costly compared to pre-aggregated metrics.
*   **B. TrackDependency:** Used for measuring calls to external systems (databases, HTTP services), not for custom internal logic values.
*   **D. TrackException:** Used for logging errors and stack traces.
*   **E. TrackTrace:** Used for unstructured text logs (printf/console.log style debugging), which is the most expensive and least efficient way to track numerical trends.

**References:**
*   [Application Insights API for custom events and metrics: TrackMetric](https://learn.microsoft.com/en-us/azure/azure-monitor/app/api-custom-events-metrics#trackmetric)
*   [GetMetric vs TrackMetric](https://learn.microsoft.com/en-us/azure/azure-monitor/app/api-dotnet-metrics)

--------------------------------------------------------------------------------
üìå Question 103 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You have an Azure API Management (APIM) Standard tier instance named APIM1 that uses a managed gateway.

You plan to use APIM1 to publish an API named API1 that uses a backend database that supports only a limited volume of requests per minute. You also need a policy for API1 that will minimize the possibility that the number of requests to the backend database from an individual IP address you specify exceeds the supported limit.

You need to identify a policy for API1 that will meet the requirements.

**Which policy should you use?**

A. ip-filter
B. quota-by-key
C. rate-limit-by-key
D. rate-limit

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. rate-limit-by-key**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Why "rate-limit-by-key" is the correct choice:**
The requirement is to limit the **rate** of requests (requests per minute) specifically based on the caller's **IP address**.
*   **`rate-limit-by-key`** allows you to define a flexible `counter-key` using policy expressions. You can set the key to `@(context.Request.IpAddress)` to track and throttle usage per individual IP.
*   It is designed specifically to prevent API usage spikes (throttling) over short time windows (seconds or minutes), which matches the requirement to protect a database with limited throughput.

**2. Why other options are incorrect:**
*   **A. `ip-filter`**: This policy is used for security access control (allowing or denying specific IP ranges entirely). It does not count requests or throttle based on volume; it simply acts as a firewall rule.
*   **B. `quota-by-key`**: Quotas are typically used for monitoring aggregate usage over longer periods (business/monetization limits, e.g., "10,000 calls per month"). While it *can* be configured for shorter periods, it is intended to enforce a hard cap on total volume, whereas rate limiting is designed for short-term traffic shaping to prevent backend overwhelming (returning 429 Too Many Requests).
*   **D. `rate-limit`**: This policy restricts call rates based on the subscription key context. It does not allow you to define an arbitrary key (like an IP address) for the counter. If you used this, you would be throttling the specific user/subscription, not necessarily the specific source IP address.

**Example Policy Implementation:**
```xml
<rate-limit-by-key calls="100"
                   renewal-period="60"
                   counter-key="@(context.Request.IpAddress)" />
```
--------------------------------------------------------------------------------
üìå Question 104
--------------------------------------------------------------------------------
After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.

You are developing a medical records document management website. The website is used to store scanned copies of patient intake forms.

If the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised.

You need to store the intake forms according to the requirements.

**Solution:**

1. Create an Azure Key Vault key named skey.
2. Encrypt the intake forms using the public key portion of skey.
3. Store the encrypted data in Azure Blob storage.

**Does the solution meet the goal?**

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Yes**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why "Yes" is the correct answer:**

The requirement states that even if the files are downloaded from storage by a third party (i.e., data exfiltration or a storage breach), the contents must remain uncompromised.

The proposed solution implements **Client-Side Encryption** using an asymmetric key pair stored in Azure Key Vault:

1.  **Encryption:** By encrypting the data *before* it is uploaded to Azure Blob Storage (using the public key of `skey`), the data stored in the cloud is ciphertext, not cleartext.
2.  **Protection:** Because the data is encrypted at the object level, anyone who downloads the blob‚Äîwhether an authorized user or a malicious third party‚Äîreceives only the encrypted bytes.
3.  **Decryption Control:** Decryption is only possible for services or users who have been granted explicit permissions to access the private key portion of `skey` inside Azure Key Vault. A third party downloading the blob from storage would lack this access and thus cannot read the form.

This specifically meets the requirement of protecting the content even if the storage layer is accessed.

**References:**
*   [Client-side encryption with .NET for Azure Storage](https://learn.microsoft.com/en-us/azure/storage/common/storage-client-side-encryption)


--------------------------------------------------------------------------------
üìå Question 105 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You have an application that includes an Azure Web app and several Azure Function apps. Application secrets including connection strings and certificates are stored in Azure Key Vault.

Secrets must not be stored in the application or application runtime environment. Changes to Azure Active Directory (Azure AD) must be minimized.

You need to design the approach to loading application secrets.

**What should you do?**

- A. Create a single user-assigned Managed Identity with permission to access Key Vault and configure each App Service to use that Managed Identity.
- B. Create a single Azure AD Service Principal with permission to access Key Vault and use a client secret from within the App Services to access Key Vault.
- C. Create a system assigned Managed Identity in each App Service with permission to access Key Vault.
- D. Create an Azure AD Service Principal with Permissions to access Key Vault for each App Service and use a certificate from within the App Services to access Key Vault.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Create a single user-assigned Managed Identity with permission to access Key Vault and configure each App Service to use that Managed Identity.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why "User-assigned Managed Identity" is the correct choice:**

1.  **No Secrets in Application:** Managed Identities eliminate the need to manage credentials (like client secrets or certificates) within your application code or configuration. The Azure platform handles the identity authentication automatically. This meets the requirement that *secrets must not be stored in the application*.
2.  **Minimizing Azure AD Changes:** The question involves multiple resources (one Web App and "several" Function Apps).
    *   **User-assigned identities** are standalone Azure resources. You create **one** identity and assign it to all your App Services. This results in only **one** object being created in Azure AD (one Service Principal).
    *   **System-assigned identities** (Option C) act per resource. Enabling it on 5 apps creates 5 separate Service Principals in Azure AD. Therefore, a single User-assigned identity minimizes changes to AD compared to System-assigned.

**Why other options are incorrect:**

*   **B & D. Service Principals:** Both require you to store credentials (either a Client Secret or a Certificate) within the application configuration so the app can authenticate as the Service Principal. This violates the requirement: *"Secrets must not be stored in the application or application runtime environment."*
*   **C. System-assigned Managed Identity:** While this creates a secure, secret-less configuration, it creates a unique identity in Azure AD for *every* App Service instance. If you have many functions, this clutters Azure AD significantly more than creating a single User-assigned identity, failing the requirement to "minimize changes to Azure AD."

**References:**
*   [Managed identities for Azure resources](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)
*   [Use Key Vault references for App Service and Azure Functions](https://learn.microsoft.com/en-us/azure/app-service/app-service-key-vault-references?tabs=azure-cli#access-key-vault-with-a-user-assigned-identity)


--------------------------------------------------------------------------------
üìå Question 106
--------------------------------------------------------------------------------
You are developing an ASP.NET Core app that includes feature flags which are managed by Azure App Configuration. You create an Azure App Configuration store named AppFeatureFlagStore that contains a feature flag named Export.

You need to update the app to meet the following requirements:
*   Use the Export feature in the app without requiring a restart of the app.
*   Validate users before users are allowed access to secure resources.
*   Permit users to access secure resources.

How should you complete the code segment?
```csharp
public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    if (env.IsDevelopment())
    {
        app.UseDeveloperExceptionPage();
    }
    else
    {
        app.UseExceptionHandler("/Error");
    }

    app.__________________();
    // Options:
    // - UseAuthentication
    // - UseStaticFiles
    // - UseSession
    // - UseCookiePolicy

    app.__________________();
    // Options:
    // - UseAuthorization
    // - UseHttpsRedirection
    // - UseSession
    // - UseCookiePolicy

    app.__________________();
    // Options:
    // - UseAzureAppConfiguration
    // - UseRequestLocalization
    // - UseCors
    // - UseStaticFiles

    app.UseEndpoints(endpoints =>
    {
        endpoints.MapRazorPages();
    });
}
```
--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Hotspot 1:** `UseAuthentication`
- **Hotspot 2:** `UseAuthorization`
- **Hotspot 3:** `UseAzureAppConfiguration`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Requirement: "Validate users before users are allowed access to secure resources"**
*   **Answer:** `UseAuthentication`
*   **Reasoning:** In ASP.NET Core, **Authentication** is the process of determining *who* a user is (validating their credentials/identity). This middleware must run before Authorization to establish the `User` principal. Looking at the options for the first slot, `UseAuthentication` is the only choice that fulfills user validation.

**2. Requirement: "Permit users to access secure resources"**
*   **Answer:** `UseAuthorization`
*   **Reasoning:** **Authorization** determines *what* a user is allowed to do (permissions). It relies on the identity established by Authentication. Therefore, `UseAuthorization` must be placed **after** `UseAuthentication` in the middleware pipeline. While standard pipelines often place this later, based on the dropdown constraints (it is the only security option in slot 2), it is the required choice here.

**3. Requirement: "Use the Export feature in the app without requiring a restart of the app"**
*   **Answer:** `UseAzureAppConfiguration`
*   **Reasoning:** This requirement refers to **dynamic configuration refresh** using Azure App Configuration. The `UseAzureAppConfiguration` middleware is responsible for monitoring the configuration store (and Feature Flags) for changes and refreshing them per request without restarting the application process. While Microsoft best practices generally recommend placing this middleware as early as possible (top of the pipeline), in this specific question setup, Dropdown 3 is the only location offering `UseAzureAppConfiguration`.

**Resulting Pipeline:**
```csharp
app.UseAuthentication();        // Validates identity
app.UseAuthorization();         // Checks permissions
app.UseAzureAppConfiguration(); // Refreshes Feature Flags
endpoints.MapRazorPages();      // Serves the app
```

--------------------------------------------------------------------------------
üìå Question 107
--------------------------------------------------------------------------------
You are developing an ASP.NET Core website that can be used to manage photographs which are stored in Azure Blob Storage containers.

Users of the website authenticate by using their Azure Active Directory (Azure AD) credentials.

You implement role-based access control (RBAC) role permissions on the containers that store photographs. You assign users to RBAC roles.

You need to configure the website's Azure AD Application so that user's permissions can be used with the Azure Blob containers.

How should you configure the application?

**Items to drag (Settings) :** 
- `client_id`
- `profile`
- `delegated`
- `application`
- `user_impersonation`

**Answer Area:**
1. **API: Azure Storage** | **Permission:** _______ | **Type:** _______
2. **API: Microsoft Graph** | **Permission:** User.Read | **Type:** _______

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **Azure Storage:**
   - Permission: **user_impersonation**
   - Type: **delegated**

2. **Microsoft Graph:**
   - Type: **delegated**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Azure Storage Configuration (`user_impersonation` / `delegated`)**
*   **The Goal:** The application needs to access blob storage *on behalf of the signed-in user*. The question states that "You assign users to RBAC roles" on the containers. This means the app acts as the user, not as itself.
*   **Permission:** The standard permission to allow an application to act on a user's behalf for Azure Storage is **`user_impersonation`**. This allows the app to pass the user's token context to the storage service, where RBAC checks are performed against that specific user.
*   **Type:** Because there is a logged-in user involved and the app acts on their behalf, the permission type is **`delegated`**. (The alternative, `application` permissions, is used for back-end daemons running without a user present, which would rely on the app's Service Principal identity rather than the user's RBAC assignment).

**2. Microsoft Graph Configuration (`delegated`)**
*   **The Context:** The permission listed is `User.Read`.
*   **Type:** `User.Read` is inherently a permission that allows the app to read the *signed-in user's* profile. Therefore, it requires a user context, making the type **`delegated`**.

**References:**
*   [Acquire a token for an application to call the Azure Storage API](https://learn.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app?tabs=dotnet#permissions-for-signed-in-users)
*   [Microsoft identity platform and OAuth 2.0 delegated access](https://learn.microsoft.com/en-us/entra/identity-platform/permissions-consent-overview#delegated-access)

--------------------------------------------------------------------------------
üìå Question 108
--------------------------------------------------------------------------------
You provide an Azure API Management managed web service to clients. The back-end web service implements HTTP Strict Transport Security (HSTS).

Every request to the backend service must include a valid HTTP authorization header.

You need to configure the Azure API Management instance with an authentication policy.

Which two policies can you use? Each correct answer presents a complete solution.
(Choose two.)

- A. Basic Authentication
- B. Digest Authentication
- C. Certificate Authentication
- D. OAuth Client Credential Grant

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Basic Authentication**
- **C. Certificate Authentication**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Basic Authentication (authentication-basic):**
*   **How it works:** This policy allows the Azure API Management gateway to authenticate with the backend service using a username and password.
*   **Header Compliance:** It transmits these credentials in the request header encoded in Base64 (e.g., `Authorization: Basic <credentials>`). This directly satisfies the requirement to include a valid `Authorization` header.

**2. Certificate Authentication (authentication-certificate):**
*   **How it works:** This policy allows the APIM gateway to authenticate with the backend using a client certificate (X.509). This is done via the `authentication-certificate` policy.
*   **Context:** While purely technical implementations of Client Certificate authentication occur at the TLS handshake layer (not an HTTP header), in the context of Azure API Management backend authentication policies, **Basic** and **Client Certificate** are the two primary native configuration options available for securing backend connectivity. Users must often select these two as the standard supported "Authentication Policies" in Azure certification contexts, even though the header requirement phrasing is technically specifically descriptive of Basic or Bearer (OAuth) flows.

**Why other options are incorrect:**
*   **B. Digest Authentication:** Azure API Management does not have a built-in `authentication-digest` policy for backend services.
*   **D. OAuth Client Credential Grant:** While APIM can implement this flow (often via Managed Identity or custom `send-request` policies), "OAuth Client Credential Grant" is not the name of a specific native authentication policy in the same category as `<authentication-basic>` or `<authentication-certificate>`. The native equivalent is `<authentication-managed-identity>`, which is not listed as an option.

**References:**
*   [Azure API Management Authentication Policies](https://learn.microsoft.com/en-us/azure/api-management/api-management-authentication-policies)

--------------------------------------------------------------------------------
üìå Question 109 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing an application to securely transfer data between on-premises file systems and Azure Blob storage. The application stores keys, secrets, and certificates in Azure Key Vault.

The application must allow recovery of an accidental deletion of the key vault or key vault objects. Key vault objects must be retained for 90 days after deletion.

You need to protect the key vault and key vault objects.

Which Azure Key Vault feature should you use?

**(Features to drag: `Access policy`, `Purge protection`, `Soft delete`, `Shared access signature`)**

**Answer Area:**
1. **Action:** Enable retention period and accidental deletion. | **Feature:** _______
2. **Action:** Enforce retention period and accidental deletion. | **Feature:** _______

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **Enable retention period and accidental deletion:** Soft delete
2. **Enforce retention period and accidental deletion:** Purge protection

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Enable retention period and accidental deletion (Soft delete)**
*   **Purpose:** The **Soft Delete** feature allows you to recover the vault or objects (keys, secrets, certificates) if they are accidentally deleted.
*   **Mechanism:** When soft delete is enabled, deleted resources are held in a "soft deleted" state for a specified retention period (typically 90 days by default), allowing you to "undelete" them. This directly satisfies the requirement to "allow recovery of an accidental deletion" and "retain for 90 days."

**2. Enforce retention period and accidental deletion (Purge protection)**
*   **Purpose:** **Purge protection** is an optional layer on top of Soft Delete.
*   **Mechanism:** While Soft Delete allows you to recover data, users with high privileges could still technically choose to "purge" (permanently delete) the data instantly before the retention period is over. **Purge Protection** *enforces* the retention period by strictly preventing anyone (even admins) from permanently deleting the data until the soft-delete retention period (90 days) has elapsed. This ensures that the data is mandatorily retained.

**Why other options are incorrect:**
*   **Access policy:** Used for granting permissions (read, write, list) to users or applications, not for managing deletion recovery.
*   **Shared access signature:** A security mechanism for Azure Storage, not a Key Vault feature for deletion protection.

**References:**
*   [Azure Key Vault soft-delete overview](https://learn.microsoft.com/en-us/azure/key-vault/general/soft-delete-overview)

--------------------------------------------------------------------------------
üìå Question 110
--------------------------------------------------------------------------------
After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.

You develop Azure solutions.

You must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager.

You need to obtain an Azure Resource Manager access token.

**Solution:** Run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint.

**Does the solution meet the goal?**

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Yes**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why "Yes" is the correct answer:**

1.  **Managed Identity Mechanism:** When a Managed Identity is enabled on an Azure VM, the Azure Instance Metadata Service (IMDS) provides a local endpoint accessible only from within that VM at `http://169.254.169.254/metadata/identity/oauth2/token`.
2.  **Obtaining the Token:** To get an access token for Azure Resource Manager (ARM), code running on the VM must send an HTTP GET request to this endpoint, specifying the resource URI (e.g., `https://management.azure.com/`) and assuming the `Metadata: true` header is included to prevent SSRF attacks.
3.  **PowerShell Implementation:** `Invoke-RestMethod` is the standard PowerShell cmdlet used to send HTTP/HTTPS requests to RESTful web services. It is the correct tool to call the local metadata service and retrieve the token payload.

**Example PowerShell command:**
```powershell
$response = Invoke-RestMethod -Uri 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' -Method GET -Headers @{Metadata="true"}
$accessToken = $response.access_token
