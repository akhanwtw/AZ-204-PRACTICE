
--------------------------------------------------------------------------------
üìå Question 21
--------------------------------------------------------------------------------
You are developing an Azure solution to collect inventory data from thousands 
of stores located around the world. Each store location will send the inventory 
data hourly to an Azure Blob storage account for processing.

The solution must meet the following requirements:
- Begin processing when data is saved to Azure Blob storage.
- Filter data based on store location information.
- Trigger an Azure Logic App to process the data for output to Azure Cosmos DB.
- Enable high availability and geographic distribution.
- Allow 24-hours for retries.
- Implement an exponential back off data processing.

You need to configure the solution.

What should you implement? To answer, select the appropriate options in the 
answer area.

1. Event Source: ______________________
2. Event Receiver: ____________________
3. Event Handler: _____________________


--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. Event Source: **Azure Blob Storage**
2. Event Receiver: **Azure Event Grid**
3. Event Handler: **Azure Logic App**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1. **Event Source: Azure Blob Storage**
   - The scenario states that "Each store location will send the inventory data 
     hourly to an Azure Blob storage account" and processing must begin "when 
     data is saved to Azure Blob storage."
   - Therefore, the origin of the event (the source) is the **Azure Blob 
     Storage** account.

2. **Event Receiver: Azure Event Grid**
   - **Azure Event Grid** is the event routing service that ingests events from 
     Azure sources (like Blob Storage) and routes them to handlers.
   - It satisfies the requirement to "Filter data based on store location 
     information" using **Subject Filtering** (e.g., filtering by blob path 
     prefixes like `/stores/store1/`).
   - It satisfies the requirements for "24-hours for retries" and "exponential 
     back off." Event Grid's default delivery policy includes exponential 
     backoff and retries for up to 24 hours if the endpoint (Logic App) is 
     unavailable or returns errors.
   - It supports high availability and is a foundational service for 
     serverless event-driven architectures.

3. **Event Handler: Azure Logic App**
   - The scenario explicitly states: "Trigger an **Azure Logic App** to process 
     the data."
   - In the Event Grid model, the service that receives the event and executes 
     business logic is called the **Event Handler**.

--------------------------------------------------------------------------------
üìå Question 22
--------------------------------------------------------------------------------
You are developing an Azure solution to collect point-of-sale (POS) device data 
from 2,000 stores located throughout the world. A single device can produce 
2 megabytes (MB) of data every 24 hours. Each store location has one to five 
devices that send data.

You must store the device data in Azure Blob storage. Device data must be 
correlated based on a device identifier. Additional stores are expected to 
open in the future.

You need to implement a solution to receive the device data.

Solution: Provision an Azure Event Hub. Configure the machine identifier as 
the partition key and enable capture.

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
A. Yes

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1.  **Ingestion Capability:** Azure Event Hubs is a big data streaming platform 
    and event ingestion service capable of receiving and processing millions of 
    events per second. It can easily handle the volume of data from 2,000+ 
    stores (approx. 10,000 devices generating ~20GB/day total).

2.  **Storage Requirement (Capture):** The solution enables **Event Hubs 
    Capture**, which automatically saves the incoming streaming data to Azure 
    Blob Storage (or Data Lake Storage) in Avro format. This directly satisfies 
    the requirement to "store the device data in Azure Blob storage" without 
    needing to write a separate consumer application to write to blobs.

3.  **Correlation (Partition Key):** The solution configures the **machine 
    identifier as the partition key**. In Event Hubs, using a partition key 
    ensures that all events with the same key are sent to the same partition 
    in the correct order. When Capture writes the data to Blob Storage, it 
    organizes files by partition. This ensures that all data for a specific 
    device is grouped together within the same partition's captured files, 
    satisfying the requirement to correlate data based on the device identifier.

4.  **Scalability:** Event Hubs is highly scalable and can handle the addition 
    of "additional stores" by adjusting Throughput Units (or Processing Units) 
    as needed.

--------------------------------------------------------------------------------
üìå Question 23
--------------------------------------------------------------------------------
You are a developer for a Software as a Service (SaaS) company. You develop 
solutions that provide the ability to send notifications by using Azure 
Notification Hubs.

You need to create sample code that customers can use as a reference for how 
to send raw notifications to Windows Push Notification Services (WNS) devices.

The sample code must not use external packages.

How should you complete the code segment? To answer, drag the appropriate code 
segments to the correct locations.

--------------------------------------------------------------------------------
üíª Code Segment
--------------------------------------------------------------------------------
```csharp
var endpoint = "...";
var payload = "...";
var request = new HttpRequestMessage(HttpMethod.Post, endpoint);
request.Headers.Add("X-WNS-Type", "wns/raw");
request.Headers.Add("ServiceBusNotification-Format", "‚ñº [ Select Option 1 ]");

request.Content = new StringContent(payload, Encoding.UTF8, "‚ñº [ Select Option 2 ]");
var client = new HttpClient();
await client.SendAsync(request);
```
--------------------------------------------------------------------------------
üîΩ Options
--------------------------------------------------------------------------------
- raw
- windows
- windowsphone
- application/xml
- application/json
- application/octet-stream

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. Select Option 1: **windows**
2. Select Option 2: **application/octet-stream**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1. **ServiceBusNotification-Format: windows**
   - The `ServiceBusNotification-Format` header specifies the target platform 
     for the notification.
   - For Windows Push Notification Services (WNS), the correct value is 
     `windows`.
   - Other values like `windowsphone` are for the legacy MPNS (Microsoft Push 
     Notification Service), and `gcm`/`apple` are for Android/iOS respectively.

2. **Content-Type: application/octet-stream**
   - The code sets the `X-WNS-Type` header to `wns/raw`, indicating a **raw 
     notification**.
   - According to the WNS and Azure Notification Hubs documentation, when 
     sending a raw notification (`wns/raw`), the `Content-Type` header must be 
     set to `application/octet-stream`.
   - `application/xml` is used for Toast, Tile, and Badge notifications.
   - `application/json` is typically used for other platforms like FCM/APNS 
     but not for WNS raw notifications in this context.

--------------------------------------------------------------------------------
üìå Question 24
--------------------------------------------------------------------------------
You are developing a REST web service. Customers will access the service by 
using an Azure API Management instance.

The web service does not correctly handle conflicts. Instead of returning an 
HTTP status code of 409, the service returns a status code of 500. The body 
of the status message contains only the word conflict.

You need to ensure that conflicts produce the correct response.

How should you complete the policy? To answer, drag the appropriate code 
segments to the correct locations.

--------------------------------------------------------------------------------
üíª Code Segment
--------------------------------------------------------------------------------
```xml
< ‚ñº [ Select Option 1 ] >
  <base />
  <choose>
    <when condition = " @( ‚ñº [ Select Option 2 ] .Response.StatusCode == 500
      && ‚ñº [ Select Option 3 ] .LastError.Message.Contains("conflict")) " >
      <return-response>
        < ‚ñº [ Select Option 4 ] code="409" reason="Conflict" />
      </return-response>
    </when>
    <otherwise />
  </choose>
< ‚ñº [ Select Option 5 ] >
```
--------------------------------------------------------------------------------
üîΩ Options
--------------------------------------------------------------------------------
- server
- context
- on-error
- set-status
- when-error
- override-status

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. Select Option 1: **on-error**
2. Select Option 2: **context**
3. Select Option 3: **context**
4. Select Option 4: **set-status**
5. Select Option 5: **on-error**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1. **on-error**: The policy logic is designed to intercept and handle errors. 
   In Azure API Management, the `<on-error>` section is where you handle 
   exceptions and failed requests. The presence of `LastError` in the condition 
   strongly indicates this is the `on-error` section.

2. **context**: The `context` variable is the implicit object in APIM policy 
   expressions that provides access to the request, response, and error 
   information. To check the status code, you use `context.Response.StatusCode`.

3. **context**: Similarly, to access error details, you use the `context` 
   variable. The property `context.LastError` contains information about the 
   error that triggered the `on-error` section.

4. **set-status**: Inside a `<return-response>` policy, you define the 
   response to be sent back to the client. The `<set-status>` policy is used 
   to explicitly set the HTTP status code (in this case, changing it to 409) 
   and the reason phrase.

5. **on-error**: This is the closing tag for the policy section opened in step 1.

--------------------------------------------------------------------------------
üìå Question 25
--------------------------------------------------------------------------------
You are developing an Azure Service application that processes queue data when 
it receives a message from a mobile application. Messages may not be sent to 
the service consistently.

You have the following requirements:
- Queue size must not grow larger than 80 gigabytes (GB).
- Use first-in-first-out (FIFO) ordering of messages.
- Minimize Azure costs.

You need to implement the messaging solution.

Solution: Use the .Net API to add a message to an Azure Service Bus Queue 
from the mobile application. Create an Azure Windows VM that is triggered 
from Azure Service Bus Queue.

Does the solution meet the goal?

A. Yes
B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
B. No

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1. **Queue Size Requirement (80 GB):**
   - **Azure Service Bus Queues** have a maximum size limit. In the Standard 
     tier, the limit is typically 1-5 GB (or up to 80 GB only if partitioning 
     is enabled).
   - **Azure Storage Queues**, on the other hand, can store up to 500 TB of 
     messages (limited only by the storage account capacity).
   - While Service Bus *can* technically reach 80 GB with partitioning, the 
     requirement "Queue size must not grow larger than 80 GB" is often a 
     trick to point towards **Azure Storage Queues** which are designed for 
     massive capacity, whereas Service Bus is typically for smaller, 
     high-value messaging. However, let's look at the other constraints.

2. **FIFO Ordering:**
   - **Azure Service Bus Queues** guarantee FIFO (First-In-First-Out) 
     ordering (especially with sessions).
   - **Azure Storage Queues** do *not* guarantee FIFO ordering.
   - This requirement strongly favors Service Bus.

3. **Minimize Costs:**
   - **Azure Storage Queues** are generally cheaper than Service Bus, 
     especially for high volume.
   - **Azure Windows VM:** The solution proposes creating an **Azure Windows 
     VM** to process messages. Running a dedicated VM (Infrastructure as a 
     Service) is significantly more expensive and requires more management 
     than serverless options like **Azure Functions**.
   - Using a VM violates the "Minimize Azure costs" requirement when a 
     serverless trigger (Azure Function) would be the standard, cost-effective 
     choice for processing queue messages.

4. **Conclusion:**
   - While Service Bus meets the FIFO requirement, the use of a **Windows VM** 
     is not the most cost-effective compute solution.
   - More importantly, the phrasing "Queue size must not grow larger than 80 
     GB" is a specific limit often associated with **Azure Storage Queues** in 
     older exam contexts (where Service Bus limits were strictly 1GB/5GB 
     without partitioning).
   - However, the primary disqualifier here is the **Windows VM** for cost 
     minimization. A better solution would use Azure Functions.

--------------------------------------------------------------------------------
üìå Question 26
--------------------------------------------------------------------------------
You develop a solution that uses Azure Virtual Machines (VMs).

The VMs contain code that must access resources in an Azure resource group. You 
grant the VM access to the resource group in Resource Manager.

You need to obtain an access token that uses the VM's system-assigned managed 
identity.

Which two actions should you perform? Each correct answer presents part of the 
solution.

A. From the code on the VM, call Azure Resource Manager using an access token.
B. Use PowerShell on a remote machine to make a request to the local managed 
   identity for Azure resources endpoint.
C. Use PowerShell on the VM to make a request to the local managed identity 
   for Azure resources endpoint.
D. From the code on the VM, call Azure Resource Manager using a SAS token.
E. From the code on the VM, generate a user delegation SAS token.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
A. From the code on the VM, call Azure Resource Manager using an access token.
C. Use PowerShell on the VM to make a request to the local managed identity 
   for Azure resources endpoint.

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1.  **Obtain the Token (Option C):**
    - Managed Identities for Azure resources provide an identity for 
      applications to use when connecting to resources that support Azure 
      Active Directory (Azure AD) authentication.
    - To obtain an access token for a system-assigned managed identity, you 
      must make a request to the **local Instance Metadata Service (IMDS)** 
      identity endpoint.
    - This endpoint is available only from within the VM itself (at 
      `http://169.254.169.254/metadata/identity/oauth2/token`).
    - Therefore, using PowerShell **on the VM** (not remotely) to call this 
      endpoint is the correct way to retrieve the token.

2.  **Use the Token (Option A):**
    - Once the access token is retrieved from the local endpoint, the code on 
      the VM must use it to authenticate requests.
    - Since the goal is to "access resources in an Azure resource group," the 
      code will make calls to the **Azure Resource Manager (ARM)** API.
    - ARM requires an **OAuth 2.0 access token** (Bearer token) in the 
      Authorization header.
    - Option A describes the action of using this token to perform the required 
      operations.

3.  **Why other options are incorrect:**
    - **B:** The managed identity endpoint (`169.254.169.254`) is non-routable 
      and accessible only from within the VM. It cannot be called from a 
      remote machine.
    - **D:** Azure Resource Manager uses Azure AD access tokens (Bearer tokens), 
      not SAS (Shared Access Signature) tokens. SAS tokens are primarily used 
      for data-plane access to services like Storage or Service Bus.
    - **E:** User delegation SAS tokens are specific to Azure Storage and 
      require an existing OAuth token to create. They are not the primary 
      mechanism for general ARM resource access.

  --------------------------------------------------------------------------------
üìå Question 27
--------------------------------------------------------------------------------
You are developing an Azure Service application that processes queue data when 
it receives a message from a mobile application. Messages may not be sent to 
the service consistently.

You have the following requirements:
- Queue size must not grow larger than 80 gigabytes (GB).
- Use first-in-first-out (FIFO) ordering of messages.
- Minimize Azure costs.

You need to implement the messaging solution.

Solution: Use the .Net API to add a message to an Azure Storage Queue from 
the mobile application. Create an Azure VM that is triggered from Azure 
Storage Queue events.

Does the solution meet the goal?

A. Yes
B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
B. No

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1. **FIFO Ordering (Critical Failure):**
   - The solution proposes using **Azure Storage Queues**.
   - Azure Storage Queues **do not guarantee First-In-First-Out (FIFO)** 
     ordering. Messages are generally processed in a best-effort order.
   - To achieve guaranteed FIFO ordering, you must use **Azure Service Bus 
     Queues** (specifically with sessions for strict ordering).

2. **Minimize Costs (Failure):**
   - The solution proposes creating an **Azure VM** to process the messages.
   - Running a dedicated Virtual Machine is significantly more expensive and 
     resource-intensive than using a serverless compute option like **Azure 
     Functions**, especially when messages are not sent consistently (idle VM 
     costs money).

3. **Queue Size:**
   - While Azure Storage Queues can easily handle 80 GB (up to 500 TB), the 
     failure to meet the FIFO requirement makes this solution incorrect.

--------------------------------------------------------------------------------
üìå Question 28
--------------------------------------------------------------------------------
You develop and deploy a web app to Azure App Service in a production 
environment. You scale out the web app to four instances and configure a 
staging slot to support changes.

You must monitor the web app in the environment to include the following 
requirements:
- Increase web app availability by re-routing requests away from instances 
  with error status codes and automatically replace instances if they remain 
  in an error state after one hour.
- Send web server logs, application logs, standard output, and standard error 
  messaging to an Azure Storage blob account.

You need to configure Azure App Service.

Which values should you use? To answer, drag the appropriate configuration 
value to the correct requirements.

--------------------------------------------------------------------------------
üîΩ Options
--------------------------------------------------------------------------------
- Health check
- Diagnostic setting
- Deployment slot
- Autoscale rule
- Zone redundancy

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. Increase availability: **Health check**
2. Send logs: **Diagnostic setting**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1. **Increase availability: Health check**
   - **Health check** is a specific feature in Azure App Service designed to 
     increase availability.
   - It works by pinging a specified path on your web app instances. If an 
     instance responds with an error status code (non-200-299), the load 
     balancer **re-routes requests away** from that unhealthy instance.
   - Furthermore, if the instance remains unhealthy for one hour, the platform 
     will **automatically replace** it. This perfectly matches the requirement description.

2. **Send logs: Diagnostic setting**
   - **Diagnostic settings** (in Azure Monitor/App Service) are the standard 
     mechanism to stream platform logs (including **AppServiceHTTPLogs**, 
     **AppServiceConsoleLogs**, **AppServiceAppLogs**, etc.) to external 
     destinations.
   - The supported destinations include **Azure Storage blob accounts**, Event 
     Hubs, and Log Analytics workspaces.
   - While "App Service logs" is a feature within the App Service blade, the 
     broader configuration that governs sending various log categories (web 
     server, app logs, stdout/stderr) to a storage account is formally managed 
     via **Diagnostic settings**.

  --------------------------------------------------------------------------------
üìå Question 29
--------------------------------------------------------------------------------
A company is implementing a publish-subscribe (Pub/Sub) messaging component by 
using Azure Service Bus. You are developing the first subscription application.

In the Azure portal you see that messages are being sent to the subscription 
for each topic. You create and initialize a subscription client object by 
supplying the correct details, but the subscription application is still not 
consuming the messages.

You need to ensure that the subscription client processes all messages.

Which code segment should you use?

A. await subscriptionClient.AddRuleAsync(new RuleDescription(RuleDescription.DefaultRuleName, new TrueFilter()));
B. subscriptionClient = new SubscriptionClient(ServiceBusConnectionString, TopicName, SubscriptionName);
C. await subscriptionClient.CloseAsync();
D. subscriptionClient.RegisterMessageHandler(ProcessMessagesAsync, messageHandlerOptions);

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
D. subscriptionClient.RegisterMessageHandler(ProcessMessagesAsync, messageHandlerOptions);

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1.  **The Missing Step:** The scenario states that the `SubscriptionClient` is 
    already created and initialized, and messages are arriving at the 
    subscription in Azure. The missing piece is the logic to actually *pull* 
    or *receive* those messages in your application code.

2.  **RegisterMessageHandler:** In the Azure Service Bus SDK (specifically the 
    older `Microsoft.Azure.ServiceBus` library where `SubscriptionClient` is 
    used), creating the client does not automatically start listening. You must 
    call `RegisterMessageHandler` to provide the callback method 
    (`ProcessMessagesAsync`) that will be executed whenever a message is 
    received. This method effectively "pumps" messages from the Service Bus to 
    your application.

3.  **Why others are incorrect:**
    - **A:** Adding a rule (`TrueFilter`) configures *which* messages the 
      subscription collects from the topic. Since the problem says "messages 
      are being sent to the subscription," the filter is likely already working. 
      Adding a rule doesn't start the consumption process in the client app.
    - **B:** This is just the constructor. The problem states you have already 
      created and initialized the object.
    - **C:** `CloseAsync()` closes the connection, which would prevent 
      processing entirely.

  --------------------------------------------------------------------------------
üìå Question 30
--------------------------------------------------------------------------------
You are building a loyalty program for a major snack producer. When customers 
buy a snack at any of 100 participating retailers the event is recorded in 
Azure Event Hub. Each retailer is given a unique identifier that is used as 
the primary identifier for the loyalty program.

Retailers must be able to be added or removed at any time. Retailers must only 
be able to record sales for themselves.

You need to ensure that retailers can record sales.

What should you do?

1. A. Use publisher policies for retailers.
2. B. Create a partition for each retailer.
3. C. Define a namespace for each retailer.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
A. Use publisher policies for retailers.

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
1.  **Publisher Policies:**
    - Event Hubs supports **Publisher Policies** (often implemented via Shared 
      Access Signatures or specific publisher identities).
    - This feature allows you to issue a unique token to each publisher 
      (retailer).
    - Crucially, it enforces that a publisher can only send events using their 
      specific identity (e.g., to an endpoint like 
      `.../publishers/<retailer-id>`). This satisfies the requirement that 
      "Retailers must only be able to record sales for themselves."
    - It is highly scalable and dynamic; adding or removing a retailer is as 
      simple as issuing or revoking a token/policy, without needing to 
      reconfigure the underlying Event Hub infrastructure.

2.  **Why others are incorrect:**
    - **B. Create a partition for each retailer:** Event Hubs has a limit on 
      partitions (typically 32 for standard tiers). Mapping 100+ retailers 
      directly to partitions is not scalable and is an anti-pattern. Partitions 
      are for throughput parallelism, not for security isolation or identity 
      management.
    - **C. Define a namespace for each retailer:** Creating a separate Event 
      Hubs Namespace for every retailer is extremely heavy-weight, expensive, 
      and difficult to manage. It would likely hit subscription quota limits 
      very quickly.
