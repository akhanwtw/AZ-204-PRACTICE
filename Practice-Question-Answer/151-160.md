--------------------------------------------------------------------------------
üìå Question 151
--------------------------------------------------------------------------------
You are developing an application that uses Azure Cosmos DB Core (SQL) API. You plan to implement client-side encryption to secure sensitive data.

You need to configure the Azure Cosmos DB environment to ensure that specific JSON properties in your documents are encrypted before they are stored in the database.

What should you do?

- A. Create a new container to include an encryption policy with the JSON properties to be encrypted.
- B. Create a customer-managed key (CMK) and store the key in a new Azure Key Vault instance.
- C. Create a data encryption key (DEK) by using the Azure Cosmos DB SDK and store the key in Azure Cosmos DB.
- D. Create an Azure AD managed identity and assign the identity to a new Azure Key Vault instance.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Create a new container to include an encryption policy with the JSON properties to be encrypted.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why A is correct:**
To implement **Client-Side Encryption** (Always Encrypted) in Azure Cosmos DB, you must define the encryption scope at the container level. When creating the container, you define a **ClientEncryptionPolicy**. This policy specifically lists the **paths (JSON properties)** that must be encrypted and maps them to the specific Data Encryption Key (DEK) that should be used. This directly addresses the requirement to "include ... JSON properties to be encrypted."

**Why others are incorrect (or are just prerequisites):**
*   **B (CMK in Key Vault):** While getting a Customer-Managed Key (CMK) is a *prerequisite* step to wrap your Data Encryption Keys, simply creating a CMK does not tell Cosmos DB which JSON properties to encrypt.
*   **C (DEK in Cosmos DB):** Creating a Data Encryption Key (DEK) is also a *prerequisite*. The DEK is the actual key used to encrypt the data, and the wrapped keys are indeed stored in Cosmos DB metadata. However, like the CMK, creating the key doesn't define *which* data fields require encryption. You must reference this key in the policy (Option A) to apply it.
*   **D (Managed Identity):** Managed Identities are used for authentication between services (e.g., app to Key Vault), but they are an identity mechanism, not an encryption configuration mechanism for JSON properties.

**References:**
*   [Client-side encryption in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/how-to-always-encrypted?tabs=dotnet#container-configuration)

--------------------------------------------------------------------------------
üìå Question 152
--------------------------------------------------------------------------------
You are developing an application that uses keys stored in Azure Key Vault.

You need to enforce a specific cryptographic algorithm and key size for keys stored in the vault.

What should you use?

- A. Secret versioning
- B. Azure Policy
- C. Key Vault Firewall
- D. Access policies

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. Azure Policy**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why B is correct (Azure Policy):**
Azure Policy allows you to define organizational standards and enforce compliance at scale. Specifically for Azure Key Vault, there are built-in policy definitions (e.g., "Keys should have the specified cryptographic type and size") that can prevent the creation or update of keys that do not meet specific criteria, such as requiring RSA keys to be at least 2048-bit or enforcing specific elliptical curve types.

**Why others are incorrect:**
*   **A. Secret versioning:** This manages the lifecycle of a specific secret/key (e.g., rotating a key creates a new version). It does not enforce rules on what *kind* of key can be created.
*   **C. Key Vault Firewall:** This controls network access to the vault (e.g., allowing specific IP addresses or virtual networks). It has no visibility into the cryptographic properties of the objects stored inside.
*   **D. Access policies:** These control *who* (which users or service principals) can perform actions (read, write, delete) on keys, secrets, and certificates. They control permission to create a key, but they cannot enforce specific parameters (like key size) during the creation process.

**References:**
*   [Integrate Azure Key Vault with Azure Policy](https://learn.microsoft.com/en-us/azure/key-vault/general/azure-policy?tabs=keys)


--------------------------------------------------------------------------------
üìå Question 153 - Case study
--------------------------------------------------------------------------------

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study -

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background -

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment -

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website -

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms -

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors -

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements -

The application components must meet the following requirements:

Corporate website -

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms -

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors -

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff -

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security -

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues -

Corporate website -

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors -

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to configure all site configuration settings for the corporate website.

Which three actions should you perform? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

- A. Create a managed identity.
- B. Update the role assignments for the Azure Key Vault.
- C. Create an Azure App Configuration store.
- D. Update the role assignments for the Azure App Configuration store.
- E. Create an Azure Key Vault.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Create a managed identity.**
- **C. Create an Azure App Configuration store.**
- **D. Update the role assignments for the Azure App Configuration store.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The requirements specify that site settings must be "centrally stored" and "secured without using secrets" (implying the use of Microsoft Entra ID authentication rather than connection strings/access keys).

1.  **Create an Azure App Configuration store (Option C):** This satisfies the requirement to **centrally store** all site settings. Azure App Configuration is designed specifically for this purpose and supports encryption at rest and in transit.
2.  **Create a managed identity (Option A):** The requirement states settings must be "secured without using secrets" and explicitly mandates the use of **Managed Identities**. A system-assigned or user-assigned managed identity gives the App Service an identity in Azure AD to authenticate against the App Configuration store without needing a connection string (which is a secret).
3.  **Update the role assignments for the Azure App Configuration store (Option D):** Just creating the identity isn't enough; it must be granted permission to read the data. You must assign an RBAC role (specifically the "App Configuration Data Reader" role) to the managed identity on the App Configuration resource.

**Why other options are incorrect:**
*   **B & E (Azure Key Vault):** While Key Vault is used for *secrets* (like passwords or certificates), the question specifically asks about "site configuration settings." Azure App Configuration is the primary service for settings. Furthermore, using Managed Identity with App Configuration removes the need to store a connection string in Key Vault. While you *can* use Key Vault references within App Configuration, the core requirement of "centrally stored settings" points primarily to App Configuration.

--------------------------------------------------------------------------------
üìå Question 154 - Case study
--------------------------------------------------------------------------------

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study -

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background -

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment -

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website -

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms -

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors -

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements -

The application components must meet the following requirements:

Corporate website -

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms -

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors -

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff -

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security -

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues -

Corporate website -

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors -

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to secure the corporate website to meet the security requirements.

What should you do?

- A. Create an Azure Cache for Redis instance. Update the code to support the cache.
- B. Create an Azure Content Delivery Network profile and endpoint. Configure the endpoint.
- –°. Create an App Service instance with a standard plan. Configure the custom domain with a TLS/SSL certificate.
- D. Create an Azure Application Gateway with a Web Application Firewall (WAF). Configure end-to-end TLS encryption and the WAF.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. Create an Azure Application Gateway with a Web Application Firewall (WAF). Configure end-to-end TLS encryption and the WAF.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why D is correct:**
This solution addresses two specific security requirements found in the case study:
1.  **"Web content must be restricted by country/region":** The Web Application Firewall (WAF) running on Azure Application Gateway supports **Geo-match custom rules**. These rules allow you to block or allow traffic based on the country or region of the client IP address.
2.  **"All web communications must be secured by using TLS/HTTPS":** Azure Application Gateway supports end-to-end TLS encryption. It can terminate the TLS session at the gateway (decrypting traffic for inspection by the WAF) and then re-encrypt it before sending it to the backend (App Service), ensuring the connection is secure throughout the entire path.

**Why others are incorrect:**
*   **A (Azure Cache for Redis):** This is a performance optimization for data retrieval. It does not provide network security features like TLS termination or geo-blocking.
*   **B (Azure CDN):** While CDN can help with "File transfer speeds must improve" and offers some geo-filtering capabilities, the Application Gateway with WAF is the robust security solution for protecting the application logic and handling the complex TLS/compliance requirements near the application origin.
*   **C (App Service with Standard Plan):** While this enables HTTPS, App Service on its own (without a WAF front-end) does not provide native, configurable firewall rules for geo-blocking countries at the complexity level implied by "corporate compliance standards" as effectively as a dedicated WAF.

**References:**
*   [Geomatch custom rules in Azure Application Gateway WAF](https://learn.microsoft.com/en-us/azure/web-application-firewall/ag/geomatch-custom-rules)
*   [End-to-end TLS with Application Gateway](https://learn.microsoft.com/en-us/azure/application-gateway/ssl-overview)

--------------------------------------------------------------------------------
üìå Question 155 - Case study
--------------------------------------------------------------------------------

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study -

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background -

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment -

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website -

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms -

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors -

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements -

The application components must meet the following requirements:

Corporate website -

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms -

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors -

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff -

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security -

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues -

Corporate website -

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors -

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to implement farmer authentication.

Which three actions should you perform? Each correct answer presents part of the solution.

- A. Add the shared access signature (SAS) token to the app.
- B. Create a shared access signature (SAS) token.
- C. Create a user flow.
- D. Add the app to the user flow.
- E. Register the app in Microsoft Entra ID.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **C. Create a user flow.**
- **D. Add the app to the user flow.**
- **E. Register the app in Microsoft Entra ID.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The case study mentions creating a "new customer tenant in the Microsoft Entra admin center" (formerly known as Azure AD B2C or Entra External ID for customers). To enable authentication for external users (farmers) in this context, the standard procedure involves:

1.  **Register the app in Microsoft Entra ID (Option E):** Every application that uses Microsoft Entra ID (or Azure AD B2C) for authentication must be registered in the tenant to obtain an Application ID.
2.  **Create a user flow (Option C):** In customer-facing tenants (B2C/External ID), a "User Flow" defines the policies for sign-up, sign-in, password reset, and profile editing. This controls the experience the farmers will see when they log in.
3.  **Add the app to the user flow (Option D):** Once the user flow is created, you must associate your registered application with that flow so that when users access the app, the correct authentication experience is triggered.

**Why other options are incorrect:**
*   **A & B (Shared Access Signature - SAS):** SAS tokens are used for delegated access to Azure Storage resources (blobs, files, queues). They are not used for user authentication into an application web portal.

**Reference:**
*   [Create a user flow in Azure Active Directory B2C](https://learn.microsoft.com/en-us/azure/active-directory-b2c/tutorial-create-user-flows?pivots=b2c-user-flow)
*   [Register a web application in Azure Active Directory B2C](https://learn.microsoft.com/en-us/azure/active-directory-b2c/tutorial-register-applications?tabs=app-reg-ga)

--------------------------------------------------------------------------------
üìå Question 156
--------------------------------------------------------------------------------
You are developing several Azure API Management (APIM) hosted APIs.

You must transform the APIs to hide private backend information and obscure the technology stack used to implement the backend processing.

You need to protect all APIs.

What should you do?

A. Configure and apply a new inbound policy scoped to a product.
B. Configure and apply a new outbound policy scoped to the operation.
C. Configure and apply a new outbound policy scoped to global.
D. Configure and apply a new backend policy scoped to global.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Configure and apply a new outbound policy scoped to global.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why C is correct (Outbound / Global):**
1.  **Outbound Direction:** Information about the backend technology stack (such as headers like `X-Powered-By`, `Server`, `X-AspNet-Version`, or stack traces in error bodies) is returned in the **response** from the backend service. To hide this information from the client, you must modify the response after it leaves the backend but before it reaches the client. This requires an **outbound** policy (e.g., `<set-header name="X-Powered-By" exists-action="delete" />`).
2.  **Global Scope:** The requirement is to protecting **all APIs** hosted in the APIM instance. The **Global** scope is the highest level of policy scope; policies defined here are applied to every API and every operation within the API Management instance. This is the most efficient way to enforce a consistent security rule across the entire environment.

**Why others are incorrect:**
*   **A (Inbound / Product):** 
    *   **Inbound** policies affect the request going *to* the backend (e.g., validating JWTs, rate limiting). They generally cannot strip response headers sent *by* the backend. 
    *   **Product** scope only applies to APIs associated with that specific product. If you have APIs not in that product, they would remain unprotected.
*   **B (Outbound / Operation):** While **Outbound** is the correct direction, the **Operation** scope is the narrowest possible scope (applying to a single endpoint, like `GET /users`). Configuring this for every operation across all APIs would be inefficient and difficult to maintain.
*   **D (Backend / Global):** **Backend** policies are used to customize the request sent from the APIM gateway to the backend service (e.g., changing the target URL or setting backend timeouts). They do not alter the response sent back to the calling client.

**References:**
*   [How to set or edit Azure API Management policies](https://learn.microsoft.com/en-us/azure/api-management/set-edit-policies)
*   [Azure API Management policy reference - set-header](https://learn.microsoft.com/en-us/azure/api-management/set-header-policy)

--------------------------------------------------------------------------------
üìå Question 157
--------------------------------------------------------------------------------
You develop Azure solutions.

A .NET application needs to receive a message each time an Azure virtual machine finishes processing data. The messages must NOT persist after being processed by the receiving application.

You need to implement the .NET object that will receive the messages.

Which object should you use?

- A. QueueClient
- B. SubscriptionClient
- C. TopicClient
- D. CloudQueueClient

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. QueueClient**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**The Scenario**
You have a decoupling scenario where multiple sources (VMs) send messages to a single receiver (.NET application). This implies a **Many-to-One** or **Point-to-Point** messaging pattern, which is best handled by a **Queue**.

**Why A (QueueClient) is correct:**
*   **Topology (Queue vs. Topic):** The requirement is for a specific application to receive a message. In Azure messaging, **Queues** are used for direct communication (1 sender to 1 receiver, or N senders to 1 receiver/competing consumers). **Topics** (and Subscriptions) are used for Publish/Subscribe scenarios (1 sender to Many distinct receivers). Since there is likely only one logical "receiving application" for these completion notifications, a Queue is the correct architectural choice.
*   **Object Implementation:** The `QueueClient` class (historically found in the `Microsoft.Azure.ServiceBus` namespace and conceptually represented in newer SDKs) allows an application to connect to a Service Bus Queue.
*   **Persistence Handling:** To satisfy the requirement that messages "must NOT persist after being processed," the `QueueClient` supports the **Peek-Lock** pattern (receive, process, then complete/delete) or the **Receive-and-Delete** mode. Calling `CompleteAsync()` on the message after processing ensures it is permanently removed from the queue.

**Why others are incorrect:**
*   **B. SubscriptionClient:** Used to receive messages from a Service Bus **Topic Subscription**. This is for Pub/Sub scenarios where multiple *different* apps need copies of the same message. It adds unnecessary complexity for a simple hand-off scenario.
*   **C. TopicClient:** This object is used to **send** messages to a Topic, not receive them.
*   **D. CloudQueueClient:** This is part of the legacy Azure Storage SDK (`Microsoft.WindowsAzure.Storage`). While Azure Storage Queues *can* receive messages, "messaging" questions in this context (involving VMs and processing signals) typically target **Azure Service Bus** as the enterprise-grade messaging broker. Furthermore, `CloudQueueClient` is the specific factory client for the legacy library, whereas `QueueClient` is the standard term for the interaction object in Service Bus (or the modern Storage v12 SDK).

**References:**
*   [Azure Service Bus Queues, Topics, and Subscriptions](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions)
*   [Get started with Service Bus queues (.NET)](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-dotnet-get-started-with-queues)

--------------------------------------------------------------------------------
üìå Question 158
--------------------------------------------------------------------------------
You are developing an application that uses Azure Storage to store customer data. The data must only be decrypted by the customer and the customer must be provided a script to rotate keys.

You need to provide a script to rotate keys to the customer.

How should you complete the command? To answer, select the appropriate options in the answer area.
```powershell
# Get the Key Vault HSM URI
$h = $(az keyvault show `
  --hsm-name "<hsm-name>" `
  --query "properties.hsmUri" `
  -o tsv)

# Get the latest key version
$x = az keyvault _________ list-versions `
  --name "<key-name>" `
  --vault-name "<key-vault-name>" `
  --query "[0].kid" `
  -o tsv

# Update storage account encryption settings
az storage account update `
  --name "<storage-account-name>" `
  --resource-group "<resource-group-name>" `
  --encryption-key-name "<key-name>" `
  --encryption-key-version $x `
  --encryption-key-source _________ `
  --encryption-key-vault $h
```

**Dropdown 1 (Command Component):**
1. key
2. secret
3. recover
4. certificate

**Dropdown 2 (Encryption Key Source):**
1. Microsoft.Secret
2. Microsoft.Storage
3. Microsoft.Keyvault
4. Microsoft.Certificate

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Dropdown 1:** key
- **Dropdown 2:** Microsoft.Keyvault

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Dropdown 1: key**
The script retrieves the latest version of the encryption object to rotate it. Azure Storage encryption with Customer-Managed Keys (CMK) uses cryptographic **Keys** stored in Azure Key Vault (or Managed HSM), not Secrets or Certificates.
*   Command syntax: `az keyvault key list-versions ...` retrieves the versions of the specified key.
*   The output is queried for `kid` (Key ID), which confirms we are dealing with a Key object.

**2. Dropdown 2: Microsoft.Keyvault**
When configuring a Storage Account to use Customer-Managed Keys, you must set the `--encryption-key-source` parameter to indicate that you are providing the key location.
*   **Microsoft.Keyvault:** This tells Azure Storage that the keys are managed by the customer in a Key Vault (or HSM). This is required when you provide parameters like `--encryption-key-name`, `--encryption-key-version` and `--encryption-key-vault`.
*   **Microsoft.Storage:** This allows Azure to manage the keys (Platform-Managed Keys), which contradicts the requirement that "data must only be decrypted by the customer".

**References:**
*   [Configure customer-managed keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/storage/common/customer-managed-keys-configure-key-vault?tabs=azure-cli)
*   [az storage account update](https://learn.microsoft.com/en-us/cli/azure/storage/account?view=azure-cli-latest#az-storage-account-update)


--------------------------------------------------------------------------------
üìå Question 159
--------------------------------------------------------------------------------
You are developing a web application that uses the Microsoft identity platform to authenticate users and resources. The web application calls several REST APIs.

The APIs require an access token from the Microsoft identity platform.

You need to request a token.

Which three properties should you use? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

- A. Redirect URI/URL
- B. Application ID
- C. Application name
- D. Application secret
- E. Supported account type

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Redirect URI/URL**
- **B. Application ID**
- **D. Application secret**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**The Scenario**
A **Web Application** (a confidential client running on a server) needs to acquire an access token to call APIs. This typically uses the **OAuth 2.0 Authorization Code Flow**.

To exchange an authorization code for an access token at the Microsoft Identity Platform `/token` endpoint, the application must provide specific credentials and parameters to prove its identity and the validity of the request.

**Why the selected options are correct:**

1.  **B. Application ID (Client ID):** uniquely identifies your application to the Microsoft identity platform. It is a mandatory parameter in every OAuth 2.0 flow (`client_id`).
2.  **D. Application Secret (Client Secret):** Because this is a "Web Application" (server-side), it is a **Confidential Client**. It must prove its identity to the token endpoint to prevent spoofing. This is done using a client secret (password) or a certificate (`client_secret`).
3.  **A. Redirect URI/URL:** In the Authorization Code flow, the `redirect_uri` used in the initial authorization request (where the user signed in) must be included in the token request. The identity platform verifies that the redirect URI in the token request matches the one in the authorization code creation to ensure the code hasn't been intercepted or used by a different redirection target.

**Why others are incorrect:**
*   **C. Application name:** This is a user-friendly display name defined in the Azure Portal (e.g., "My HR App"). Protocol messages use the GUID-based **Application ID**, not the display name.
*   **E. Supported account type:** (e.g., "Accounts in any organizational directory" or "Personal Microsoft accounts"). This is a configuration setting chosen during App Registration that determines *who* can sign in. It is not a parameter sent in the HTTP request to the `/token` endpoint.

**References:**
*   [Microsoft Identity Platform - OAuth 2.0 authorization code flow](https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-auth-code-flow#request-an-access-token)
*   [ConfidentialClientApplication](https://learn.microsoft.com/en-us/dotnet/api/microsoft.identity.client.confidentialclientapplicationbuilder?view=msal-dotnet-latest)

--------------------------------------------------------------------------------
üìå Question 160
--------------------------------------------------------------------------------
You have an application that uses Azure Blob storage.

You need to update the metadata of the blobs.

Which three methods should you use to develop the solution? To answer, move the appropriate methods from the list of methods to the answer area and arrange them in the correct order.

**Methods:**
*   Metadata.Add
*   SetMetadataAsync
*   FetchAttributesAsync
*   UploadFileStream
*   SetPropertiesAsync

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Step 1:** FetchAttributesAsync
- **Step 2:** Metadata.Add
- **Step 3:** SetMetadataAsync

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To update metadata on an existing blob using the classic Azure Storage SDK (which this question references), you follow the **Retrieve-Modify-Save** pattern:

1.  **FetchAttributesAsync**: First, you retrieve the current properties and metadata of the blob from the server to populate your local blob object. If you skip this, your local object's metadata collection might be empty, and saving it could overwrite existing metadata on the server with an empty list (or just your new item).
2.  **Metadata.Add**: You modify the local metadata collection. The `Metadata` property on the blob object is a dictionary, so you use the `.Add()` method (or indexer) to insert or update your key-value pairs.
3.  **SetMetadataAsync**: Finally, you commit the changes to Azure. This method sends a `PUT` request to update the user-defined metadata associated with the blob.

**Why others are incorrect:**
*   **UploadFileStream:** This is used to upload the actual content (body) of the file, not the metadata.
*   **SetPropertiesAsync:** This method is used to update system properties (like `Content-Type`, `Cache-Control`, `Content-Language`), not user-defined metadata.

**References:**
*   [CloudBlob.SetMetadataAsync Method](https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblob.setmetadataasync)
*   [CloudBlob.FetchAttributesAsync Method](https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblob.fetchattributesasync)
