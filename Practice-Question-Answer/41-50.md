--------------------------------------------------------------------------------
üìå Question 41
--------------------------------------------------------------------------------
You are developing several microservices to run on Azure Container Apps for a company. External TCP ingress traffic from the internet has been enabled for the microservices.

The company requires that the microservices must scale based on an Azure Event Hub trigger.

You need to scale the microservices by using a custom scaling rule.

Which two Kubernetes Event-driven Autoscaling (KEDA) trigger fields should you use? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

1. A. metadata
2. B. type
3. C. authenticationRef
4. D. name
5. E. metricType

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. A. metadata
2. B. type

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
When configuring a custom scaling rule in Azure Container Apps (which utilizes KEDA under the hood), you must define the `custom` rule properties. The two essential fields required to define the behavior of the scaler are:

1.  **type** (Option B): This field specifies the KEDA scaler identifier. To scale based on Azure Event Hubs, you must set this value to `azure-eventhub`.
2.  **metadata** (Option A): This field contains a set of key-value pairs required to configure the specific scaler. For the Azure Event Hub scaler, this includes parameters such as the consumer group, threshold (lag count), and references to connection strings or authentication secrets.

**Why other options are incorrect:**
*   **authenticationRef**: While KEDA uses `authenticationRef` to link TriggerAuthentication objects, in Azure Container Apps, authentication is often handled via the `auth` property alongside the rule, but `type` and `metadata` are the fundamental fields defining the trigger itself.
*   **name**: While the scaling rule itself requires a name, it is an identifier for the rule definition, not a configuration field for the KEDA trigger logic inside the `custom` block.
*   **metricType**: This is typically associated with standard Kubernetes HPA (Horizontal Pod Autoscaler) metrics (e.g., Utilization or AverageValue), not a direct configuration field for a KEDA scaler definition in this context.

--------------------------------------------------------------------------------
üìå Question 42
--------------------------------------------------------------------------------

You develop an image upload service that is exposed using Azure API Management. Images are analyzed after upload for automatic tagging.

Images over 500 KB are processed by a different backend that offers a lower tier of service that costs less money. The lower tier of service is denoted by a header named x-large-request. Images over 500 KB must never be processed by backends for smaller images and must always be charged the lower price.

You need to implement API Management policies to ensure that images are processed correctly.

How should you complete the API Management inbound policy? To answer, select the appropriate options in the answer area.

NOTE: Each correct selection is worth one point.
```xml
<inbound>    
    <base />  
    <set-variable name="imageSize" value="@(context.Request.Headers.GetValueOrDefault("Content-Length", "0"))" />
    <choose>        
        <when condition="@(int.Parse(context.Variables.GetValueOrDefault<string>("imageSize")) < 512000)">            
            <set-header name="x-large-request" exists-action="____BOX1_____" />
        </when>
        <otherwise>            
            <_____BOX2______  _____BOX3_____=" {{large-image-host}}" />
        </otherwise>
    </choose>
</inbound>
```
Options for Box 1 (exists-action): 
1. A. skip
2. B. append
3. C. delete
4. D. override

Options for Box 2 (Policy Name): 

1. A. set-body
2. B. forward-request
3. C. set-backend-service
4. D. set-query-parameter

Options for Box 3 (Attribute): 
1. A. base-url
2. B. dimension
3. c. vary-by-header
4. D. publish-to-dapr

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------

Box 1: C. delete  
Box 2: C. set-backend-service  
Box 3: A. base-url  

--------------------------------------------------------------------------------
‚úÖ Completed Policy
--------------------------------------------------------------------------------
```xml
<inbound>
    <base />
    <set-variable name="imageSize"
                  value="@(context.Request.Headers.GetValueOrDefault("Content-Length", "0"))" />
    <choose>
        <when condition="@(int.Parse(context.Variables.GetValueOrDefault<string>("imageSize")) < 512000)">
            <set-header name="x-large-request" exists-action="delete" />
        </when>
        <otherwise>
            <set-backend-service base-url="{{large-image-host}}" />
        </otherwise>
    </choose>
</inbound>
```
--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------

Box 1: delete  
The policy logic is contained within a <when> block defined by the condition:  
@(int.Parse(context.Variables.GetValueOrDefault<string>("imageSize")) < 512000)  
This condition identifies Small Images (less than 500 KB).

The prompt states:

Small Images (Default Tier) are the standard, more expensive service.  
Large Images (> 500 KB) are the "Lower Tier" service.  
The Lower Tier is denoted by the header x-large-request.  

We must ensure that Small Images are NOT processed as belonging to the lower tier.  
If an incoming request for a small image includes the x-large-request header (either by mistake or malicious spoofing to get a cheaper rate), the system would incorrectly charge the lower price. To prevent this, we must remove the header if it exists.

override with <value>true</value> (as seen in the XML structure) would mark small images as large, charging the wrong (lower) price. This is incorrect.  
skip would allow a user to send the header and bypass the check.  
delete removes the x-large-request header from the request before it reaches the backend, ensuring the default (expensive) tier logic is applied. (Note: When delete is selected, the <value> node in the XML becomes irrelevant or is removed by the editor).

Box 2: set-backend-service  
The <otherwise> block executes for Large Images (>= 500 KB). The goal is to route these requests to a different backend server: {{large-image-host}}.

--------------------------------------------------------------------------------
üìå Question 43
--------------------------------------------------------------------------------
You are developing an application to store millions of images in Azure blob storage.

The application has the following requirements:
‚Ä¢ Store the Exif (exchangeable image file format) data from the image as blob metadata when the application uploads the image.
‚Ä¢ Retrieve the Exif data from the image while minimizing bandwidth and processing time.
‚Ä¢ Utilizes the REST API.

You need to use the image Exif data as blob metadata in the application.

Which HTTP verbs should you use?

**Application Metadata Action: Store Exif data.**
1. GET
2. PUT
3. POST
4. HEAD

**Application Metadata Action: Retrieve Exif data.**
1. PUT
2. POST
3. HEAD
4. CONNECT

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **Store Exif data:** PUT
2. **Retrieve Exif data:** HEAD

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Store Exif data (PUT)**
In the Azure Blob Storage REST API, creating a new blob or updating its content is done via the **Put Blob** operation, which uses the **PUT** verb. You can include user-defined metadata (such as Exif data) in the request headers (prefixed with `x-ms-meta-`) during this operation to store it simultaneously with the image. Alternatively, if updating only metadata for an existing blob, the **Set Blob Metadata** operation also uses the **PUT** verb.

**Why others are incorrect for Storing:**
*   **GET:** Used for retrieving data, not modifying or creating it.
*   **POST:** In Azure Blob Storage, POST is generally not used for creating specific block blobs or setting metadata on a specific resource URI (it is used in rare cases like query batches, but not for standard blob upload).
*   **HEAD:** Used only to retrieve headers/properties, conveying no body or intent to write.

**2. Retrieve Exif data (HEAD)**
The requirement specifically asks to "minimize bandwidth". The **Get Blob Properties** operation allows you to retrieve all user-defined metadata (the Exif data) and system properties without downloading the actual image content (the blob body). This operation uses the **HEAD** verb.

**Why others are incorrect for Retrieving:**
*   **PUT:** This is used for writing/uploading data, not reading it.
*   **POST:** Not used for retrieving blob attributes.
*   **CONNECT:** This verb is reserved for establishing network tunnels (e.g., SSL tunneling) and has no function in the Azure Storage REST API for data retrieval.
*   *(Note on GET)*: While a standard `GET` request would retrieve the metadata, it would also download the entire image file (the body). This fails the requirement to minimize bandwidth. However, `GET` was not even provided as an option in the second dropdown.

**References:**
*   [Put Blob (REST API)](https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob)
*   [Get Blob Properties (REST API)](https://learn.microsoft.com/en-us/rest/api/storageservices/get-blob-properties)

--------------------------------------------------------------------------------
üìå Question 44
--------------------------------------------------------------------------------
You are building a B2B web application that uses Azure B2B collaboration for authentication. Paying customers authenticate to Azure B2B using federation.

The application allows users to sign up for trial accounts using any email address.

When a user converts to a paying customer, the data associated with the trial should be kept, but the user must authenticate using federation.

You need to update the user in Azure Active Directory (Azure AD) when they convert to a paying customer.

Which Graph API parameter is used to change authentication from one-time passcodes to federation?

1. A. resetRedemption
2. B. Status
3. C. userFlowType
4. D. invitedUser

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. A. resetRedemption

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To change the authentication method for an existing B2B guest user (e.g., from Email One-Time Passcode to Federation) while retaining their user object and data, you must reset their redemption status.

The **resetRedemption** parameter is used with the Microsoft Graph `invitations` API.

*   **How it works:** You send a `POST` request to the `/invitations` endpoint for the existing user. In the request body, you set the `resetRedemption` property to `true`.
*   **The Result:** This invalidates the user's previous redemption link and status. The next time the user attempts to sign in (redeem the new invitation), Azure AD (now Microsoft Entra ID) re-evaluates the authentication policy for that user's domain. If you have configured Federation for their domain in the interim (because they became a paying customer), the user will be directed to the federated Identity Provider instead of receiving an OTP.

**Why other options are incorrect:**
*   **Status:** There is no generic `Status` parameter on the user object that directly toggles the authentication protocol between OTP and Federation.
*   **userFlowType:** This parameter is associated with Azure AD B2C (Business to Consumer) user flows or specific self-service sign-up flows, not for resetting the invitation status of an existing B2B guest to switch authentication methods.
*   **invitedUser:** This is a property on the invitation resource that represents the user object itself, not a configuration flag used to trigger a change in the authentication method.

**Reference:**
[Reset redemption status for a guest user - Microsoft Entra External ID](https://learn.microsoft.com/en-us/entra/external-id/reset-redemption-status)

--------------------------------------------------------------------------------
üìå Question 45
--------------------------------------------------------------------------------
You are developing several Azure API Management (APIM) hosted APIs.

You must make several minor and non-breaking changes to one of the APIs. The API changes include the following requirements:

- Must not disrupt callers of the API.
- Enable roll back if you find issues.
- Documented to enable developers to understand what is new.
- Tested before publishing.

You need to update the API.

What should you do?

1. A. Configure and apply header-based versioning.
2. B. Create and publish a product.
3. C. Configure and apply a custom policy.
4. D. Add a new revision to the API.
5. E. Configure and apply query string-based versioning.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
4. D. Add a new revision to the API.

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
In Azure API Management, **Revisions** are designed specifically for making **non-breaking changes** to an API safely.

*   **Non-disruptive & Testing:** When you create a new revision, it is not accessible to the public immediately (unless they specifically target that revision). The integration remains online and untouched for current consumers. You can test the new revision using a special URL (e.g., by appending `;rev=n`) before making it live.
*   **Rollback:** Since revisions are stored history, you can easily switch the "Current" tag back to a previous revision if issues are discovered.
*   **Documentation:** Revisions allow you to maintain a "Change log". When you make a revision "Current", you can publish the description of changes to the developer portal, fulfilling the documentation requirement.

**Why other options are incorrect:**
*   **Versioning (Options A & E):** API **Versions** are intended for **breaking changes** (e.g., removing a field or changing a data type). Implementing versioning usually forces clients to update their connection strings (change URL path, query string, or header) to access the new logic, which is overkill for "minor and non-breaking" changes.
*   **Products (Option B):** Products are used to group APIs for developers and manage visibility/subscriptions (access control), not for managing the lifecycle of code updates or API definitions.
*   **Custom Policy (Option C):** Policies are used to change the behavior of the API (like rate limiting, transformation, or authentication) on the fly. While you could implement some logic changes via policies, they do not inherently provide the testing, rollback, and documentation lifecycle workflow that Revisions do for API definition updates.

**Reference:**
[Revisions - Azure API Management](https://learn.microsoft.com/en-us/azure/api-management/api-management-get-started-revise-api?tabs=azure-portal)

--------------------------------------------------------------------------------
üìå Question 46
--------------------------------------------------------------------------------
You develop and deploy an ASP.NET Core application that connects to an Azure Database for MySQL instance.

Connections to the database appear to drop intermittently and the application code does not handle the connection failure.

You need to handle the transient connection errors in code by implementing retries.

What are three possible ways to achieve this goal? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

A. Close the database connection and immediately report an error.
B. Disable connection pooling and configure a second Azure Database for MySQL instance.
C. Wait five seconds before repeating the connection attempt to the database.
D. Set a maximum number of connection attempts to 10 and report an error on subsequent connections.
E. Increase connection repeat attempts exponentially up to 120 seconds.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Wait five seconds before repeating the connection attempt to the database.**
**D. Set a maximum number of connection attempts to 10 and report an error on subsequent connections.**
**E. Increase connection repeat attempts exponentially up to 120 seconds.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
Handling transient faults (temporary failures like network blips or timeouts) requires a **Retry Pattern**. A robust retry strategy is typically composed of a wait interval, a repetition limit, and a backoff strategy.

**Why Options C, D, and E are correct:**

1.  **Wait five seconds (Option C):** A basic retry logic must include a delay between attempts. Immediate retries can flood the network or the service, worsening the issue. A fixed delay (like 5 seconds) gives the transient issue time to resolve.
2.  **Set a maximum number of connection attempts (Option D):** An infinite retry loop can cause the application to hang indefinitely or exhaust resources (like threads or ports). A "Circuit Breaker" or simple max-retry count (e.g., 10 attempts) ensures the application eventually fails gracefully if the database remains unreachable.
3.  **Increase attempts exponentially (Option E):** Known as **Exponential Backoff**, this is the industry standard for cloud applications. Instead of waiting a fixed 5 seconds every time, the delay increases (e.g., 1s, 2s, 4s, 8s...). This approach prevents the "thundering herd" problem where many clients retry simultaneously, overwhelming the database just as it recovers.

**Why others are incorrect:**

*   **A (Close connection & report error):** This is a "Fail Fast" approach, which is the exact opposite of handling transient errors. It creates a poor user experience by surfacing temporary glitches that would likely have succeeded on a second try.
*   **B (Disable pooling & second instance):**
    *   **Disabling Connection Pooling:** This significantly degrades performance by forcing the app to handshake/authenticate for every single query.
    *   **Second Instance:** While redundancy is good for disaster recovery, it is an infrastructure solution, not a code-level solution for handling intermittent drops on the active connection.

**Implementation Note:**
In .NET, developers often use the **Polly** library or EF Core's built-in `EnableRetryOnFailure()` which implements these exact strategies (Retries, Max Count, and Exponential Backoff) automatically.

**References:**
*   [Transient fault handling](https://learn.microsoft.com/en-us/azure/architecture/best-practices/transient-faults)
*   [Implement retries with exponential backoff](https://learn.microsoft.com/en-us/dotnet/architecture/microservices/implement-resilient-applications/implement-retries-exponential-backoff)

--------------------------------------------------------------------------------
üìå Question
--------------------------------------------------------------------------------
You have an Azure API Management instance named API1 that uses a managed gateway.

You plan to implement a policy that will apply at a product scope and will set the header of inbound requests to include information about the region hosting the gateway of API1. The policy definition contains the following content:

```xml
<policies>
  <inbound>
    ___TARGET1___
    <set-header name="x-request-context-data" exists-action="override">
      <value>@(___TARGET2___.Deployment.Region)</value>
    </set-header>
  </inbound>
</policies>
```

You have the following requirements for the policy definition:
‚Ä¢ Ensure that the header contains the information about the region hosting the gateway of API1.
‚Ä¢ Ensure the policy applies only after any global level policies are processed first.

You need to complete the policy definition.

**TARGET1**
1. `<base/>`
2. `<value>root</value>`
3. `<wait for="all"></wait>`

**TARGET2**
1. context
2. config
3. policy

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**TARGET1:** `<base/>`
**TARGET2:** `context`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. TARGET1: <base/>**
The requirement states: *"Ensure the policy applies only after any global level policies are processed first."*

In Azure API Management, policies are nested. When a policy is executed at a specific scope (like the **Product** scope in this scenario), the `<base />` element determines where the policies from the parent scope (the **Global** scope) are executed relative to the current scope's policies.

*   By placing `<base />` at the **top** of the `<inbound>` section (before the `<set-header>` policy), you ensure that all Global-level inbound policies run first. Once they complete, the execution flow continues to the local `set-header` policy.
*   If `<base />` were omitted, the behavior defaults to running parent policies, but placing it explicitly is the standard way to control execution order. If you placed it *after* the set-header, the header would be set before the global policies ran.

**2. TARGET2: context**
The requirement states: *"Ensure that the header contains the information about the region hosting the gateway of API1."*

In APIM policy expressions (which use C# syntax), the `context` variable is the standard object used to access information about the current request, response, and the environment.

*   `context.Deployment` provides information about the gateway deployment.
*   `context.Deployment.Region` specifically returns the string representing the Azure region (e.g., "West US", "North Europe") where the gateway handling the request is hosted.
*   `config` and `policy` are not valid context objects for retrieving deployment metadata.

**References:**
*   [API Management Policies - base element](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-policies#policy-definition)
*   [API Management Policy Expressions - Context variable](https://learn.microsoft.com/en-us/azure/api-management/api-management-policy-expressions#ContextVariables)

--------------------------------------------------------------------------------
üìå Question 47
--------------------------------------------------------------------------------
You are developing a road tollway tracking application that sends tracking events by using Azure Event Hubs using premium tier.

Each road must have a throttling policy uniquely assigned.

You need to configure the event hub to allow for per-road throttling.

What should you do?

A. Use a unique consumer group for each road.
B. Ensure each road stores events in a different partition.
C. Ensure each road has a unique connection string.
D. Use a unique application group for each road.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. Use a unique application group for each road.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Application Groups** (available in Azure Event Hubs Premium and Dedicated tiers) are the specific feature designed to handle resource governance and throttling for client applications.

1.  **Isolation & Governance:** An Application Group effectively acts as a logical grouping of client applications (producers or consumers) identified by a shared security context (like a specific SAS policy or Microsoft Entra ID context).
2.  **Throttling Policies:** You can apply distinct throttling policies to each Application Group. For example, "Road A" can be mapped to "AppGroup_A" with a limit of 1000 events/second, while "Road B" maps to "AppGroup_B" with a limit of 500 events/second.
3.  **Client Identification:** To implement this, you would likely also give each road a unique credential (part of Option C), but the *configuration* that actually enforces the **throttling policy** is the Application Group, not the connection string itself.

**Why other options are incorrect:**
*   **A. Consumer Groups:** These are used for the *reading* (consumption) side of Event Hubs to allow multiple downstream applications to read the stream independently (state/offset tracking). They do not control ingress throttling for publishers.
*   **B. Partitions:** Partitions provide data organization and parallel throughput capacity using a hash key. While partitions have physical limits, you do not "configure a throttling policy" on a partition; you simply hit the physical partition limit, which affects all publishers writing to that partition evenly.
*   **C. Connection Strings:** While you need a unique identity (SAS key) to distinguish the roads, the connection string itself determines *authentication* (who you are) and *authorization* (can you send/listen). It does not natively store numerical *throttling limits* (e.g., "Max 50 msgs/sec"). You need to map that identity to an Application Group (Option D) to enforce those limits.

**References:**
*   [Resource governance with application groups in Azure Event Hubs](https://learn.microsoft.com/en-us/azure/event-hubs/resource-governance-overview)

--------------------------------------------------------------------------------
üìå Question 48
--------------------------------------------------------------------------------
You plan to implement an Azure Functions app.

The Azure Functions app has the following requirements:
‚Ä¢ Must be triggered by a message placed in an Azure Storage queue.
‚Ä¢ Must use the queue name set by an app setting named input_queue.
‚Ä¢ Must create an Azure Blob Storage named the same as the content of the message.

You need to identify how to reference the queue and blob name in the function.json file of the Azure Functions app.

**Queue name**
1. input_queue
2. {input_queue}
3. %input_queue%

**Blob name**
1. {queueTrigger}
2. {input_queue}/{id}
3. %input_queue%/{filename}

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**Queue name:** `%input_queue%`
**Blob name:** `{queueTrigger}`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Queue name: %input_queue%**
In Azure Functions, specifically within the `function.json` file (or trigger decorators in code), the percent sign syntax `%AppSettingName%` is used to resolve values from **Application Settings** (environment variables).
*   **Requirement:** "Must use the queue name set by an app setting named input_queue."
*   **Solution:** `%input_queue%` tells the runtime to look up the value of the environment variable `input_queue` and use that as the actual queue name to listen to.
*   *Why others are incorrect:* `input_queue` would look for a queue literally named "input_queue". `{input_queue}` is typically syntax for resolving binding data (parameters), not environment variables configuration.

**2. Blob name: {queueTrigger}**
Binding expressions in `function.json` allow you to use data from the trigger to configure input/output bindings. Typically, wrapping a property in curly braces `{}` allows access to trigger metadata or payload.
*   **Requirement:** "Must create an Azure Blob Storage named the same as the content of the message."
*   **Solution:** For a Queue Trigger, the binding expression `{queueTrigger}` resolves to the string content (the payload) of the queue message. If the message content is "file1.txt", the blob will be named "file1.txt".
*   *Why others are incorrect:* `{id}` resolves to the Message ID (a GUID), not the user-defined content. `%input_queue%/{filename}` assumes the queue message is a JSON object with a `filename` property and uses the input queue name as the container, which is valid *only if* the message is JSON, but `{queueTrigger}` is the direct answer to "named same as the content".

**References:**
*   [Azure Functions binding expressions and patterns](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-expressions-patterns#binding-expressions---app-settings)
*   [Azure Queue storage trigger for Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue-trigger?tabs=python-v2%2Cin-process&pivots=programming-language-csharp#message-metadata)

--------------------------------------------------------------------------------
üìå Question 49
--------------------------------------------------------------------------------
You are developing several Azure API Management (APIM) hosted APIs.

You must inspect request processing of the APIs in APIM. Requests to APIM by using a REST client must also be included. The request inspection must include the following information:
‚Ä¢ requests APIM sent to the API backend and the response it received
‚Ä¢ policies applied to the response before sending back to the caller
‚Ä¢ errors that occurred during the processing of the request and the policies applied to the errors
‚Ä¢ original request APIM received from the caller and the policies applied to the request

You need to inspect the APIs.

Which three actions should you do? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

1. A. Enable the Allow tracing setting for the subscription used to inspect the API.
2. B. Add the Ocp-Apim-Trace header value to the API call whit a value set to true.
3. C. Add the Ocp-Apim-Subscription-Key header value to the key for a subscription that allows access to the API.
4. D. Create and configure a custom policy. Apply the policy to the inbound policy section with a global scope.
5. E. Create and configure a custom policy. Apply the policy to the outbound policy section with an API scope.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **A. Enable the Allow tracing setting for the subscription used to inspect the API.**
2. **B. Add the Ocp-Apim-Trace header value to the API call whit a value set to true.**
3. **C. Add the Ocp-Apim-Subscription-Key header value to the key for a subscription that allows access to the API.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To use the **API Inspector** (tracing) feature in Azure API Management to view detailed execution steps (inbound processing, backend request, outbound processing, and policy execution), you must perform a specific handshake between the client and the APIM gateway.

1.  **Enable Permission (Option A):** Tracing exposes sensitive information (keys, headers, payloads). Therefore, it is disabled by default for security. You must go to the APIM portal, navigate to the **Subscriptions** blade, find the subscription you are using for testing, and explicitly check the box to **Allow tracing**.
2.  **Trigger Trace (Option B):** The REST client must signal effectively that it wants a trace for this specific call. This is done by adding the HTTP header `Ocp-Apim-Trace: true`.
3.  **Authenticate (Option C):** You must include the `Ocp-Apim-Subscription-Key` header with the value of the subscription key for which you enabled tracing in step 1.

**Outcome:** When these three conditions are met, the API response will include a header called `Ocp-Apim-Trace-Location`. This header contains a URL to a JSON file hosted by Microsoft that contains the full step-by-step trace of the request processing.

**Why other options are incorrect:**
*   **D & E (Custom Policies):** You do not need to write custom policies (like `trace` or `log-to-eventhub`) to get the standard API Inspector trace described in the requirements. The built-in tracing feature covers exactly what is asked (original request, backend request/response, policy application order, and errors) without modifying the API definition itself.

**References:**
*   [Debug your APIs using request tracing](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-api-inspector)

--------------------------------------------------------------------------------
üìå Question 50 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing a new API to be hosted by Azure API Management (APIM). The backend service that implements the API has not been completed. You are creating a test API and operation.

You must enable developers to continue with the implementation and testing of the APIM instance integrations while you complete the backend API development.

You need to configure a test API response.

How should you complete the configuration?

**APIM Configuration Setting: Policy**
1. proxy
2. set-status
3. mock-response
4. forward-request

**APIM Configuration Setting: Policy section**
1. inbound
2. backend
3. on-error
4. outbound

**APIM Configuration Setting: HTTP response code**
1. 200
2. 400
3. 500
4. 501

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **Policy:** `mock-response`
2. **Policy section:** `inbound`
3. **HTTP response code:** `200`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Policy: mock-response**
The **mock-response** policy is explicitly designed for this scenario (First-API-design workflow). It allows you to return a sample response defined in your API schema to the caller without having to forward the request to a backend service. This unblocks frontend or consumer developers while the backend is still being written.

**2. Policy section: inbound**
To prevent APIM from attempting to contact the (non-existent or incomplete) backend, the mocking logic must interrupt the request processing pipeline early. The **inbound** section is the entry point for the request. Placing `mock-response` here stops the pipeline from proceeding to the backend section and immediately generates the response.

**3. HTTP response code: 200**
To enable developers to test the integration successfully ("happy path"), the mock should return a success status code. **200 (OK)** is the standard code for a successful HTTP request. While 501 (Not Implemented) is semantically true for the backend, it would be treated as an error by the client application, preventing them from testing data handling logic.

**References:**
*   [Mock API responses in Azure API Management](https://learn.microsoft.com/en-us/azure/api-management/mock-api-responses)
