--------------------------------------------------------------------------------
üìå Question 91
--------------------------------------------------------------------------------
You develop and deploy an Azure App Service web app that connects to Azure Cache for Redis as a content cache. All resources have been deployed to the East US 2 region.

The security team requires the following audit information from Azure Cache for Redis:
‚Ä¢ The number of Redis client connections from an associated IP address.
‚Ä¢ Redis operations completed on the content cache.
‚Ä¢ The location (region) in which the Azure Cache for Redis instance was accessed.

The audit information must be captured and analyzed by a security team application deployed to the Central US region.

You need to log information on all client connections to the cache.

Which configuration values should you use?

**Requirement 1: Store log information.**
1. Log Analytics workspace
2. Blob Storage account
3. Data Lake Storage Gen2 Storage account
4. Event hub

**Requirement 2: Enable client connection logging.**
1. Diagnostic setting
2. Managed identity
3. App registration
4. Environment variable

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**1. Store log information:** Blob Storage account
**2. Enable client connection logging:** Diagnostic setting

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Store log information (Blob Storage account)**
The requirement specifically asks to "Store" log information for a security team application to analyze. In Azure Monitor, **Blob Storage accounts** are the standard destination for archiving/storing resource logs for long-term retention or external processing.
*   **Why it fits:** The security team application (deployed in Central US) can be configured to read and process the log files stored in the Blob Storage account. You can create the storage account in the Central US region if required by the team, or in any region, as Diagnostic Settings allow cross-region logging to storage.
*   **Why others are incorrect:** 
    *   **Event Hub:** Typically used when the requirement is to *stream* logs in real-time or near real-time to a third-party SIEM. While possible, the prompt explicitly asks to "Store" the logs, pointing to the archive capability of Storage.
    *   **Log Analytics:** Used for querying logs directly within the Azure Portal or via KQL. While it captures data, it is an analysis tool itself rather than a raw "storage" location for a custom external application to ingest formatted files from.

**2. Enable client connection logging (Diagnostic setting)**
To capture specific logs from Azure resources (such as `ConnectedClientList` or audit logs for Redis) and send them to a destination (like a Storage Account), you must configure **Diagnostic settings**.
*   **Why it fits:** Diagnostic settings are the unified mechanism in Azure Monitor to tell a resource "Save these specific log categories to this specific destination."
*   **Why others are incorrect:** 
    *   **Managed identity / App registration:** These are authentication identity constructs, not monitoring configurations.
    *   **Environment variable:** Used for application configuration (like connection strings), not for enabling platform-level logging infrastructure.

**References:**
*   [Monitor Azure Cache for Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-monitor-troubleshoot-faq#how-do-i-enable-diagnostics-logs-for-azure-cache-for-redis)
*   [Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings)

--------------------------------------------------------------------------------
üìå Question 92
--------------------------------------------------------------------------------
You develop an ASP.NET Core app that uses Azure App Configuration. You also create an App Configuration containing 100 settings.

The app must meet the following requirements:
‚Ä¢ Ensure the consistency of all configuration data when changes to individual settings occur.
‚Ä¢ Handle configuration data changes dynamically without causing the application to restart.
‚Ä¢ Reduce the overall number of requests made to App Configuration APIs.

You must implement dynamic configuration updates in the app.

What are two ways to achieve this goal? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

- A. Create and register a sentinel key in the App Configuration store. Set the refreshAll parameter of the Register method to true.
- B. Increase the App Configuration cache expiration from the default value.
- C. Decrease the App Configuration cache expiration from the default value.
- D. Create and configure Azure Key Vault. Implement the Azure Key Vault configuration provider.
- E. Register all keys in the App Configuration store. Set the refreshAll parameter of the Register method to false.
- F. Create and implement environment variables for each App Configuration store setting.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Create and register a sentinel key in the App Configuration store. Set the refreshAll parameter of the Register method to true.**
- **B. Increase the App Configuration cache expiration from the default value.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
This scenario requires balancing data consistency, dynamic updates, and API quota usage.

**1. Using a Sentinel Key (Option A)**
To ensure **consistency** and **reduce requests**, the standard design pattern in Azure App Configuration is to use a "Sentinel key."
*   Instead of polling all 100 settings individually for changes (which would consume massive API quota), the application only polls a single key (the sentinel).
*   When you change configuration values (e.g., update 10 different database settings), you update the sentinel key last.
*   The application detects the change in the sentinel key. Because the `refreshAll` parameter is set to **true**, the library invalidates the cached values for *all* configuration settings and reloads them simultaneously. This ensures the application sees the new configuration as a consistent set, rather than a mix of old and new values.

**2. Increase Cache Expiration (Option B)**
To **reduce the overall number of requests**, you should increase the cache expiration time.
*   The App Configuration provider caches values locally. The default expiration is often 30 seconds.
*   Every time the cache expires, the application sends a request to check if the monitored key (the sentinel) has changed.
*   By increasing this duration (e.g., to 5 minutes), you significantly reduce the frequency of polling checks sent to the Azure App Configuration API.

**Why others are incorrect:**
*   **C:** Decreasing the cache expiration would *increase* the number of API requests (polling more frequently), violating the requirement.
*   **D:** Azure Key Vault is for secret management. While useful, it does not inherently solve the consistency or polling optimization problems described for general configuration data.
*   **E:** If `refreshAll` is false, the library would only reload the specific key that changed (the sentinel), leaving the actual 100 application settings stale.
*   **F:** Environment variables typically require an application restart to update, violating the "Handle... without causing the application to restart" requirement.

**References:**
*   [Use dynamic configuration in an ASP.NET Core app](https://learn.microsoft.com/en-us/azure/azure-app-configuration/enable-dynamic-configuration-aspnet-core)
*   [Best practices for Azure App Configuration](https://learn.microsoft.com/en-us/azure/azure-app-configuration/howto-best-practices#reduce-requests-made-to-app-configuration)

--------------------------------------------------------------------------------
üìå Question 93
--------------------------------------------------------------------------------
You develop and deploy a Java application to Azure. The application has been instrumented by using the Application Insights SDK.

The telemetry data must be enriched and processed before it is sent to the Application Insights service.

You need to modify the telemetry data.

Which Application Insights SDK features should you use? To answer, drag the appropriate features to the correct requirements.

**Requirement 1:** Reduce the volume of telemetry without affecting statistics.
1. Sampling
2. Telemetry initializer
3. Telemetry processor
4. Telemetry channel

**Requirement 2:** Enrich telemetry with additional properties or override an existing one.
1. Sampling
2. Telemetry initializer
3. Telemetry processor
4. Telemetry channel

**Requirement 3:** Completely replace or discard a telemetry item.
1. Sampling
2. Telemetry initializer
3. Telemetry processor
4. Telemetry channel

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **1. Reduce the volume of telemetry... :** Sampling
- **2. Enrich telemetry with additional properties... :** Telemetry initializer
- **3. Completely replace or discard a telemetry item:** Telemetry processor

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Reduce the volume of telemetry without affecting statistics: Sampling**
Sampling is specifically designed to reduce traffic and storage costs by selecting a representative subset of telemetry items to send. Crucially, Application Insights automatically adjusts the counts/metrics (like "Request Count") in the portal to account for the sampling rate (e.g., if you sample 50%, every 1 request counts as 2 in the stats), so your statistics remain accurate ("without affecting statistics").

**2. Enrich telemetry with additional properties or override an existing one: Telemetry initializer**
A `TelemetryInitializer` is called for every telemetry item before it is sent. It is the standard way to add global properties (like a custom "Environment" or "UserType" property) to all telemetry items or to modify standard properties. Initializers are generally additive.

**3. Completely replace or discard a telemetry item: Telemetry processor**
A `TelemetryProcessor` gives you more granular control than an initializer. While initializers are for adding properties, processors sit in the processing chain and can return `false` to block (discard) an item completely, preventing it from obtaining the next processor or being sent. They are used for advanced filtering logic (e.g., "drop all trace logs from this specific dependency").

**Why others are incorrect:**
*   **Telemetry channel:** The channel is the transport mechanism responsible for buffering and transmitting the items to the endpoint. It doesn't modify, sample, or filter individual items based on business logic.

**References:**
*   [Filter and preprocess telemetry in the Application Insights SDK](https://learn.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling)
*   [Sampling in Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling)

--------------------------------------------------------------------------------
üìå Question 94
--------------------------------------------------------------------------------
You are developing a Java application that uses Cassandra to store key and value data. You plan to use a new Azure Cosmos DB resource and the Cassandra API in the application.

You create an Azure Active Directory (Azure AD) group named Cosmos DB Creators to enable provisioning of Azure Cosmos accounts, databases, and containers.

The Azure AD group must not be able to access the keys that are required to access the data.

You need to restrict access to the Azure AD group.

Which role-based access control should you use?

- A. DocumentDB Accounts Contributor
- B. Cosmos Backup Operator
- C. Cosmos DB Operator
- D. Cosmos DB Account Reader

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Cosmos DB Operator**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Cosmos DB Operator is correct:**
The **Cosmos DB Operator** built-in role is specifically designed for scenarios where a user (or group) needs to manage the Azure resources (provision accounts, databases, and containers) but **must not** have access to the data stored within them.
*   It grants permissions to perform management operations (Create, Update, Delete) on the Cosmos DB account resource.
*   Crucially, it **excludes** the `Microsoft.DocumentDB/databaseAccounts/listKeys/action` permission. Without this permission, the user cannot retrieve the Read/Write keys or connection strings required to connect to the Data Plane (Cassandra API) and read/write actual rows.

**Why others are incorrect:**
*   **A. DocumentDB Accounts Contributor:** This role allows full management of the Cosmos DB account, including the ability to list access keys (`listKeys`). If a user has the keys, they can access the data, violating the security requirement.
*   **B. Cosmos Backup Operator:** This role is limited to managing periodic restore actions and does not have sufficient permissions to provision new accounts, databases, or containers.
*   **D. Cosmos DB Account Reader:** This role is read-only for the Azure resource configuration. It cannot provision new resources.

**Reference:**
*   [Azure Cosmos DB built-in roles: Cosmos DB Operator](https://learn.microsoft.com/en-us/azure/cosmos-db/role-based-access-control#cosmos-db-operator)


--------------------------------------------------------------------------------
üìå Question 95
--------------------------------------------------------------------------------
You develop an Azure App Service web app and deploy to a production environment. You enable Application Insights for the web app.

The web app is throwing multiple exceptions in the environment.

You need to examine the state of the source code and variables when the exceptions are thrown.

Which Application Insights feature should you configure?

- A. Smart detection
- B. Profiler
- C. Snapshot Debugger
- D. Standard test

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Snapshot Debugger**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Snapshot Debugger is correct:**
The **Snapshot Debugger** is specifically capable of capturing a "snapshot" of your production application's state at the moment an exception is thrown. This snapshot includes the full call stack and, crucially, the values of **local variables, arguments, and object properties**. This allows you to debug issues in production without having to reproduce the exact state locally or attaching a remote debugger that would pause the live application.

**Why others are incorrect:**
*   **B. Profiler:** The Application Insights Profiler is designed to trace the execution time of code paths to identify performance bottlenecks (slow requests). While it provides call stacks, it focuses on timing and does not capture variable values for debugging exceptions.
*   **A. Smart detection:** This feature applies machine learning to telemetry to automatically warn you about potential issues like distinct failure patterns or performance anomalies. It is a notification system, not a deep-dive debugging tool for inspecting variable state.
*   **D. Standard test:** This typically refers to "Availability tests" (ping tests) which verify if your application is responding to HTTP requests from around the world. It detects downtime, not internal code state during exceptions.

**References:**
*   [Debug snapshots on exceptions in .NET apps](https://learn.microsoft.com/en-us/azure/azure-monitor/app/snapshot-debugger)
*   [Enable Snapshot Debugger for .NET and .NET Core apps in Azure App Service](https://learn.microsoft.com/en-us/azure/azure-monitor/app/snapshot-debugger-appservice)

--------------------------------------------------------------------------------
üìå Question 96 Case Study
--------------------------------------------------------------------------------
-

Case study

-

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study

-

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background

-

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment

-

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website

-

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms

-

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors

-

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements

-

The application components must meet the following requirements:

Corporate website

-

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms

-

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors

-

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff

-

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security

-

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues

-

Corporate website

-

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors

-

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to resolve the authentication errors for developers.

Which Service Bus security configuration should you use? To answer, select the appropriate options in the answer area.

**Security configuration setting: Azure role-based access control (RBAC) role**
1. Owner
2. Contributor
3. Service Bus Data Owner
4. Service Bus Data Sender

**Security configuration setting: Service Bus scope**
1. Queue
2. Namespace
3. Subscription
4. Resource group

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**1. Azure role-based access control (RBAC) role:** Service Bus Data Sender
**2. Service Bus scope:** Queue

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Azure role-based access control (RBAC) role: Service Bus Data Sender**
The case study describes the "Corporate website" using a "queue-based load leveling pattern" to handling high volumes of resource requests. In this pattern, the website acts as a **producer**, sending messages to the Service Bus queue. The "Issues" section states that developers are debugging locally and receiving authentication errors. To fix this while adhering to the **"principle of least privilege"** (stated in the Security requirements), you must grant the minimum permission required to perform the task.
*   **Service Bus Data Sender:** Correct. This grants send access to the namespace and its entities, which is exactly what the website code does.
*   **Service Bus Data Owner:** Incorrect. This allows full access (send, receive, and manage), which is excessive privileges for a sender application.
*   **Owner / Contributor:** Incorrect. These are management plane roles (managing the Azure resource itself) and do not natively grant data plane access (sending/receiving messages) without explicit assignment or fallback, and they violate least privilege.

**2. Service Bus scope: Queue**
The Security requirements explicitly state: *"The principle of least privilege must be applied when providing any user rights or process access rights."*
*   **Queue:** Correct. Assigning the RBAC role at the specific **Queue** level is the most granular scope available. This ensures developers can only send messages to the specific queue used by the application and not to other queues in the namespace.
*   **Namespace / Resource group / Subscription:** Incorrect. These are broader scopes. Assigning permissions here would grant access to *all* queues and topics within that container, which violates the strict least privilege requirement.

**References:**
*   [Azure built-in roles for Azure Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-role-based-access-control#azure-built-in-roles-for-azure-service-bus)
*   [Azure Service Bus Data Sender Role](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#azure-service-bus-data-sender)

--------------------------------------------------------------------------------
üìå Question 96
--------------------------------------------------------------------------------
You are developing an online game that includes a feature that allows players to interact with other players on the same team within a certain distance. The calculation to determine the players in range occurs when players move and are cached in an Azure Cache for Redis instance.

The system should prioritize players based on how recently they have moved and should not prioritize players who have logged out of the game.

You need to select an eviction policy.

Which eviction policy should you use?

- A. allkeys-Iru
- B. volatile-Iru
- C. allkeys-lfu
- D. volatile-ttl

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. allkeys-lru**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why allkeys-lru is correct:**
The core requirement is to prioritize players based on **recency** of activity ("how recently they have moved"). The **LRU (Least Recently Used)** algorithm is specifically designed for this: it keeps the items that have been accessed or updated most recently and evicts those that haven't been touched in the longest time.
*   When a player moves, their cache entry is updated, marking it as "recently used."
*   When a player logs out or stops moving, their key stops being updated, causing it to fall to the bottom of the LRU list.
*   **allkeys-lru** checks *all* keys in the cache for eviction eligibility. This is the standard configuration for a dedicated cache where you want to maximize the retention of active data (players in game) and aggressively discard inactive data (logged out players) when memory is full.

**Why others are incorrect:**
*   **C. allkeys-lfu (Least Frequently Used):** This prioritizes keys based on how *often* they are used, not how *recently*. A player who was very active an hour ago but has since logged out might still have a high frequency count and be kept over a new player who just logged in. This violates the "recently moved" requirement.
*   **D. volatile-ttl:** This evicts keys with the shortest remaining time-to-live (TTL). While useful for strictly enforcing expiration, it relies on managing exact TTL values rather than the natural usage pattern of the application. It is less precise for "recently used" prioritization than the native LRU algorithm.
*   **B. volatile-lru:** This behaves like LRU but *only* allows eviction of keys that have an expiration (TTL) set. If the application creates keys without an expiration, they would never be evicted, potentially filling the cache. Unless there is a specific requirement to keep persistent "non-expiring" keys (which is not mentioned), **allkeys-lru** is the safer and more comprehensive choice to ensure the cache never locks up due to full memory.

--------------------------------------------------------------------------------
üìå Question 97 Case Study
--------------------------------------------------------------------------------
-

Case study

-

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study

-

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background

-

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment

-

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website

-

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms

-

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors

-

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements

-

The application components must meet the following requirements:

Corporate website

-

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms

-

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors

-

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff

-

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security

-

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues

-

Corporate website

-

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors

-

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to correct the errors for farmers and distributors.

Which solution should you use? To answer, select the appropriate options in the answer area.

NOTE: Each correct selection is worth one point.

**Issue: Farmers' errors**
1. Scale up the App Service plan to Premium.
2. Add an App Service staging deployment slot.
3. Configure the App Service Local Cache feature.
4. Create an Azure Content Delivery Network profile and endpoint.

**Issue: Distributors' errors**
1. Scale up the App Service plan to Premium.
2. Configure the App Service Local Cache feature.
3. Restart the application from the App Service portal.
4. Create a custom autoscale rule to increase the instance count.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**1. Farmers' errors:** Scale up the App Service plan to Premium.
**2. Distributors' errors:** Create a custom autoscale rule to increase the instance count.

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Farmers' errors (Scale up/Premium)**
*   **Symptoms:** Farmers report **HTTP 503 (Service Unavailable)**, and staff report **high CPU and memory usage** at the same time.
*   **Diagnosis:** High CPU/Memory usage causing 503 errors typically indicates that the single instance (or current tier) is overwhelmed and hitting its resource ceiling (vertical scaling limit).
*   **Why Scale Up:** Upgrading to a higher tier (like Premium) provides more powerful hardware (faster CPUs, more RAM) per instance. This is "Scaling Up" (Vertical Scaling). While autoscaling (Scaling Out) helps with load, if the underlying machine is maxing out on memory/CPU just to run the process, moving to a larger SKU is often the first step to stability. Additionally, the requirements mention "File transfer speeds must improve," and Premium tiers offer better performance.
*   **Why not CDN:** CDN helps with static content load speed (Requirement: "webpage-load performance must increase"), not specifically with backend CPU/Memory exhaustion causing 503s on application logic.

**2. Distributors' errors (Autoscale)**
*   **Symptoms:** Distributors report **HTTP 502 (Bad Gateway)** errors, coinciding with **high networking traffic** and response times.
*   **Diagnosis:** 502 errors often occur when the upstream server (App Service) takes too long to respond to the load balancer because it is overwhelmed by the *number* of concurrent requests (networking traffic).
*   **Why Autoscale:** The issue here is volume ("high networking traffic"). The solution for high volume traffic is "Scaling Out" (Horizontal Scaling)‚Äîadding more instances to handle the incoming requests. Creating a **custom autoscale rule** allows the system to automatically add instances when traffic or load metrics increase, ensuring the application stays responsive.
*   **Requirement check:** The requirements explicitly state: *"Applications must automatically scale independent of the compute resources."* This directly validates the need for autoscaling rules.

**References:**
*   [Scale up an app in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/manage-scale-up)
*   [Get started with Autoscale in Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-get-started)

--------------------------------------------------------------------------------
üìå Question 98
--------------------------------------------------------------------------------
You are developing an Azure-based web application. The application goes offline periodically to perform offline data processing. While the application is offline, numerous Azure Monitor alerts fire which result in the on-call developer being paged.

The application must always log when the application is offline for any reason.

You need to ensure that the on-call developer is not paged during offline processing.

What should you do?

- A. Add Azure Monitor alert processing rules to suppress notifications.
- B. Disable Azure Monitor Service Health Alerts during offline processing.
- C. Create an Azure Monitor Metric Alert.
- D. Build an Azure Monitor action group that suppresses the alerts.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Add Azure Monitor alert processing rules to suppress notifications.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Alert Processing Rules (Option A) are correct:**
Alert processing rules (formerly known as Action Rules) allow you to modify the behavior of fired alerts without changing the alert rule definitions themselves. Specifically, they support a **suppression** action, which stops notifications (Action Groups) from firing during a specific schedule (like a planned maintenance window).
*   **Notifications:** The pager is suppressed (Requirement met).
*   **Logging:** The alert instance is still generated and recorded in the portal/logs (logging requirement met).

**Why others are incorrect:**
*   **B. Disable Service Health Alerts:** This disables monitoring for Azure platform-level issues (datacenter outages), not your specific application alerts. It also requires manual intervention or scripts to toggle, which is error-prone.
*   **C. Create a Metric Alert:** This would create a new alert, not fix the issue of existing alerts firing too often.
*   **D. Action Group:** Action Groups define *receivers* (email, SMS, webhook). They do not have built-in logic to suppress themselves based on time windows. That logic belongs in the Alert Processing Rule layer.

**References:**
*   [Alert processing rules in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-action-rules)

--------------------------------------------------------------------------------
üìå Question 99 Case Study
--------------------------------------------------------------------------------

DRAG DROP

-

Case study

-

This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study

-

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background

-

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment

-

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website

-

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms

-

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors

-

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements

-

The application components must meet the following requirements:

Corporate website

-

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms

-

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors

-

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff

-

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security

-

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues

-

Corporate website

-

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors

-

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

You need to correct the internal staff issue with webpages.

Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.

## Actions

- Create an Azure Content Delivery Network profile
- Create an Azure Content Delivery Network origin group
- Configure Azure Content Delivery Network compression
- Create an Azure Content Delivery Network endpoint
- Configure a new Azure Content Delivery Network origin
- Add the Azure Content Delivery Network origin to the origin group

---

## Answer Area

1. 
2. 
3. 


--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **1. Create an Azure Content Delivery Network profile.**
- **2. Create an Azure Content Delivery Network endpoint.**
- **3. Configure Azure Content Delivery Network compression.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The issue states that "webpage load sizes are large and take a long time to load." To solve this, you need to implement a Content Delivery Network (CDN) to cache content closer to users (improving speed) and enable compression to reduce the size of the files being transferred.

The correct administrative sequence to set this up in Azure is:

1.  **Create an Azure Content Delivery Network profile:** This is the top-level resource that holds your CDN configurations. You cannot create endpoints without a profile.
2.  **Create an Azure Content Delivery Network endpoint:** The endpoint is the specific resource within the profile where you configure the *origin* (your App Service) and the domain that users will access.
3.  **Configure Azure Content Delivery Network compression:** Once the endpoint is created, you configure compression settings (such as which MIME types to compress) on that endpoint. This directly addresses the issue of "large webpage load sizes" by significantly reducing the byte size of HTML, CSS, and JavaScript files during transfer.

**Why other options are not the priority:**
*   *Origin Groups:* These are typically used for load balancing or failover scenarios between multiple backends. While valid configuration steps for complex setups, they are not the primary steps required to simply turn on a CDN and enable compression for a single web app.

**References:**
*   [Improve performance by compressing files in Azure CDN](https://learn.microsoft.com/en-us/azure/cdn/cdn-improve-performance)
*   [Quickstart: Create an Azure CDN profile and endpoint](https://learn.microsoft.com/en-us/azure/cdn/cdn-create-new-endpoint)

--------------------------------------------------------------------------------
üìå Question 100
--------------------------------------------------------------------------------
You are building a web application that performs image analysis on user photos and returns metadata containing objects identified. The image analysis is very costly in terms of time and compute resources. You are planning to use Azure Redis Cache so duplicate uploads do not need to be reprocessed.

In case of an Azure data center outage, metadata loss must be kept to a minimum.

You need to configure the Azure Redis cache instance.

Which two actions should you perform? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

- A. Configure Azure Redis with AOF persistence.
- B. Configure Azure Redis with RDB persistence.
- C. Configure second storage account for persistence.
- D. Set backup frequency to the minimum value.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Configure Azure Redis with AOF persistence.**
- **C. Configure second storage account for persistence.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Configure Azure Redis with AOF persistence (A)**
To satisfy the requirement that "metadata loss must be kept to a minimum," **AOF (Append Only File)** persistence is the superior choice.
*   **AOF** writes every write operation to a log file. In the event of a crash or outage, the cache can reconstruct the dataset by replaying these operations. By default, Azure Redis syncs the log every second, meaning at worst you lose one second of data.
*   **Why not RDB (B/D):** RDB (Redis Database) takes snapshots at specific intervals (e.g., every 15, 30, or 60 minutes). If an outage occurs 14 minutes after a 15-minute snapshot, you lose 14 minutes of data. This does not meet the "minimum" loss requirement as well as AOF.

**2. Configure second storage account for persistence (C)**
When enabling AOF persistence in the Azure Redis Cache Premium tier, the configuration requires connection to an Azure Storage account to store the persistence files.
*   **Redundancy:** It is a best practice and a standard configuration option to specify a **Second Setup Storage Account**. This ensures that if the primary storage account experiences a temporary outage or throttling, the Redis service can switch to the secondary account to continue writing the persistence logs, thereby preventing data loss during storage interruptions.

**References:**
*   [Configure data persistence for a Premium Azure Cache for Redis instance](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-how-to-premium-persistence)
