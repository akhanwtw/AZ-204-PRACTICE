--------------------------------------------------------------------------------
üìå Question 201 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are implementing a software as a service (SaaS) ASP.NET Core web service that will run as an Azure Web App. The web service will use an on-premises SQL Server database for storage. The web service also includes a WebJob that processes data updates. Four customers will use the web service.

*   Each instance of the WebJob processes data for a single customer and must run as a singleton instance.
*   Each deployment must be tested by using deployment slots prior to serving production data.
*   Azure costs must be minimized.
*   Azure resources must be located in an isolated network.

You need to configure the App Service plan for the Web App.

How should you configure the App Service plan? To answer, select the appropriate settings in the answer area.

**Configuration Table:**

1.  **Number of VM instances**
    *   2
    *   4
    *   8
    *   16

2.  **Pricing tier**
    *   Isolated
    *   Standard
    *   Premium
    *   Consumption

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**1. Number of VM instances:** 4
**2. Pricing tier:** Isolated

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Number of VM instances: 4**
The requirement states there are **four customers**, and "Each instance of the WebJob processes data for a single customer and **must run as a singleton instance**."
A singleton WebJob runs on a single instance. If you scale out the App Service Plan to multiple instances, a standard continuous WebJob would normally run on *all* instances. However, asking for a distinct "singleton instance" to process data for "a single customer" implies we need capacity for all four distinct processing workloads simultaneously.
Additionally, since the requirement specifies that each instance handles *one* customer, and there are 4 customers, you need 4 instances to ensure each customer's singleton process has a place to run (or that the workload is distributed across 4 scaled-out instances).

**2. Pricing tier: Isolated**
The critical requirement here is "**Azure resources must be located in an isolated network.**"
*   **Isolated:** The App Service Environment (ASE) v3 (or earlier versions) runs in an **Isolated** pricing tier. This is the only tier that deploys the App Service directly inside a dedicated Azure Virtual Network (VNet), providing complete network isolation.
*   **Standard/Premium:** While these support VNet *Integration* (connecting to an on-prem DB), they run on shared public multi-tenant infrastructure, not in an "isolated network" in the strict sense required by ASE scenarios.
*   **Consumption:** This is for Azure Functions, not standard Web Apps with dedicated WebJobs requiring singleton uptime.

Although "Azure costs must be minimized" usually points away from Isolated (which is expensive), the strict requirement for "isolated network" overrides the cost preference. You must choose the cheapest option that *satisfies* the requirements.

**References:**
*   [App Service Environment overview](https://learn.microsoft.com/en-us/azure/app-service/environment/overview)
*   [Run background tasks with WebJobs in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/webjobs-create)

--------------------------------------------------------------------------------
üìå Question 202 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You need to resolve the log capacity issue.

What should you do?

- A. Create an Application Insights Telemetry Filter
- B. Change the minimum log level in the host.json file for the function
- C. Implement Application Insights Sampling
- D. Set a LogCategoryFilter during startup

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Implement Application Insights Sampling**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Sampling is the correct choice:**

1.  **Purpose of Sampling:** Sampling is the primary feature in Application Insights designed specifically to handle high-volume telemetry. It reduces telemetry traffic, data costs, and storage usage (capacity) while preserving a statistically correct analysis of application data.
2.  **Preserves Diagnostics:** Unlike simply raising the log level (which drops all data below a certain severity), sampling retains related data points across requests. If a request fails, sampling logic attempts to preserve the associated logs and dependencies, ensuring that you can still diagnose failures even while reducing overall volume.
3.  **Capacity & Throttling:** When an application generates too much data, it can hit the daily cap or ingestion limits (throttling). Sampling is the standard mechanism to keep ingestion within these capacity limits without blindly discarding data.

**Why other options are incorrect:**

*   **A. Create an Application Insights Telemetry Filter:** Telemetry Filters (using `ITelemetryProcessor`) are used to discard specific items based on logic (e.g., "ignore requests from this specific URL" or "ignore synthetic traffic"). While effective for noise reduction, Sampling is the broader architectural solution for general capacity and volume issues.
*   **B. Change the minimum log level in the host.json file:** Raising the log level (e.g., from `Information` to `Error`) will reduce volume, but it does so by completely blinding you to lower-severity telemetry. You lose the ability to see successful operations or operational details, whereas sampling keeps a representative subset of those operations.
*   **D. Set a LogCategoryFilter during startup:** This allows you to fine-tune which categories of logs are recorded (e.g., muting `System.Net` logs). Like options A and B, this is a filtering mechanism, not a capacity-management mechanism designed to maintain statistical observability like Sampling.

**References:**
*   [Configure sampling for Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/configure-monitoring?tabs=v2#configure-sampling)
*   [Sampling in Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling)

--------------------------------------------------------------------------------
üìå Question 203
--------------------------------------------------------------------------------
You need to resolve the capacity issue.

What should you do?

- A. Convert the trigger on the Azure Function to an Azure Blob storage trigger
- B. Ensure that the consumption plan is configured correctly to allow scaling
- C. Move the Azure Function to a dedicated App Service Plan
- D. Update the loop starting on line PC09 to process items in parallel

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. Update the loop starting on line PC09 to process items in parallel**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why parallel processing is the correct choice:**

1.  **Execution Time Limits:** Azure Functions running on a Consumption Plan have a default timeout of 5 minutes (configurable up to 10 minutes). If a function processes a batch of items sequentially (one after another) in a loop, the total execution time is the sum of all individual processing times. Large batches will eventually cause the function to time out, failing to process the data (a capacity/throughput failure).
2.  **Efficiency:** By updating the loop to process items in parallel (using specific language constructs like `Parallel.ForEach` in C# or `Task.WhenAll`), you utilize the available CPU resources more effectively. This compresses the total execution time, ensuring the batch completes within the Consumption plan's timeout limits.

**Why other options are incorrect:**

*   **A. Convert the trigger on the Azure Function to an Azure Blob storage trigger:** Changing the trigger mechanism generally addresses latency or how the function starts, but it does not improve the performance of the code execution itself. If the code is slow due to sequential processing, the trigger type won't fix the timeout.
*   **B. Ensure that the consumption plan is configured correctly to allow scaling:** The Consumption plan is designed to scale out automatically based on event triggers. While you can limit scaling, you rarely "configure" it to *allow* scaling (it's the default). If a single function execution instance times out processing a specific batch, adding more instances won't help that specific execution finish; the internal logic of the function needs optimization.
*   **C. Move the Azure Function to a dedicated App Service Plan:** While a Dedicated (App Service) Plan allows for unlimited execution duration (solving the timeout symptom), it is generally more expensive and requires managing fixed scale. Optimizing the code to run efficiently in parallel is the detailed engineering solution that preserves the benefits of the serverless Consumption model.

**References:**
*   [Optimize the performance and reliability of Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/performance-reliability#use-async-code-but-avoid-blocking-calls)
*   [Azure Functions scale and hosting - limits](https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#timeout)

--------------------------------------------------------------------------------
üìå Question 204
--------------------------------------------------------------------------------
You need to correct the corporate website error.

Which four actions should you recommend be performed in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.

**List of Actions:**
1.  Upload the certificate to Azure Key Vault.
2.  Update line SC05 of Security.cs to include error handling and then redeploy the code.
3.  Update line SC03 of Security.cs to include a using statement and then re-deploy the code.
4.  Add the certificate thumbprint to the WEBSITE_LOAD_CERTIFICATES app setting.
5.  Upload the certificate to source control.
6.  Import the certificate to Azure App Service.
7.  Generate a certificate.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Step 1:** Generate a certificate.
- **Step 2:** Import the certificate to Azure App Service.
- **Step 3:** Add the certificate thumbprint to the WEBSITE_LOAD_CERTIFICATES app setting.
- **Step 4:** Update line SC03 of Security.cs to include a using statement and then re-deploy the code.

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The error often associated with this scenario (common in Azure Exam case studies) is related to incorrect disposal of the `X509Store` or `X509Certificate2` objects in C# code, or the application code failing to find the certificate because it hasn't been loaded into the user profile.

1.  **Generate a certificate:** Before you can fix a certificate issue, you typically need a valid certificate (often self-signed for dev or designated for the corporate site).
2.  **Import the certificate to Azure App Service:** The certificate must be uploaded to the App Service environment so it resides in the certificate store.
3.  **Add the certificate thumbprint to the `WEBSITE_LOAD_CERTIFICATES` app setting:** This is the crucial configuration step for Azure App Service. By adding the thumbprint (or `*` to load all), Azure loads the specified certificates into the Current User\My certificate store, making them accessible to the application code. Without this, standard C# code to find certificates in the store will fail.
4.  **Update line SC03 of Security.cs to include a `using` statement:** In C#, classes that implement `IDisposable` (like `X509Store`) should be wrapped in a `using` statement to ensure unmanaged resources are released immediately. Failure to do so can lead to resource exhaustion errors (like "System.Security.Cryptography.CryptographicException: The system cannot find the file specified" or handle leaks when checking certificates repeatedly).

**Why other options are incorrect:**
*   **Upload to Azure Key Vault:** While Key Vault is best practice for *storage*, the App Service specifically needs the `WEBSITE_LOAD_CERTIFICATES` setting and the certificate present in its local store for the legacy `X509Store` code patterns usually referenced in these case studies to work.
*   **Upload to source control:** Never upload private keys or certificates (pfx) to source control for security reasons.
*   **Update line SC05 (Error Handling):** While error handling is good, wrapping `IDisposable` objects in a `using` block (Option related to SC03) is the fundamental fix for resource leaks, which is the specific "corporate website error" usually hinted at in this coding pattern.

**References:**
*   [Use a TLS/SSL certificate in your code in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/configure-ssl-certificate-in-code#load-the-certificate-in-linuxwindows-containers)
*   [C# 'using' statement](https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/statements/using)

--------------------------------------------------------------------------------
üìå Question 205
--------------------------------------------------------------------------------
After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.

You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.

The app continues to time out after four minutes. The app must process the blob data.

You need to ensure the app does not time out and processes the blob data.

**Solution:** Configure the app to use an App Service hosting plan and enable the Always On setting.

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**

1.  **The "Four Minute" Limit (230 Seconds):** The specific mention of a timeout after "four minutes" is a reference to the **Azure Load Balancer's idle timeout** for HTTP requests, which is hard-coded to **230 seconds** (approximately 3.8 minutes).
2.  **App Service Plan Limitations:** While an App Service Plan allows for "unbounded" execution durations for background tasks (unlike the Consumption plan's 5-10 minute limit), it **does not** bypass the Azure Load Balancer's 230-second limit for **HTTP-triggered** responses. Even if your function runs for an hour on the backend, the HTTP client waiting for the response will be disconnected with a timeout error (502/503) after ~4 minutes.
3.  **Always On:** The "Always On" setting prevents the app from idling out due to inactivity (cold starts), but it does not extend the active HTTP request timeout window.

**Better Solution:**
To process data that takes longer than ~4 minutes via an HTTP trigger, you must use an **asynchronous pattern**:
*   Use **Durable Functions** (Async HTTP APIs pattern).
*   Or, have the HTTP trigger explicitly **offload the work/message to a Queue** (Queue Storage or Service Bus), and have a separate function (triggered by the queue) process the blob. Queue triggers on an App Service Plan generally do not suffer from the load balancer timeout because there is no open HTTP connection to a client.

**References:**
*   [Azure Functions scale and hosting - Limits](https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#timeout) ("Regardless of the function app timeout setting, 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request.")

--------------------------------------------------------------------------------
üìå Question 206
--------------------------------------------------------------------------------
You need to grant access to the retail store location data for the inventory service development effort.

What should you use?

A. Azure AD access token
B. Azure RBAC role
C. Shared access signature (SAS) token
D. Azure AD ID token
E. Azure AD refresh token

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Shared access signature (SAS) token**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Shared Access Signature (SAS) is the correct choice:**

1.  **Granular & Temporary Access:** A Shared Access Signature (SAS) allows you to grant limited access to specific storage resources (like the container holding the retail store location data) with specific permissions (e.g., Read only) for a defined period. This is often ideal for "development efforts" where you need to provide access to developers or a service under construction without sharing the master storage account keys or assigning permanent Role-Based Access Control (RBAC) roles that might be too broad.
2.  **Constraint Isolation:** Using a SAS token ensures that the inventory service (or the developers working on it) can only access the exact data they need, and the access can be easily revoked (if using a stored access policy) or expired automatically after the development sprint or testing phase is complete.

**Why other options are incorrect:**

*   **A. Azure AD access token:** While Azure AD is the recommended modern auth standard, an "Access Token" itself is the artifact obtained after authentication. To "grant access," you would need to assign an **Azure RBAC role** or use ACLs. Just "using an access token" describes the mechanism of the client, not the configuration that grants the permission.
*   **B. Azure RBAC role:** Assigning an Azure RBAC role (like Storage Blob Data Reader) is a valid way to grant access, but in the context of specific exam scenarios regarding "development efforts" or external integrations, SAS is often preferred for its ability to limit scope to a specific blob/container and time window without managing Azure AD identities for every temporary component. (Note: In production internal services, RBAC/Managed Identity is preferred, but "SAS" is the standard answer for this specific "development effort" exam question context).
*   **D. Azure AD ID token:** An ID token contains claims about the identity of the authenticated user (like their name and email). It is used for authentication verification by the app, not for authorizing access to Azure Storage data.
*   **E. Azure AD refresh token:** A refresh token is used by a client to obtain a new access token when the current one expires. It plays no direct role in defining or granting data access permissions.

**References:**
*   [Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)

--------------------------------------------------------------------------------
üìå Question 207
--------------------------------------------------------------------------------
After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.

You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.

The app continues to time out after four minutes. The app must process the blob data.

You need to ensure the app does not time out and processes the blob data.

**Solution:** Pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Yes**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution works:**

1.  **Avoids the Load Balancer Timeout:** The root cause of the "four minute" timeout is the Azure Load Balancer, which has a hard limit of **230 seconds** for holding an HTTP connection open. No matter how much you increase the function's internal `functionTimeout` setting, the network connection will drop if an HTTP response isn't sent within that window.
2.  **Asynchronous Pattern:** By moving the processing logic to a Queue Trigger (Service Bus or Storage Queue), the HTTP function only needs to perform a quick operation: write a message to the queue and return an HTTP 202 (Accepted) or 200 (OK). This takes milliseconds, well within the timeout limit.
3.  **Background Processing:** The Queue Trigger then picks up the message and processes the blob data in the background. Queue triggers are not subject to the 230-second HTTP limit and can run for up to 10 minutes on the Consumption plan (or indefinitely on Dedicated plans), allowing sufficient time to complete the work.

**References:**
*   [Azure Functions best practices - Avoid long running functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-best-practices?tabs=csharp#avoid-long-running-functions)
*   [Async Request-Reply Pattern](https://learn.microsoft.com/en-us/azure/architecture/patterns/async-request-reply)

--------------------------------------------------------------------------------
üìå Question 208
--------------------------------------------------------------------------------
After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.

You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.

The app continues to time out after four minutes. The app must process the blob data.

You need to ensure the app does not time out and processes the blob data.

**Solution:** Use the Durable Function async pattern to process the blob data.

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Yes**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution works:**

1.  **Async HTTP APIs Pattern:** The Durable Functions **HTTP Async Request-Reply** pattern is specifically designed to handle long-running operations triggered by HTTP.
2.  **Decoupling:** When the HTTP request hits the function:
    *   The HTTP trigger starts a Durable Orchestration (which handles the blob processing logic).
    *   It immediately returns a `202 Accepted` response to the client with a `Location` header (status query URL).
    *   This initial response happens in milliseconds, well before the 230-second (approx. 4 minutes) Azure Load Balancer timeout occurs.
3.  **Background Processing:** The actual work (processing the blob) happens in the background via Activity Functions, which are not subject to the HTTP load balancer idle timeout. The client can poll the status URL provided in the initial response to see when the work is finished.

**References:**
*   [Durable Functions overview - Application patterns](https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=csharp#async-http)
*   [Async HTTP APIs in Durable Functions](https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-http-features)

--------------------------------------------------------------------------------
üìå Question 209
--------------------------------------------------------------------------------
You develop a website. You plan to host the website in Azure. You expect the website to experience high traffic volumes after it is published.

You must ensure that the website remains available and responsive while minimizing cost.

You need to deploy the website.

What should you do?

- A. Deploy the website to a virtual machine. Configure the virtual machine to automatically scale when the CPU load is high.
- B. Deploy the website to an App Service that uses the Shared service tier. Configure the App Service plan to automatically scale when the CPU load is high.
- C. Deploy the website to a virtual machine. Configure a Scale Set to increase the virtual machine instance count when the CPU load is high.
- D. Deploy the website to an App Service that uses the Standard service tier. Configure the App Service plan to automatically scale when the CPU load is high.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. Deploy the website to an App Service that uses the Standard service tier. Configure the App Service plan to automatically scale when the CPU load is high.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Option D is the correct choice:**

1.  **Requirement: High Traffic & Availability (Autoscaling):** To handle high traffic and remain responsive, the solution must support **Autoscale** (automatically adding instances based on metrics like CPU percentage). In Azure App Service, the **Standard** tier is the entry-level production tier that supports Autoscaling.
2.  **Requirement: Minimize Cost:** While Premium tiers offer faster processors and more features, the **Standard** tier is less expensive than Premium while still providing the necessary Autoscaling (up to 10 instances) and deployment slots. It is the most cost-effective tier that meets the technical requirements.

**Why other options are incorrect:**

*   **A. Deploy to a virtual machine:** A standalone Virtual Machine does not inherently support "automatic scaling" (scaling out) to add more instances of itself. You would need a Virtual Machine Scale Set (VMSS) for that. Vertical scaling (resizing the VM) usually requires a reboot/downtime, violating the availability requirement.
*   **B. App Service with Shared tier:** The **Shared** tier is designed for development and testing or very low-traffic sites. It runs on shared infrastructure and **does not support scaling** (neither manual nor automatic). It cannot handle high traffic volumes efficiently.
*   **C. Deploy to a VM / Scale Set:** While a Virtual Machine Scale Set (VMSS) does support autoscaling, deploying a simple website to IaaS (Infrastructure as a Service) requires significantly more management overhead (OS patching, runtime configuration, load balancer setup) compared to App Service (PaaS). For a pure website scenario, App Service is the standard recommendation. Furthermore, often a managed App Service Plan is more cost-efficient for web workloads than managing the equivalent cluster of VMs plus load balancers, especially when factoring in operational costs.

**App Service Scaling Capabilities by Tier:**
*   **Free/Shared:** No scaling.
*   **Basic:** Manual scaling only (up to 3 instances).
*   **Standard:** **Autoscale supported** (up to 10 instances).
*   **Premium:** Autoscale supported (up to 20+ instances).

**References:**
*   [App Service pricing](https://azure.microsoft.com/en-us/pricing/details/app-service/windows/)
*   [Scale up an app in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/manage-scale-up)

--------------------------------------------------------------------------------
üìå Question 210
--------------------------------------------------------------------------------
After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.

You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure Storage Blob storage. The storage account type is General-purpose V2.

When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.

You need to design the process that starts the photo processing.

**Solution:** Convert the Azure Storage account to a BlockBlobStorage storage account.

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**

1.  **Purpose of BlockBlobStorage:** `BlockBlobStorage` accounts are **Premium** performance storage accounts. They store data on SSDs and are optimized for scenarios requiring high transaction rates (IOPS) or very low storage access latency (reading/writing bytes).
2.  **Triggering Latency vs. I/O Latency:** The requirement is about the time it takes to **start** the process (trigger latency), not the time it takes to read the file once started (I/O latency). Changing the storage hardware (Standard HDD/SSD vs Premium SSD) does not change how fast the processing application *detects* a new file.
3.  **The Actual Solution:** The issue of starting within "less than one minute" addresses the limitations of standard Blob Triggers (which rely on polling logs and can take up to 10 minutes on Consumption plans). To guarantee sub-minute triggering, you should use **Azure Event Grid** notifications. Converting the account type doesn't implement Event Grid.

**References:**
*   [Azure Blob storage: Premium performance tiers](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-performance-tiers)
*   [Azure Functions - Blob storage trigger - Latency](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=python-v2%2Cin-process&pivots=programming-language-csharp#polling-and-latency)
