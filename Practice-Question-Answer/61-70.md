--------------------------------------------------------------------------------
üìå Question 61
--------------------------------------------------------------------------------
A company has multiple warehouses. Each warehouse contains IoT temperature devices which deliver temperature data to an Azure Service Bus queue.

You need to send email alerts to facility supervisors immediately if the temperature at a warehouse goes above or below specified threshold temperatures.

Which five actions should you perform in sequence?

**Available Actions:**
*   Add a logic app trigger that fires when one or more messages arrive in the queue.
*   Add a Recurrence trigger that schedules the app to run every 15 minutes.
*   Add an action that sends an email to specified personnel if the temperature is outside of those thresholds.
*   Add a trigger that reads IoT temperature data from a Service Bus queue.
*   Add a logic app action that fires when one or more messages arrive in the queue.
*   Add a condition that compares the temperature against the upper and lower thresholds.
*   Create a blank Logic app.
*   Add an action that reads IoT temperature data from the Service Bus queue.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **1. Create a blank Logic app.**
- **2. Add a logic app trigger that fires when one or more messages arrive in the queue.**
- **3. Add an action that reads IoT temperature data from the Service Bus queue.**
- **4. Add a condition that compares the temperature against the upper and lower thresholds.**
- **5. Add an action that sends an email to specified personnel if the temperature is outside of those thresholds.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Step 1: Create a blank Logic app.** 
Before defining flow logic, you must instantiate the Logic App resource.

**Step 2: Add a logic app trigger that fires when one or more messages arrive in the queue.** 
The requirement states alerts must be sent **"immediately"**.
*   **Why this trigger?** You use the Azure Service Bus trigger (specifically "When one or more messages arrive in a queue"). This is an event-driven (long-polling) trigger that initiates the workflow as soon as data hits the Service Bus.
*   **Why not Recurrence?** A recurrence trigger runs on a schedule (e.g., every 15 minutes), which would introduce unacceptable latency for an "immediate" alert system.

**Step 3: Add an action that reads IoT temperature data from the Service Bus queue.** 
Although the trigger brings the message payload, in Logic Apps, you typically need an action (conceptually often **"Parse JSON"**) to interpret the raw message body into usable fields (like "temperature") that can be evaluated in subsequent steps. This option represents that data extraction/parsing phase.

**Step 4: Add a condition that compares the temperature...**
Once the data is parsed, you use a **Control** action (Condition) to evaluate the logic: `IF temperature > max OR temperature < min`.

**Step 5: Add an action that sends an email...**
Inside the "True" branch of the condition, you add the notification action (e.g., Office 365 Outlook connector) to alert the staff.

--------------------------------------------------------------------------------
üìå Question 62
--------------------------------------------------------------------------------
You are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output.

You must use a storage mechanism with the following requirements:
*   Share session state across all ASP.NET web applications.
*   Support controlled, concurrent access to the same session state data for multiple readers and a single writer.
*   Save full HTTP responses for concurrent requests.

Proposed Solution: **Deploy and configure an Azure Database for PostgreSQL. Update the web applications.**

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**
While it is technically *possible* to write a custom provider to store session state in PostgreSQL, it is not the optimal or intended solution for the requirements, particularly for **Output Caching** ("Save full HTTP responses").

1.  **Performance:** Storing full HTML responses (Output Caching) in a relational, disk-based database like PostgreSQL introduces unnecessary latency. Output caching is designed to be extremely fast to reduce server load, which requires an in-memory store.
2.  **Concurrency:** The requirement for "controlled, concurrent access... for multiple readers and a single writer" is natively and efficiently handled by in-memory data structures designed for caching.
3.  **Best Practice:** The intended Azure solution for sharing session state and output caching across ASP.NET web apps is **Azure Cache for Redis**. Redis is an in-memory key-value store that provides the sub-millisecond latency required for effective output caching and high-throughput session state management.

**References:**
*   [Cache in-memory in ASP.NET Core](https://learn.microsoft.com/en-us/aspnet/core/performance/caching/memory)
*   [Output caching middleware in ASP.NET Core](https://learn.microsoft.com/en-us/aspnet/core/performance/caching/output) (mentions Redis)
*   [Azure Cache for Redis best practices](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-best-practices)

--------------------------------------------------------------------------------
üìå Question 63
--------------------------------------------------------------------------------
You are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output.

You must use a storage mechanism with the following requirements:
*   Share session state across all ASP.NET web applications.
*   Support controlled, concurrent access to the same session state data for multiple readers and a single writer.
*   Save full HTTP responses for concurrent requests.

Proposed Solution: **Enable Application Request Routing (ARR).**

Does the solution meet the goal?

- A. Yes
- B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**
Application Request Routing (ARR) is a load balancing mechanism used in Azure App Service to route requests. While it provides "Session Affinity" (sticky sessions) via the `ARRAffinity` cookie‚Äîensuring a user stays on the same server instance during a session‚Äîit does **not** provide a distributed storage mechanism.

1.  **No Shared Storage:** ARR does not actually *store* the session data or HTML output in a place accessible by multiple web applications. It simply routes traffic.
2.  **No Cross-App Sharing:** Since ARR just keeps a user pinned to one process, it cannot facilitate sharing state *across* different applications.
3.  **Output Caching limitation:** ARR is not an output cache provider. To "Save full HTTP responses" and share them, you need a distributed cache store.

**Correct Approach:**
To meet these requirements (shared session state, concurrent access, and output caching), you should use **Azure Cache for Redis**.

**References:**
*   [Configure ASP.NET Core Session State](https://learn.microsoft.com/en-us/aspnet/core/fundamentals/app-state)
*   [Disable Session affinity cookie (ARR cookie)](https://azure.github.io/AppService/2016/05/16/Disable-Session-affinity-cookie-(ARR-cookie)-for-Azure-web-apps.html)

--------------------------------------------------------------------------------
üìå Question 64
--------------------------------------------------------------------------------
You develop a web app that uses the tier D1 app service plan by using the Web Apps feature of Microsoft Azure App Service.

Spikes in traffic have caused increases in page load times.

You need to ensure that the web app automatically scales when CPU load is about 85 percent and minimize costs.

Which four actions should you perform in sequence?

**Actions:**
*   Configure the web app to the Premium App Service tier.
*   Configure the web app to the Standard App Service tier.
*   Enable autoscaling on the web app.
*   Add a Scale rule.
*   Switch to an Azure App Services consumption plan.
*   Configure a Scale condition.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**1. Configure the web app to the Standard App Service tier.**
**2. Enable autoscaling on the web app.**
**3. Configure a Scale condition.**
**4. Add a Scale rule.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Configure the web app to the Standard App Service tier.**
The current tier is **D1 (Shared)**, which does not support autoscaling. To enable this feature, you must scale up. The choices are Standard or Premium. The requirement is to **minimize costs**, and the **Standard** tier is significantly cheaper than the Premium tier while still supporting autoscaling.

**2. Enable autoscaling on the web app.**
Once the plan is upgraded, you must explicitly enable the "Custom autoscale" feature in the "Scale out (App Service plan)" settings.

**3. Configure a Scale condition.**
Autoscaling works based on "Scale Conditions" (also known as profiles). You must configure a condition which defines the instance limits (Minimum, Maximum, and Default instances). Without defining the Maximum limit in the condition, the scaler cannot add instances.

**4. Add a Scale rule.**
Inside the scale condition, you add the specific logic to trigger the scale action. In this scenario, you add a rule that states: "If CPU Percentage > 85%, increase instance count by 1".

**Why others are incorrect:**
*   **Premium App Service tier:** While it supports scaling, it is more expensive than Standard, violating the "minimize costs" requirement.
*   **Consumption plan:** This is primarily for Azure Functions (serverless) and does not apply to standard Web App scaling in this context without re-platforming using Consumption logic.

**References:**
*   [Scale up an app in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/manage-scale-up)
*   [Get started with Autoscale in Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-get-started)

--------------------------------------------------------------------------------
üìå Question 65
--------------------------------------------------------------------------------
You are implementing an application by using Azure Event Grid to push near-real-time information to customers.

You have the following requirements:
*   You must send events to thousands of customers that include hundreds of various event types.
*   The events must be filtered by event type before processing.
*   Authentication and authorization must be handled by using Microsoft Entra ID.
*   The events must be published to a single endpoint.

You need to implement Azure Event Grid.

**Proposed Solution:** Publish events to a **system topic**. Create an event subscription for each customer.

Does the solution meet the goal?

A. Yes
B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**

1.  **System Topics are for Azure Services:** System topics are built-in topics used by Azure services (like Blob Storage, Azure Resource Groups, or IoT Hub) to publish their own telemetry and lifecycle events. You **cannot** create a System Topic to publish your own custom application events. You must use a **Custom Topic** or an **Event Grid Domain**.
2.  **Scalability Limits (Subscription Count):** A standard Event Grid Topic supports a maximum of **500 event subscriptions**. The requirement specifies "thousands of customers". If you attempt to create a subscription for each customer on a single topic, you will hit this limit.
3.  **Better Alternative:** The correct solution for this scenario is to use an **Event Grid Domain**. Domains allow you to publish events to a single endpoint while managing thousands of topics (often one per customer/tenant) and handle authentication and high-scale distribution efficiently.

**References:**
*   [Event Grid System Topics](https://learn.microsoft.com/en-us/azure/event-grid/system-topics)
*   [Event Grid Domains](https://learn.microsoft.com/en-us/azure/event-grid/event-domains)
*   [Event Grid Limits and Quotas](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits#event-grid-limits)

--------------------------------------------------------------------------------
üìå Question 66
--------------------------------------------------------------------------------
You are implementing an application by using Azure Event Grid to push near-real-time information to customers.

You have the following requirements:
*   You must send events to thousands of customers that include hundreds of various event types.
*   The events must be filtered by event type before processing.
*   Authentication and authorization must be handled by using Microsoft Entra ID.
*   The events must be published to a single endpoint.

You need to implement Azure Event Grid.

**Proposed Solution:** Publish events to a **partner topic**. Create an event subscription for each customer.

Does the solution meet the goal?

A. Yes
B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**

1.  **Scalability Limit:** Similar to standard custom topics, **Partner Topics** generally support a maximum of **500 event subscriptions** per topic. The requirement specifies "thousands of customers". Assigning one subscription per customer to a single topic will cause you to hit this limit.
2.  **Use Case Mismatch:** Partner Topics are primarily designed for SaaS providers (like Auth0 or SAP) to bridge events *into* a customer's Azure subscription. While functionally similar to custom topics, they do not solve the "one endpoint, thousands of disparate subscribers" scale problem.
3.  **Correct Solution:** The intended feature for this scenario ("single endpoint" + "thousands of tenants/customers") is an **Event Grid Domain**. Domains expose a single publishing endpoint to your application but allow you to manage thousands of topics (one per customer or group) on the backend, bypassing the subscription limit of a single topic.

**References:**
*   [Azure Event Grid quotas and limits](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits#event-grid-limits)
*   [Understand Event Domains for managing massive topics](https://learn.microsoft.com/en-us/azure/event-grid/event-domains)

--------------------------------------------------------------------------------
üìå Question 67 - Case Study 
--------------------------------------------------------------------------------
This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.

To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.

At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.

To start the case study

-

To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.

Background

-

Munson‚Äôs Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.

Current Environment

-

The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.

Corporate website

-

‚Ä¢ The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.

Farms

-

‚Ä¢ The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.

Distributors

-

‚Ä¢ Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.

Requirements

-

The application components must meet the following requirements:

Corporate website

-

‚Ä¢ The site must be migrated to Azure App Service.

‚Ä¢ Costs must be minimized when hosting in Azure.

‚Ä¢ Applications must automatically scale independent of the compute resources.

‚Ä¢ All code changes must be validated by internal staff before release to production.

‚Ä¢ File transfer speeds must improve, and webpage-load performance must increase.

‚Ä¢ All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.

‚Ä¢ A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.

Farms

-

‚Ä¢ Farmers must authenticate to applications by using Microsoft Entra ID.

Distributors

-

‚Ä¢ The company must track a custom telemetry value with each API call and monitor performance of all APIs.

‚Ä¢ API telemetry values must be charted to evaluate variations and trends for resource data.

Internal staff

-

‚Ä¢ App and API updates must be validated before release to production.

‚Ä¢ Staff must be able to select a link to direct them back to the production app when validating an app or API update.

‚Ä¢ Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.

Security

-

‚Ä¢ All web communications must be secured by using TLS/HTTPS.

‚Ä¢ Web content must be restricted by country/region to support corporate compliance standards.

‚Ä¢ The principle of least privilege must be applied when providing any user rights or process access rights.

‚Ä¢ Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.

Issues

-

Corporate website

-

‚Ä¢ Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.

‚Ä¢ Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.

‚Ä¢ Internal staff report webpage load sizes are large and take a long time to load.

‚Ä¢ Developers receive authentication errors to Service Bus when they debug locally.

Distributors

-

‚Ä¢ Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.

-------------------------------------------------------------------------------
You need to provide internal staff access to the production site after a validation.

How should you complete the code segment? To answer, select the appropriate options in the answer area.

**Code Segment:**
`<a href="https://www.munsonspicklesandpreservesfarm.com/? [Dropdown 1] = [Dropdown 2] ">Go back to production app</a>`

**Dropdown 1 Options:**
*   x-ms-app
*   x-ms-user
*   x-ms-routing-name
*   x-ms-client-request-id

**Dropdown 2 Options:**
*   self
*   staging
*   production

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Dropdown 1:** `x-ms-routing-name`
- **Dropdown 2:** `self`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Dropdown 1: `x-ms-routing-name`**
In Azure App Service, the Traffic Routing feature allows you to direct a specific percentage of user traffic to different deployment slots (like "staging" or a "beta" slot). To **manually force** a user's session to a specific slot (overriding the random percentage assignment), you use the query parameter `x-ms-routing-name`. This sets a session cookie (`x-ms-routing-name`) in the user's browser, pinning them to that slot for the duration of their session.

**2. Dropdown 2: `self`**
To explicitly route a user back to the **production** slot (the default slot) and opt them out of a specific validation slot (like "staging"), Microsoft documentation specifies using the value `self`. While routing to a named slot uses the slot's name (e.g., `x-ms-routing-name=staging`), reverting to the main production endpoint is done with `x-ms-routing-name=self`.

**References:**
*   [Azure App Service: Connect to a specific slot (Manually route traffic)](https://learn.microsoft.com/en-us/azure/app-service/deploy-staging-slots#manually-route-traffic)
    *   *Quote: "To let users opt out of your beta app and return to the production app, you can use a link with the query parameter x-ms-routing-name=self."*
 
--------------------------------------------------------------------------------
üìå Question 67
--------------------------------------------------------------------------------
You are implementing an application by using Azure Event Grid to push near-real-time information to customers.

You have the following requirements:
*   You must send events to thousands of customers that include hundreds of various event types.
*   The events must be filtered by event type before processing.
*   Authentication and authorization must be handled by using Microsoft Entra ID.
*   The events must be published to a single endpoint.

You need to implement Azure Event Grid.

**Proposed Solution:** Enable ingress, create a TCP scale rule, and apply the rule to the container app.

Does the solution meet the goal?

A. Yes
B. No

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. No**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why this solution fails:**
The proposed solution describes configuring networking (ingress) and auto-scaling (TCP scale rules, likely KEDA-based) for **Azure Container Apps**. This is a compute configuration step. It does **not** configure the Azure Event Grid architecture required to handle thousands of subscribers. It fails to address the core requirement of managing event routing for thousands of customers.

**Correct Approach:**
To handle "thousands of customers" (subscribers) while publishing to a "single endpoint", you must use **Event Grid Domains**.
*   Standard Topics have a limit of 500 subscriptions, which is insufficient for "thousands of customers".
*   Event Domains allow you to expose a single publishing endpoint to your backend, while internally partitioning traffic into thousands of individual topics (one per customer), satisfying the scalability requirement.

**References:**
*   [Set scaling rules in Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/scale-app)
*   [Understand Event Domains for managing massive topics](https://learn.microsoft.com/en-us/azure/event-grid/event-domains)

--------------------------------------------------------------------------------
üìå Question 68
--------------------------------------------------------------------------------
An organization has web apps hosted in Azure.

The organization wants to track events and telemetry data in the web apps by using Application Insights.

You need to configure the web apps for Application Insights.

Which three actions should you perform in sequence?

**Actions:**
*   Configure the Azure App Service SDK for the app
*   Configure the Application Insights SDK in the app
*   Copy the connection string
*   Create an Azure Machine Learning workspace
*   Create an Application Insights resource

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **1. Create an Application Insights resource**
- **2. Copy the connection string**
- **3. Configure the Application Insights SDK in the app**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Create an Application Insights resource**
Before you can collect any data, you must provision the **Application Insights resource** in Azure. This serves as the container and processing engine for the telemetry data sent by your application.

**2. Copy the connection string**
Once the resource is created, you need the unique identifier to link your specific application to that resource. Historically, an "Instrumentation Key" was used, but the modern standard is the **Connection String**, which includes the instrumentation key and endpoints. You must copy this value from the Azure Portal blade of the resource you just created.

**3. Configure the Application Insights SDK in the app**
Finally, you install the **Application Insights SDK** (e.g., via NuGet for .NET or npm for Node.js) into your application code. You then configure this SDK using the connection string copied in the previous step. This enables the application to start collecting and transmitting telemetry data.

**Why others are incorrect:**
*   **Create an Azure Machine Learning workspace:** This service is for training AI models, not for general application telemetry or performance monitoring.
*   **Configure the Azure App Service SDK:** While App Service has built-in monitoring, "tracking events and telemetry... by using Application Insights" specifically refers to the Application Insights SDK implementation.

**References:**
*   [Application Insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)
*   [Enable Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/create-new-resource)

--------------------------------------------------------------------------------
üìå Question 69
--------------------------------------------------------------------------------
You plan to deploy a web app to App Service on Linux. You create an App Service plan. You create and push a custom Docker image that contains the web app to Azure Container Registry.

You need to access the console logs generated from inside the container in real-time.

How should you complete the Azure CLI command?

**Command Structure:**
1. `az webapp log [Dropdown 1] --name ContosoWeb --resource-group ContosoDevRG`
   `[Dropdown 2] filesystem`
2. `az [Dropdown 3] log [Dropdown 4] --name ContosoWeb --resource-group ContosoDevRG`

**Dropdown Options:**
*   **Dropdown 1:** `config`, `download`, `show`, `tail`
*   **Dropdown 2:** `--web-server-logging`, `--docker-container-logging`, `--application-logging`
*   **Dropdown 3:** `webapp`, `acr`, `aks`
*   **Dropdown 4:** `config`, `download`, `show`, `tail`

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **1. Dropdown 1:** `config`
- **2. Dropdown 2:** `--docker-container-logging`
- **3. Dropdown 3:** `webapp`
- **4. Dropdown 4:** `tail`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Step 1: Configure Logging**
Before you can stream logs, you must enable them. The command `az webapp log config` modifies the logging configuration for the web app.
*   **Why `--docker-container-logging`?** The scenario specifies a **custom Docker image** and asks for "console logs generated from inside the container". In Azure App Service for Linux, the stdout/stderr output from the container process is captured via **Docker container logging**.
*   **Why `filesystem`?** You are enabling the file system as the destination for these logs.

**Step 2: Stream Logs**
To access the logs in **real-time**, you use the `tail` command.
*   **Service:** You are targeting the **App Service** (Web App), so the command starts with `az webapp`.
*   **Action:** `log tail` starts a live log streaming session to your terminal.

**Full Commands:**
1.  `az webapp log config --name ContosoWeb --resource-group ContosoDevRG --docker-container-logging filesystem`
2.  `az webapp log tail --name ContosoWeb --resource-group ContosoDevRG`

**References:**
*   [Enable diagnostics logging for apps in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs#enable-application-logging-linuxcontainer)
*   [az webapp log (Azure CLI Reference)](https://learn.microsoft.com/en-us/cli/azure/webapp/log)

--------------------------------------------------------------------------------
üìå Question 70
--------------------------------------------------------------------------------
You are developing an ASP.NET Core time sheet application that runs as an Azure Web App. Users of the application enter their time sheet information on the first day of every month.

The application uses a third-party web service to validate data.

The application encounters periodic server errors due to errors that result from calling a third-party web server. Each request to the third-party server has the same chance of failure.

You need to configure an Azure Monitor alert to detect server errors unrelated to the third-party service. You must minimize false-positive alerts.

How should you complete the Azure Resource Manager template?
```json
"type": "Microsoft.Insights/metricAlerts",
"properties": {
  "criteria": {
    "odata.type": ". . .",
    "allOf": [
      {
        "criterionType": "‚ñæ
DynamicThresholdCriterion
SingleResourceMultipleMetricCriteria
",
        "metricName": "‚ñæ
Http4xx
Http5xx
",
        "alertSensitivity": "‚ñæ
Low
High
"
      }
    ]
  }
}
```
**criterionType**
1. DynamicThresholdCriterion
2. SingleResourceMultipleMetricCriteria

**metricName**
1. Http4xx
2. Http5xx

**alertSensitivity**
1. Low
2. High

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **criterionType:** DynamicThresholdCriterion
2. **metricName:** Http5xx
3. **alertSensitivity:** Low

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. criterionType: DynamicThresholdCriterion**
The application has a distinct usage pattern ("users enter their time sheet information on the first day of every month"). This creates a traffic spike that occurs regularly (seasonality). Static thresholds (`SingleResourceMultipleMetricCriteria`) would likely produce false positives during the monthly spike or false negatives during low-traffic periods.
**Dynamic Thresholds** use machine learning to learn historical patterns (seasonality) and define the "normal" range automatically. It can learn that higher error counts are expected on the 1st of the month due to higher volume and the consistent failure rate of the third-party service, alerting only when the error rate exceeds this learned expectation.

**2. metricName: Http5xx**
The prompt explicitly mentions the application encounters "server errors". In specific HTTP status code terminology:
*   **Http4xx:** Client errors (e.g., 400 Bad Request, 404 Not Found). This is typically user error.
*   **Http5xx:** Server errors (e.g., 500 Internal Server Error, 502 Bad Gateway). This indicates the server failed to fulfill a valid request, which matches the scenario of downstream service failures causing issues.

**3. alertSensitivity: Low**
The requirement is to "minimize false-positive alerts".
*   **High Sensitivity:** Sets tight boundaries around the learned metric pattern. Slight deviations trigger alerts.
*   **Low Sensitivity:** Sets wider boundaries (looser thresholds) around the learned pattern.
Since the third-party service has a known failure rate ("same chance of failure"), there will be baseline noise. To distinguish unexpected errors from this baseline noise and avoid triggering on minor fluctuations, a **Low** sensitivity is appropriate. It ensures alerts are only fired for significant deviations.

**References:**
*   [Metric Alerts with Dynamic Thresholds in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-dynamic-thresholds)
*   [Supported resources for metric alerts](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-metric-supported-resource-types)
