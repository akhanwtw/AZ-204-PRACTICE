--------------------------------------------------------------------------------
üìå Question 121 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
**DRAG DROP**

An organization plans to deploy Azure storage services.

You need to configure shared access signature (SAS) for granting access to Azure Storage.

Which SAS types should you use? To answer, drag the appropriate SAS types to the correct requirements. Each SAS type may be used once, more than once, or not at all.

**SAS Types:**
1.  Account-level
2.  Service-level
3.  User delegation

**Answer Area:**
1.  **Delegate access to resources in one or more of the storage services:** [Select SAS Type]
2.  **Delegate access to a resource in a single storage service:** [Select SAS Type]
3.  **Secure a resource by using Azure AD credentials:** [Select SAS Type]

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------

1.  **Delegate access to resources in one or more of the storage services:** **Account-level**
2.  **Delegate access to a resource in a single storage service:** **Service-level**
3.  **Secure a resource by using Azure AD credentials:** **User delegation**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------

**1. Delegate access to resources in one or more of the storage services (Account-level)**
An **Account SAS** delegates access to resources in one or more of the storage services (Blob, File, Queue, Table). It allows you to grant access to service-level operations (like listing file shares) that are not available with a Service SAS. It also lets you delegate access to read, write, and delete operations on containers, file shares, queues, and tables.

**2. Delegate access to a resource in a single storage service (Service-level)**
A **Service SAS** delegates access to a resource in only **one** of the storage services: Azure Blob Storage, Azure Queue Storage, Azure Table Storage, or Azure Files. For example, if you want to grant read access to a specific container in Blob storage without exposing other services or the account key itself, you use a Service SAS.

**3. Secure a resource by using Azure AD credentials (User delegation)**
A **User Delegation SAS** is secured with Microsoft Entra (formerly Azure AD) credentials. Unlike the other two types (which are signed with the storage account key), this SAS is signed with a user delegation key generated via Azure AD authentication. Ideally, you should use Microsoft Entra credentials whenever possible as a security best practice, avoiding the need to store your account key in code.

**Summary of SAS Capabilities:**

| Feature | User Delegation SAS | Service SAS | Account SAS |
| :--- | :--- | :--- | :--- |
| **Supported Services** | Blob, Queue, Table, File (Preview) | Blob, Queue, Table, File | All |
| **Signed With** | Microsoft Entra (Azure AD) creds | Storage Account Key | Storage Account Key |
| **Scope** | Single Service | Single Service | Multiple Services |

**References:**
*   [Grant limited access to Azure Storage resources using SAS](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)
*   [Create a user delegation SAS](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-user-delegation-sas-create-cli)

--------------------------------------------------------------------------------
üìå Question 122
--------------------------------------------------------------------------------
You are a developer for a SaaS company that offers many web services.

All web services for the company must meet the following requirements:
*   Use API Management to access the services
*   Use OpenID Connect for authentication
*   Prevent anonymous usage

A recent security audit found that several web services can be called without any authentication.

Which API Management policy should you implement?

- A. jsonp
- B. authentication-certificate
- C. check-header
- D. validate-jwt

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. validate-jwt**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Why `validate-jwt` is the correct answer:**
OpenID Connect (OIDC) is an identity layer on top of the OAuth 2.0 protocol. It uses **JSON Web Tokens (JWTs)** (specifically ID tokens and access tokens) to assert the identity of a user or service.

To strictly enforce OIDC authentication and prevent anonymous usage, Azure API Management must verify that the incoming request contains a valid token signed by the trusted Identity Provider. The **`validate-jwt`** policy is specifically designed for this. It:
*   Extracts the token from a specified header (usually `Authorization`).
*   Checks that the token is not expired.
*   Validates the token signature against the OpenID Connect configuration URL (`check-header` cannot do this).
*   Validates claims like Audience (`aud`) and Issuer (`iss`).

If the token is missing or invalid, API Management rejects the request, thus preventing anonymous usage.

**2. Why the others are incorrect:**
*   **A. jsonp:** The `jsonp` policy adds JSON with Padding support to operations. This is used for cross-domain data fetching in browsers and has absolutely nothing to do with authentication or security enforcement.
*   **B. authentication-certificate:** This policy is used to authenticate with a specific backend service using a client certificate (Mutual TLS). While this is a security measure, the requirement specifically asks for **OpenID Connect**, which is token-based, not certificate-based.
*   **C. check-header:** The `check-header` policy enforces that a request or response contains a specific HTTP header and optionally validates its value. While you *could* check if an `Authorization` header exists, this policy cannot cryptographically validate the JWT signature, check expiration, or verify claims. Therefore, it cannot securely enforce OpenID Connect.

**References:**
*   [Protect an API by using OAuth 2.0 and OpenID Connect](https://learn.microsoft.com/en-us/azure/api-management/protect-with-oauth2-openid-connect)
*   [API Management access restriction policies (validate-jwt)](https://learn.microsoft.com/en-us/azure/api-management/validate-jwt-policy)


--------------------------------------------------------------------------------
üìå Question 123
--------------------------------------------------------------------------------
You have a new Azure subscription. You are developing an internal website for employees to view sensitive data. The website uses Azure Active Directory (Azure AD) for authentication.

You need to implement multifactor authentication for the website.

Which two actions should you perform? Each correct answer presents part of the solution.

- A. Configure the website to use Azure AD B2C.
- B. In Azure AD, create a new conditional access policy.
- C. Upgrade to Azure AD Premium.
- D. In Azure AD, enable application proxy.
- E. In Azure AD conditional access, enable the baseline policy.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **B. In Azure AD, create a new conditional access policy.**
- **C. Upgrade to Azure AD Premium.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why you need to upgrade to Azure AD Premium (C):**
Azure Active Directory Free edition provides basic security defaults, but it does not support custom **Conditional Access** policies. To Granularly enforce MFA specifically for a single internal website while leaving other access patterns alone (or configuring different rules for them), you require the advanced identity features found in **Azure AD Premium P1 or P2** (now part of Microsoft Entra ID P1/P2).

**Why you need to create a new conditional access policy (B):**
Once you have the Premium license, the mechanism to enforce Multi-Factor Authentication (MFA) based on specific conditions (like accessing a specific application, user group, or risk level) is **Conditional Access**. You would create a policy that:
1.  **Assignments**: Selects the users (Employees) and the Cloud App (The internal website).
2.  **Access Controls**: Selects "Grant access" but requires "Multi-factor authentication".

**Why the others are incorrect:**
*   **A. Azure AD B2C:** This is designed for business-to-consumer (customer-facing) applications. Since the requirements state the website is for **employees** (who are already in your main organizational Azure AD tenant), B2C is not the correct directory service.
*   **D. Enable application proxy:** Azure AD Application Proxy is used to publish on-premises web applications to the internet so remote users can access them securely. While it supports pre-authentication, enabling the proxy itself does not automatically turn on MFA. You would still need a Conditional Access policy to enforce the MFA requirement effectively.
*   **E. Enable the baseline policy:** Baseline policies in Azure AD contain pre-defined rules (like "Require MFA for admins"). However, these have been **deprecated** and replaced by **Security Defaults**. Furthermore, for a specific custom application requirement, creating a *new* targeted policy is the correct approach rather than relying on global defaults.

**References:**
*   [What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)
*   [Common Conditional Access policy: Require MFA for all users](https://learn.microsoft.com/en-us/entra/identity/conditional-access/howto-conditional-access-policy-all-users-mfa)

--------------------------------------------------------------------------------
üìå Question 124
--------------------------------------------------------------------------------
Your company is developing an Azure API.

You need to implement authentication for the Azure API. You have the following requirements:
*   All API calls must be secure.
*   Callers to the API must not send credentials to the API.

Which authentication mechanism should you use?

- A. Basic
- B. Anonymous
- C. Managed identity
- D. Client certificate

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**C. Managed identity**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Why Managed identity is the correct answer:**
Managed identities for Azure resources provide Azure services with an automatically managed identity in Microsoft Entra ID (formerly Azure AD).
*   **No credentials sent:** When using managed identities, the calling service (e.g., an Azure Function, VM, or App Service) obtains an access token from the local Azure Instance Metadata Service (IMDS) endpoint. It does *not* send a username, password, or client secret to the target API. Instead, it sends an OAuth 2.0 access token. The code itself does not handle or transmit credentials; the Azure infrastructure handles the identity negotiation.
*   **Secure:** It uses standard OAuth 2.0 and Azure AD mechanisms, ensuring encrypted and authenticated communication.

**2. Why the others are incorrect:**
*   **A. Basic:** Basic authentication requires the caller to send a username and password (credentials) encoded in the HTTP header with every request. This violates the requirement that callers must not send credentials.
*   **B. Anonymous:** Anonymous authentication provides no security or identity verification. While technically no credentials are sent, it fails the requirement that "All API calls must be secure" (assuming "secure" implies authenticated/authorized access).
*   **D. Client certificate:** While secure, Client Certificate authentication involves the caller sending a certificate (which acts as a credential) during the TLS handshake to prove its identity. The prompt's phrasing "must not send credentials" usually targets scenarios where secrets/passwords/keys are transmitted, favoring the transparency of Managed Identity.

**References:**
*   [What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)
*   [Authentication in Azure API Management](https://learn.microsoft.com/en-us/azure/api-management/api-management-authentication-policies)

--------------------------------------------------------------------------------
üìå Question 125
--------------------------------------------------------------------------------
**DRAG DROP**

You develop a web application.

You need to register the application with an active Azure Active Directory (Azure AD) tenant.

Which three actions should you perform in sequence? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.

**Actions:**
*   Select **Manifest** from the middle-tier service registration.
*   In Enterprise Applications, select **New application**.
*   Add a Cryptographic key.
*   Create a new application and provide the name, account type, and redirect URI.
*   Select the Azure AD instance.
*   Use an access token to access the secure resource.
*   In App Registrations, select **New registration**.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------

1.  **Select the Azure AD instance.**
2.  **In App Registrations, select New registration.**
3.  **Create a new application and provide the name, account type, and redirect URI.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To register a new custom-developed application in Azure Active Directory, you follow this standard workflow in the Azure Portal:

**Step 1: Select the Azure AD instance.**
First, you must ensure you are working within the correct Azure Active Directory tenant where the application should be registered. This involves navigating to the Azure Active Directory blade in the portal.

**Step 2: In App Registrations, select New registration.**
For application developers creating a new app (as opposed to adding an existing gallery app), the "App registrations" blade is the correct location. You click the "New registration" button to begin the wizard.
*(Note: "Enterprise Applications" is typically used for managing the service principals of apps that already exist or adding pre-integrated gallery applications, not for registering new custom code).*

**Step 3: Create a new application and provide the name, account type, and redirect URI.**
In the registration form, you must configure the core identity details:
*   **Name:** The user-facing display name.
*   **Supported account types:** Who can use the app (e.g., single tenant, multi-tenant).
*   **Redirect URI:** (Optional during creation, but standard for web apps) The endpoint where the authorization server sends tokens.
Finally, clicking "Register" completes the process.

**References:**
*   [Quickstart: Register an application with the Microsoft identity platform](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app)

--------------------------------------------------------------------------------
üìå Question 126
--------------------------------------------------------------------------------
You are developing an ASP.NET Core website that uses Azure FrontDoor. The website is used to build custom weather data sets for researchers. Data sets are downloaded by users as Comma Separated Value (CSV) files. The data is refreshed every 10 hours.

Specific files must be purged from the FrontDoor cache based upon Response Header values.

You need to purge individual assets from the Front Door cache.

Which type of cache purge should you use?

- A. single path
- B. wildcard
- C. root domain

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. single path**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Why single path is the correct answer:**
Azure Front Door allows you to purge content from the cache in two main ways: by **single path** or by **wildcard**.
The requirement states exactly: "**You need to purge individual assets**".
A **Single path** purge allow you to specify the exact path of the file you want to remove (e.g., `/weather/data/dataset1.csv`). This is the mechanism used to remove specific, individual files without affecting other cached content.

**2. Why the others are incorrect:**
*   **B. wildcard:** If you use a wildcard (e.g., `/weather/data/*`), you would purge *all* files under that path, not just the specific individual assets that require refreshing. This is less efficient and doesn't match the specific requirement to purge "individual assets".
*   **C. root domain:** This would purge the cache for the entire endpoint (effectively `/`), which is definitely not targeted at individual assets.

**References:**
*   [Cache purging in Azure Front Door](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-cache-purge)

--------------------------------------------------------------------------------
üìå Question 127
--------------------------------------------------------------------------------
A development team is creating a new REST API. The API will store data in Azure Blob storage. You plan to deploy the API to Azure App Service.

Developers must access the Azure Blob storage account to develop the API for the next two months. The Azure Blob storage account must not be accessible by the developers after the two-month time period.

You need to grant developers access to the Azure Blob storage account.

What should you do?

- A. Generate a shared access signature (SAS) for the Azure Blob storage account and provide the SAS to all developers.
- B. Create and apply a new lifecycle management policy to include a last accessed date value. Apply the policy to the Azure Blob storage account.
- C. Provide all developers with the access key for the Azure Blob storage account. Update the API to include the Coordinated Universal Time (UTC) timestamp for the request header.
- D. Grant all developers access to the Azure Blob storage account by assigning role-based access control (RBAC) roles.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Generate a shared access signature (SAS) for the Azure Blob storage account and provide the SAS to all developers.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Why SAS is the correct answer:**
A **Shared Access Signature (SAS)** is a URI that grants restricted access rights to Azure Storage resources. Crucially, a SAS token includes a specific **expiration time (se)**. You can generate a SAS token that is valid for exactly two months. Once the expiration date passes, the token becomes invalid automatically, ensuring the developers can no longer access the storage account without any manual intervention from administrators. This perfectly satisfies the requirement to revoke access after a set time period.

**2. Why the others are incorrect:**
*   **B. Lifecycle management policy:** This feature is used to manage the lifecycle of the *data* (blobs) itself, such as moving older files to cooler storage tiers or deleting them to save costs. It has nothing to do with managing *user access* permissions or authentication.
*   **C. Storage Account Access Key:** Access keys provide full, unrestricted administrative access to the storage account. They do not have an expiration date. To revoke access, you would have to manually regenerate the keys, which would break any other applications using them. Sharing master keys with developers is also a security risk. The mention of UTC timestamps in headers is irrelevant to access control.
*   **D. RBAC (Role-Based Access Control):** While RBAC is the recommended way to manage long-term access, standard RBAC assignments do not have a built-in automatic expiration mechanism (unless you use Azure AD Privileged Identity Management, which is a premium feature and not specified here). If you assigned standard RBAC roles, you would have to remember to manually remove the assignment after two months.

**References:**
*   [Grant limited access to Azure Storage resources using SAS](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)
*   [Define a stored access policy (to manage SAS expiry centrally)](https://learn.microsoft.com/en-us/rest/api/storageservices/define-stored-access-policy)

--------------------------------------------------------------------------------
üìå Question 128
--------------------------------------------------------------------------------
You develop an app that allows users to upload photos and videos to Azure storage. The app uses a storage REST API call to upload the media to a blob storage account named Account1. You have blob storage containers named Container1 and Container2.

Uploading of videos occurs on an irregular basis.

You need to copy specific blobs from Container1 to Container2 when a new video is uploaded.

What should you do?

- A. Copy blobs to Container2 by using the Put Blob operation of the Blob Service REST API
- B. Create an Event Grid topic that uses the Start-AzureStorageBlobCopy cmdlet
- C. Use AzCopy with the Snapshot switch to copy blobs to Container2
- D. Download the blob to a virtual machine and then upload the blob to Container2

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**B. Create an Event Grid topic that uses the Start-AzureStorageBlobCopy cmdlet**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Why Event Grid + Start-AzureStorageBlobCopy is the best solution:**
*   **Event-Driven Trigger (Event Grid):** The requirement states that uploads occur on an "irregular basis" and acts "when a new video is uploaded." Azure Event Grid is designed exactly for this scenarios. It eliminates the need for polling or running constant compute resources. It can subscribe to the `Microsoft.Storage.BlobCreated` event and trigger a handler (like an Azure Function or Automation Runbook) immediately.
*   **Server-Side Copy (Start-AzureStorageBlobCopy):** This PowerShell cmdlet (which wraps the underlying `Copy Blob` REST API) performs an asynchronous server-side copy. This means Azure copies the data directly from Container1 to Container2 without moving the data through your application or network. This is highly efficient for large files like videos.

**2. Why the others are incorrect:**
*   **A. Put Blob operation:** The `Put Blob` operation is used to upload new data or update content by sending the data stream. It is not used for copying existing blobs (that would be the `Copy Blob` operation). Additionally, simply calling an API doesn't solve the "trigger" requirement of reacting to a new upload event.
*   **C. AzCopy:** AzCopy is a command-line utility optimized for bulk data transfer or migration scripts. It is not designed to be a real-time event handler for individual low-latency reactions to file uploads.
*   **D. Download and Upload:** This is the "hairpin" approach. It is extremely inefficient because you pay for egress (downloading out of Azure) and ingress, and it takes significantly longer than a server-side copy.

**References:**
*   [Reacting to Blob storage events](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview)
*   [Start-AzureStorageBlobCopy (Az.Storage)](https://learn.microsoft.com/en-us/powershell/module/az.storage/start-azstorageblobcopy)

--------------------------------------------------------------------------------
üìå Question 129
--------------------------------------------------------------------------------
You deploy an Azure App Service web app. You create an app registration for the app in Azure Active Directory (Azure AD) and Twitter.

The app must authenticate users and must use SSL for all communications. The app must use Twitter as the identity provider.

You need to validate the Azure AD request in the app code.

What should you validate?

- A. ID token header
- B. ID token signature
- C. HTTP response code
- D. Tenant ID

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**D. Tenant ID**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
In an Azure App Service application using built-in authentication (formerly known as "Easy Auth"), the platform handles the complexity of federated identity. Even when Twitter is the identity provider, Azure AD (acting as the authentication broker or via B2C/External Identities) issues the final ID Token to your application.

**Why Tenant ID (D) is the correct answer:**
When App Service authentication is enabled, the platform middleware automatically validates the **signature** of the ID tokens it receives before passing the request to your application code. It ensures the token was signed by Azure AD and hasn't been tampered with.

However, the platform validation does not strictly enforce *which* tenant the user belongs to if the App Registration is configured as "Multi-Tenant". Therefore, to secure your application logic and ensure the request originates from the expected organization (or specific directory), the developer is explicitly responsible for validating the **Tenant ID (`tid`)** claim within the application code.

**Why others are incorrect/insufficient:**
*   **A. ID token header:** The header contains metadata (like the algorithm used), but checking it does not validate the integrity or origin of the token.
*   **B. ID token signature:** While validating the signature is cryptographically essential, checking it in *app code* is redundant if using App Service Authentication (which does this automatically). If the question implied writing raw middleware without Easy Auth, this would be correct, but "Tenant ID" is the intended answer for the App Service context in Microsoft certification standards.
*   **C. HTTP response code:** This is an output of a request, not a validation mechanism for an incoming authentication request.

**References:**
*   [Configure your App Service app to use Twitter login](https://learn.microsoft.com/en-us/azure/app-service/configure-authentication-provider-twitter)
*   [Azure App Service: Validate tokens from providers](https://learn.microsoft.com/en-us/azure/app-service/configure-authentication-oauth-tokens)

--------------------------------------------------------------------------------
üìå Question 130
--------------------------------------------------------------------------------
You develop a containerized application. You plan to deploy the application to a new Azure Container instance by using a third-party continuous integration and continuous delivery (CI/CD) utility.

The deployment must be unattended and include all application assets. The third-party utility must only be able to push and pull images from the registry. The authentication must be managed by Azure Active Directory (Azure AD). The solution must use the principle of least privilege.

You need to ensure that the third-party utility can access the registry.

Which authentication options should you use?

**Registry authentication method:**
*   Service principal
*   Individual identity
*   Repository-scoped access token
*   Managed identity for Azure resources

**RBAC role:**
*   AcrPull
*   Owner
*   AcrPush
*   Contributor

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
1. **Registry authentication method:** Service principal
2. **RBAC role:** AcrPush

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Registry authentication method: Service principal**
For unattended authentication scenarios involving third-party tools (like Jenkins, CircleCI, etc.) interacting with Azure resources, a **Service Principal** is the standard approach.
*   **Managed identity** is typically used for Azure resources accessing other Azure resources (e.g., ACI accessing ACR), but the question specifies a *third-party* CI/CD utility is doing the pushing/deployment.
*   **Individual identity** is not suitable for unattended/automated scenarios.
*   **Repository-scoped access token** does not use Azure AD directly for management in the way a Service Principal does (it's a token management feature of ACR itself, though often tied to it, Service Principal is the primary mechanism for standard Azure AD integration for CI/CD). Furthermore, the core requirement is Azure AD management for the utility entity.

**2. RBAC role: AcrPush (Correct)**
*   **Requirement:** "Push and pull images... principle of least privilege."
*   **Reasoning:** Contrary to what its name suggests, the **AcrPush** role specifically grants permissions to **both push and pull** images. It is effectively the "CI/CD/Builder" role.
*   *Why not Contributor?* The `Contributor` role grants permission to push and pull, but also to delete the registry, manage policies, and perform other administrative actions. This violates the "principle of least privilege."
*   *Why not AcrPull?* This role is Read-only (pull only) and would fail the requirement to "push" images.

**Reference:**
*   [Azure Container Registry roles and permissions](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-roles) ("AcrPush: Push and pull content to/from the registry.")
