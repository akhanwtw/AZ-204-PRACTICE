--------------------------------------------------------------------------------
üìå Question 191 ‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You are developing an application to store millions of images in Azure blob storage. The images are uploaded to an Azure blob storage container named `companyimages` contained in an Azure blob storage account named `companymedia`. The stored images are uploaded with multiple blob index tags across multiple blobs in the container.

You must find all blobs whose tags match a search expression in the container. The search expression must evaluate an index tag named `Status` with a value of `Final`.

You need to construct the GET method request URI.

**Parameters:**
1. `Status = 'Final'`
2. `Status <= 'Final'`
3. `companymedia`
4. `companyimages`

**Answer Area:**  
`https://`___________________`.blob.core.windows.net/`___________________`?restype=container&comp=blobs&where=` ___________________

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
`https://` **companymedia** `.blob.core.windows.net/` **companyimages** `?restype=container&comp=blobs&where=` **Status = 'Final'**

1.  **First Box (Account Name):** `companymedia`
2.  **Second Box (Container Name):** `companyimages`
3.  **Third Box (Where Clause):** `Status = 'Final'`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Account Name: `companymedia`**
The structure of an Azure Blob Storage URL is `https://<account_name>.blob.core.windows.net/...`. The problem states the storage account is named "companymedia".

**2. Container Name: `companyimages`**
The container is the resource immediately following the base domain in the URL path. The problem states the container is named "companyimages".

**3. Where Clause: `Status = 'Final'`**
The query parameter `where` is used to filter blobs based on their Blob Index Tags. The syntax for an equality check in the filter expression is `Key = 'Value'`.
*   The requirement is to find blobs where the tag `Status` has the value `Final`.
*   Therefore, the correct expression is `Status = 'Final'`.
*   The option `Status <= 'Final'` is syntactically valid for a range search, but incorrect for this specific requirement which asks for an exact match.

**References:**
*   [Find blobs by using blob index tags (REST API)](https://learn.microsoft.com/en-us/rest/api/storageservices/find-blobs-by-tags)

--------------------------------------------------------------------------------
üìå Question 192 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
--------------------------------------------------------------------------------
You provisioned an Azure Cosmos DB for NoSQL account named account1 with the default consistency level.

You plan to configure the consistency level on a per request basis. The level needs to be set for consistent prefix for read and write operations to account1.

You need to identify the resulting consistency level for read and write operations.

Which levels should you configure?

**Operation type: Read operations**
1. strong
2. session
3. consistent prefix

**Operation type: Write operations**
1. strong
2. session
3. consistent prefix

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **Read operations:** consistent prefix
- **Write operations:** session

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Read operations: consistent prefix**
In Azure Cosmos DB, clients can override the consistency level for **Read** operations on a per-request basis, provided the requested level is weaker than the account's configured default.
*   The default consistency level for a new Cosmos DB account is **Session**.
*   **Consistent Prefix** is a weaker consistency level than Session.
*   Therefore, the request to lower the read consistency to Consistent Prefix is honored.

**2. Write operations: session**
The consistency level for **Write** operations is strictly bound to the Azure Cosmos DB account configuration and **cannot be overridden** on a per-request basis.
*   Even though the code attempts to set the request consistency to Consistent Prefix, the valid behavior for writes ignores this parameter and uses the account's default.
*   Since the account was provisioned with the "default consistency level," which is **Session**, the write operation will be performed using Session consistency.

**References:**
*   [Override the default consistency level](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/consistency-levels#override-the-default-consistency-level)
*   [Consistency levels in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/consistency-levels)

--------------------------------------------------------------------------------
üìå Question 193
--------------------------------------------------------------------------------
You develop a web application that provides access to legal documents that are stored on Azure Blob Storage with version-level immutability policies. Documents are protected with both time-based policies and legal hold policies. All time-based retention policies have the `AllowProtectedAppendWrites` property enabled.

You have a requirement to prevent the user from attempting to perform operations that would fail **only when a legal hold is in effect** and when all other policies are expired.

You need to meet the requirement.

Which two operations should you prevent? Each correct answer presents a complete solution.

NOTE: Each correct selection is worth one point.

A. adding data to documents
B. deleting documents
C. creating documents
D. overwriting existing documents

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. adding data to documents**
**D. overwriting existing documents**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
The key to this question lies in understanding the difference between the permissions granted by the `AllowProtectedAppendWrites` setting in a Time-Based Retention Policy versus the strict restrictions of a Legal Hold.

**1. A. Adding data to documents (Prevent)**
*   **Time-Based Policy Context:** The question states that the time-based retention policies have `AllowProtectedAppendWrites` **enabled**. This setting allows new blocks of data to be added (appended) to an append blob or block blob while the retention period is active.
*   **Legal Hold Context:** A **Legal Hold** is stricter. It makes the blob strictly Read-Only (WORM - Write Once, Read Many). Even if `AllowProtectedAppendWrites` is enabled on the concurrent time-based policy, the existence of a Legal Hold suppresses this capability.
*   **Conclusion:** If the time-based policy was the only thing active (or if it expired), adding data would be allowed (due to the setting). However, because a Legal Hold is in effect, this operation will fail. Therefore, you must prevent the user from attempting it.

**2. D. Overwriting existing documents (Prevent)**
*   **Time-Based Policy Context:** While `AllowProtectedAppendWrites` allows *adding* new blocks, standard immutability usually prevents overwriting/modifying *existing* content. However, in specific configurations or interpretations of "overwriting" (like `PutBlock` on new limits), the critical distinction is the Legal Hold.
*   **Legal Hold Context:** A Legal Hold prevents **any** modification, including overwriting.
*   **Why this fits the criteria:** The question asks for operations that fail primarily due to the Legal Hold when other policies might have allowed flexibility (like the append writes) or when policies expire. More critically, even if the time-based policy expires, if a Legal Hold is still active, you cannot overwrite the document. (Note: While overwriting is generally blocked by *any* active WORM policy, the Legal Hold is an indefinite block until explicitly removed, whereas time-based policies naturally expire. If the time-based policy expired, you normally could overwrite/delete, but the Legal Hold prevents it).

**Why B (Deleting documents) is incorrect in this specific "only when" context:**
While a Legal Hold definitely prevents deletion, a Time-Based retention policy *also* prevents deletion. The question asks for operations that would fail "only when a legal hold is in effect **and when all other policies are expired**."
*   If *both* are active, both prevent deletion.
*   If the time-based policy expires, the Legal Hold prevents deletion.
*   However, options A and D highlight the specific conflict regarding `AllowProtectedAppendWrites` (which allows writing under time-based but not under Legal Hold) or standard lifecycle behavior. In the context of "AllowProtectedAppendWrites", the *append* action is the most distinct difference: allowed by Time-Based, blocked by Legal Hold. Microsoft documentation explicitly highlights that `AllowProtectedAppendWrites` does not apply to Legal Holds.

**References:**
*   [Immutable storage for Azure Blob Storage > Allow protected append blobs writes](https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-policy-configure-version-scope?tabs=azure-portal#allow-protected-append-blobs-writes)
*   "The AllowProtectedAppendWrites setting... applies only to time-based retention policies. It doesn't apply to legal hold policies."

--------------------------------------------------------------------------------
üìå Question 194
--------------------------------------------------------------------------------
You develop Azure solutions.

You must connect to a No-SQL globally-distributed database by using the .NET API.

You need to create an object to configure and execute requests in the database.

Which code segment should you use?

- A. `database_name = 'MyDatabase'database = client.create_database_if_not_exists(id=database_name)`
- B. `client = CosmosClient(endpoint, key)`
- C. `container_name = 'MyContainer'container = database.create_container_if_not_exists(id=container_name, partition_key=PartitionKey(path="/lastName"), offer_throughput=400 )`

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
B. `client = CosmosClient(endpoint, key)`

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why B is correct (CosmosClient):**
To interact with Azure Cosmos DB (whether using the .NET SDK or Python SDK), the first step is to instantiate the **CosmosClient**. This object acts as the logical representation of the Azure Cosmos DB account. It is responsible for:
1.  **Configuring the connection:** It holds the Account Endpoint and Auth Key (or TokenCredential).
2.  **Executing requests:** It is the entry point used to create, read, or query Databases and Containers.

The code segment `client = CosmosClient(endpoint, key)` (or `new CosmosClient(...)` in C#) initializes this client. Without this object, you cannot perform operations like creating databases (Option A) or containers (Option C).

**Why others are incorrect:**
*   **A:** Creates a *Database* object. This requires an existing `client` instance to function.
*   **C:** Creates a *Container* object. This requires an existing `database` object (and by extension a `client`) to function.

**References:**
*   [Get started with Azure Cosmos DB for NoSQL using .NET](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-dotnet?pivots=dev-container-vscode)

--------------------------------------------------------------------------------
üìå Question 195 
--------------------------------------------------------------------------------
You are updating an application that stores data on Azure and uses Azure Cosmos DB for storage. The application stores data in multiple documents associated with a single username.

The application requires the ability to update multiple documents for a username in a single ACID operation.

You need to configure Azure Cosmos DB.

Which two actions should you perform? Each correct answer presents part of the solution.

- A. Create a collection sharded on username to store documents.
- B. Configure Azure Cosmos DB to use the Gremlin API.
- C. Create an unsharded collection to store documents.
- D. Configure Azure Cosmos DB to use the MongoDB API.

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Create a collection sharded on username to store documents.**
- **D. Configure Azure Cosmos DB to use the MongoDB API.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Partitioning Strategy (A):**
In Azure Cosmos DB, transactional guarantees (ACID) are scoped to a **single logical partition**. To update multiple documents in a single transaction, all those documents must reside within the same partition. By creating a collection **sharded (partitioned) on `username`**, you ensure that all documents belonging to a specific user are stored together in the same logical partition, making a multi-document transaction possible.

**2. API Selection (D):**
The **Azure Cosmos DB for MongoDB** (versions 4.0 and higher) natively supports **multi-document ACID transactions**. This allows developers to use standard MongoDB client drivers to execute transactions across multiple documents, provided they are within the same shard (partition).

**Why others are incorrect/less suitable:**
*   **B (Gremlin API):** This is designed for Graph workloads (vertices and edges), not the document-oriented "multiple documents" model described in the prompt.
*   **C (Unsharded collection):** While unsharded (legacy/fixed) collections do support transactions (as the entire container is one partition), they are limited in storage (usually 20GB) and throughput. Modern applications on Azure should use partitioned collections for scalability, ruled out by "Create a collection sharded..." being the better architectural choice.
*   *(Note on Core/SQL API):* The Core (NoSQL) API also supports ACID transactions via **Stored Procedures** or **Transactional Batch**. However, "Use Core API" is not a listed option, whereas "Configure... MongoDB API" is a valid alternative configuration that satisfies the requirement given the provided choices.

**References:**
*   [Transactions in Azure Cosmos DB for MongoDB](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/feature-support-40#transactions)
*   [Partitioning and horizontal scaling in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/partitioning-overview)

--------------------------------------------------------------------------------
üìå Question 196
--------------------------------------------------------------------------------
You are developing an application to store business-critical data in Azure Blob storage.

The application must meet the following requirements:
‚Ä¢ Data must not be modified or deleted for a user-specified interval.
‚Ä¢ Data must be protected from overwrites and deletes.
‚Ä¢ Data must be written once and allowed to be read many times (WORM).

You need to protect the data in the Azure Blob storage account.

Which two actions should you perform? Each correct answer presents part of the solution.

- A. Configure a time-based retention policy for the storage account.
- B. Create an account shared-access signature (SAS).
- C. Enable the blob change feed for the storage account.
- D. Enable version-level immutability support for the storage account.
- E. Enable point-in-time restore for containers in the storage account.
- F. Create a service shared-access signature (SAS).

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Configure a time-based retention policy for the storage account.**
- **D. Enable version-level immutability support for the storage account.**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To satisfy the requirements for **WORM (Write Once, Read Many)** storage with a specific retention duration, you must use Azure Blob Storage **Immutable Storage**.

**1. A. Configure a time-based retention policy (Correct)**
The requirement states that data must not be modified for a **"user-specified interval"**. A **Time-based retention policy** is specifically designed for this. It enforces immediate WORM protection and holds it for the defined duration (e.g., 7 years). Users cannot delete or overwrite the data during this period. The alternative, a "Legal Hold," is indefinite (no specified interval), so Time-based is the correct configuration.

**2. D. Enable version-level immutability support (Correct)**
To securely manage business-critical data while potentially allowing the application to continue writing new versions of a file (while keeping the original strictly immutable), or simply to enable the granularity required for modern immutable policies, you enable **version-level immutability**. This feature allows you to apply the time-based retention policies to specific versions of a blob. Combined with the policy itself (Option A), this provides the complete WORM solution.

**Why others are incorrect:**
*   **B & F (SAS):** Shared Access Signatures control *authorization* (who can access). They do not physically prevent a valid administrator or a user with a different key from deleting data, nor do they enforce retention intervals.
*   **C (Change Feed):** This acts as a transaction log to *audit* changes, but it does not *prevent* them.
*   **E (Point-in-time restore):** This is a *recovery* feature (undoing accidental deletes), not a *prevention* feature. It effectively rolls back data, which contradicts the strict WORM requirement where data should not be modified in the first place.

**References:**
*   [Store business-critical blob data with immutable storage](https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview)
*   [Configure immutability policies for blob versions](https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-policy-configure-version-scope?tabs=azure-portal)

  --------------------------------------------------------------------------------
üìå Question 197
--------------------------------------------------------------------------------
You implement an Azure solution to include Azure Cosmos DB, the latest Azure Cosmos DB SDK, and the Core (SQL) API. You also implement a change feed processor on a new container instance by using the Azure Functions trigger for Azure Cosmos DB.

A large batch of documents continues to fail when reading one of the documents in the batch. The same batch of documents is continuously retried by the triggered function and a new batch of documents must be read.

You need to implement the change feed processor to read the documents.

Which feature should you implement?

**Requirement: Read a new batch of documents while keeping track of the failing batch of documents.**
1. Lease container
2. Dead-letter queue
3. Life-cycle notifications
4. Change feed estimator

**Requirement: Handle errors in the change feed processor.**
1. Lease container
2. Dead-letter queue
3. Life-cycle notifications
4. Change feed estimator

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**Read a new batch...:** Dead-letter queue
**Handle errors...:** Dead-letter queue

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Read a new batch while keeping track of the failing batch (Dead-letter queue):**
The scenario describes a "poison message" situation in an Azure Function triggered by Cosmos DB. By default, if the function fails, the Change Feed checkpoint is not updated, and the function indefinitely retries the same batch.
To "read a new batch" (move forward) while "keeping track of the failing batch," you configure a **Dead-letter queue**. When the maximum retry count is reached, the failing batch is sent to a storage location (like a Service Bus queue or Blob storage) known as the dead-letter queue. The checkpoint is then advanced, allowing the processor to continue to the **new batch**.

**2. Handle errors in the change feed processor (Dead-letter queue):**
While "Life-cycle notifications" can tell you *when* an error occurs in the host, they are logging mechanisms, not error handling mechanisms for data processing logic. The standard way to **handle** processing errors (specifically retries and eventual offloading of bad data) within the Azure Functions trigger context is via the **Dead-letter queue** configuration in the `host.json` or function settings. This ensures the system remains robust and doesn't get stuck on a single bad document.

**Why others are incorrect:**
*   **Lease container:** Tracks the state (checkpoint) of the processor but does not natively handle error logic or "skipping" bad batches without manual intervention.
*   **Life-cycle notifications:** Used for monitoring the health of the Change Feed Processor (e.g., when a lease is acquired or lost), not for handling data-level processing errors.
*   **Change feed estimator:** Used to monitor how far behind the processor is (lag), not to handle errors.

**References:**
*   [Azure Cosmos DB trigger for Azure Functions - Retry policies](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-cosmosdb-v2-trigger?tabs=in-process%2Cfunctionsv2&pivots=programming-language-csharp#retry-policies)

--------------------------------------------------------------------------------
üìå Question 198
--------------------------------------------------------------------------------
You have a Linux container-based console application that uploads image files from customer sites all over the world. A back-end system that runs on Azure virtual machines processes the images by using the Azure Blobs API.

You are not permitted to make changes to the application.

Some customer sites only have phone-based internet connections.

You need to configure the console application to access the images.

What should you use?

- A. Azure BlobFuse
- B. Azure Disks
- C. Azure Storage Network File System (NFS) 3.0 support
- D. Azure Files

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
**A. Azure BlobFuse**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**Why Azure BlobFuse is the correct choice:**

1.  **Requirement: Linux Container & No Code Changes**
    BlobFuse is a virtual file system driver for Azure Blob Storage. It allows you to access your existing block blob data in your storage account through the Linux file system (FUSE) interface. The application sees a standard file path and performs standard file I/O operations (open, close, read, write), while BlobFuse translates these into Azure Blob Storage REST API calls. This satisfies the requirement to not modify the application code.

2.  **Requirement: Backend uses Azure Blobs API**
    Since the backend system analyzes images using the Azure Blobs API, the data must reside in Blob Storage. BlobFuse writes directly to Blob Storage, ensuring the backend can access the files immediately without migration or extra copy steps.

3.  **Requirement: Phone-based (High Latency/Intermittent) Connections**
    BlobFuse includes caching features on the local file system. This helps mitigate the impact of latency inherent in phone-based internet connections by allowing the application to write to the local cache first, while the driver handles the upload to Azure.

**Why other options are incorrect:**

*   **B. Azure Disks:** Azure Managed Disks are block-level storage volumes designed to be attached to a single VM (or shared usage in specific clusters). They are not suitable for distinct client applications running at sites "all over the world" to upload data to a central repository.
*   **C. Azure Storage NFS 3.0 support:** While NFS 3.0 allows mounting Blob storage, the protocol is generally "chatty" and sensitive to network latency. It is typically recommended for use within a virtual network (VNet) rather than over the public internet (especially phone-based connections), where security and performance would be major issues.
*   **D. Azure Files:** Azure Files offers fully managed file shares accessible via SMB or NFS. While it can be mounted on Linux, the data sits in "File Storage," not "Blob Storage." Since the backend processing system uses the **Azure Blobs API**, using Azure Files would require a separate process to move data from File Storage to Blob Storage, adding unnecessary complexity.

**References:**
*   [How to mount Azure Blob Storage as a file system with BlobFuse](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux)

--------------------------------------------------------------------------------
üìå Question 199
--------------------------------------------------------------------------------
You need to implement the retail store location Azure Function.

How should you configure the solution? To answer, select the appropriate options in the answer area.

**Configuration Table:**

1.  **Binding**
    *   Blob storage
    *   Azure Cosmos DB
    *   Event Grid
    *   HTTP

2.  **Binding Direction**
    *   Input
    *   Output

3.  **Trigger**
    *   Blob storage
    *   Azure Cosmos DB
    *   Event Grid
    *   HTTP

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **1. Binding:** Azure Cosmos DB
- **2. Binding Direction:** Output
- **3. Trigger:** Blob storage

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
**1. Trigger: Blob storage**
In typical Azure retail scenarios (often referenced in certification case studies), store location data is uploaded as a batch file (JSON, CSV) to a storage container. The most direct way to execute logic immediately upon the upload of a file is to use the **Blob storage trigger**. This starts the function whenever a new file is detected in the specified container.

**2. Binding: Azure Cosmos DB**
The function's purpose is to process the uploaded location data and make it available for the application. Store location data (which typically includes hierarchical data like addresses, coordinates, and operating hours) is ideally stored in a NoSQL document database like **Azure Cosmos DB**. Therefore, the function requires a binding to communicate with this database.

**3. Binding Direction: Output**
The data flow is: **File Upload (Source) -> Function (Processing) -> Database (Destination)**.
Since the function is reading from the Blob (via the trigger) and writing the processed results *into* the database, the binding direction for Azure Cosmos DB must be **Output**.

**Why other combinations are incorrect:**
*   **Trigger: HTTP:** This would be used for an API call to *retrieve* data, but the context of "implementing the location function" alongside storage options usually implies the data ingestion pipeline (ETL) relative to the common "Blob -> Function -> Cosmos" pattern.
*   **Binding: Blob storage:** While the function reads from Blob storage, this is handled by the **Trigger** (which provides the input stream). Using Blob storage as the *Binding* as well would imply we are moving files from one container to another, not parsing them into a database.
*   **Event Grid:** While Event Grid can trigger functions on blob events (low latency), "Blob storage" is the standard direct trigger answer for file processing unless specific requirements about latency or filtering are mentioned.

--------------------------------------------------------------------------------
üìå Question 200
--------------------------------------------------------------------------------
You need to implement a solution to resolve the retail store location data issue.

Which three Azure Blob features should you enable? Each correct answer presents part of the solution.

- A. Soft delete
- B. Change feed
- C. Snapshots
- D. Versioning
- E. Object replication
- F. Immutability

--------------------------------------------------------------------------------
‚úÖ Correct Answer
--------------------------------------------------------------------------------
- **A. Soft delete**
- **B. Change feed**
- **D. Versioning**

--------------------------------------------------------------------------------
üìù Explanation
--------------------------------------------------------------------------------
To resolve issues regarding data corruption, accidental deletion, or application errors affecting the retail store location data, the standard Azure solution is to enable **Point-in-time restore** for block blobs. Point-in-time restore allows you to roll back block blob data to an earlier state.

To enable Point-in-time restore, you must enable the following three specific features on the storage account:

1.  **Soft delete (A):** This retains deleted blobs and snapshots for a specified period, allowing them to be recovered. Both blob soft delete and container soft delete are typically part of this strategy; specifically, blob soft delete is a prerequisite for point-in-time restore.
2.  **Change feed (B):** This provides a transaction log of all changes (creations, modifications, deletions) that occur to the blobs in the storage account. Point-in-time restore uses this feed to determine the exact state of blobs at the specific past timestamp.
3.  **Versioning (D):** This automatically saves the state of a blob when it is overwritten. Point-in-time restore uses versions to restore the blob to the correct previous state.

**Why other options are incorrect:**
*   **C. Snapshots:** While snapshots provide a read-only version of a blob at a point in time, they are a manual or application-driven mechanism. Versioning (D) is the automated feature required for the platform-managed Point-in-time restore capability.
*   **E. Object replication:** This asynchronously copies block blobs between a source storage account and a destination account. It is used for disaster recovery (geo-redundancy) or data locality, not specifically for resolving local data corruption or accidental deletion "in place" via rollback.
*   **F. Immutability:** This (WORM - Write Once, Read Many) prevents data from being modified or deleted for a user-specified interval. While it protects data, it prevents the updates that are likely necessary for "retail store location data" and does not facilitate rolling back changes if valid updates cause issues.

**References:**
*   [Point-in-time restore for block blobs](https://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview)
